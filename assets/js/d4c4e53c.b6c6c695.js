"use strict";(globalThis.webpackChunkhumanoid_robotics_course=globalThis.webpackChunkhumanoid_robotics_course||[]).push([[3626],{7633:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-1-physical-ai-foundations/practical-lab","title":"Module 1: Practical Lab - Hands-On Physical AI and Embodied Intelligence","description":"Lab Overview","source":"@site/docs/module-1-physical-ai-foundations/practical-lab.md","sourceDirName":"module-1-physical-ai-foundations","slug":"/module-1-physical-ai-foundations/practical-lab","permalink":"/humanoid-robotics-course/docs/module-1-physical-ai-foundations/practical-lab","draft":false,"unlisted":false,"editUrl":"https://github.com/RizSheik/humanoid-robotics-course/edit/main/docs/module-1-physical-ai-foundations/practical-lab.md","tags":[],"version":"current","frontMatter":{}}');var o=i(4848),t=i(8453);const s={},l="Module 1: Practical Lab - Hands-On Physical AI and Embodied Intelligence",a={},c=[{value:"Lab Overview",id:"lab-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Required Software/Tools",id:"required-softwaretools",level:3},{value:"Lab Duration",id:"lab-duration",level:3},{value:"Lab 1: Basic Embodied Agent",id:"lab-1-basic-embodied-agent",level:2},{value:"Objective",id:"objective",level:3},{value:"Setup",id:"setup",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Analysis",id:"analysis",level:3},{value:"Code Template",id:"code-template",level:3},{value:"Lab 2: Morphological Computation",id:"lab-2-morphological-computation",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Setup",id:"setup-1",level:3},{value:"Implementation Steps",id:"implementation-steps-1",level:3},{value:"Analysis",id:"analysis-1",level:3},{value:"Code Template",id:"code-template-1",level:3},{value:"Lab 3: Sensorimotor Learning",id:"lab-3-sensorimotor-learning",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Setup",id:"setup-2",level:3},{value:"Implementation Steps",id:"implementation-steps-2",level:3},{value:"Analysis",id:"analysis-2",level:3},{value:"Code Template",id:"code-template-2",level:3},{value:"Lab 4: Balance and Dynamic Control",id:"lab-4-balance-and-dynamic-control",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Setup",id:"setup-3",level:3},{value:"Implementation Steps",id:"implementation-steps-3",level:3},{value:"Analysis",id:"analysis-3",level:3},{value:"Code Template",id:"code-template-3",level:3},{value:"Lab Report Requirements",id:"lab-report-requirements",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Lab Extension Ideas",id:"lab-extension-ideas",level:2},{value:"Troubleshooting Tips",id:"troubleshooting-tips",level:2},{value:"References and Further Reading",id:"references-and-further-reading",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-1-practical-lab---hands-on-physical-ai-and-embodied-intelligence",children:"Module 1: Practical Lab - Hands-On Physical AI and Embodied Intelligence"})}),"\n",(0,o.jsx)(n.h2,{id:"lab-overview",children:"Lab Overview"}),"\n",(0,o.jsx)(n.p,{children:"This practical lab provides hands-on experience with the principles of Physical AI and embodied intelligence using simulation environments. The lab is structured to progress from simple concepts to more complex applications, allowing students to experience how physical embodiment can enhance robot intelligence."}),"\n",(0,o.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this lab, students will be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement and evaluate simple embodied agents in simulation"}),"\n",(0,o.jsx)(n.li,{children:"Design control systems that exploit morphological computation"}),"\n",(0,o.jsx)(n.li,{children:"Apply sensorimotor learning to robotic tasks"}),"\n",(0,o.jsx)(n.li,{children:"Analyze the trade-offs between embodied and traditional approaches"}),"\n",(0,o.jsx)(n.li,{children:"Understand the role of physical dynamics in robotic intelligence"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"required-softwaretools",children:"Required Software/Tools"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"ROS 2 (Humble Hawksbill or later)"}),"\n",(0,o.jsx)(n.li,{children:"Gazebo Harmonic or Isaac Sim"}),"\n",(0,o.jsx)(n.li,{children:"Python 3.11+"}),"\n",(0,o.jsx)(n.li,{children:"Basic knowledge of ROS 2 packages and nodes"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"lab-duration",children:"Lab Duration"}),"\n",(0,o.jsx)(n.p,{children:"This lab is designed for 12-15 hours of work, typically spread over 2-3 weeks."}),"\n",(0,o.jsx)(n.h2,{id:"lab-1-basic-embodied-agent",children:"Lab 1: Basic Embodied Agent"}),"\n",(0,o.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Create a simple wheeled robot that demonstrates basic embodied intelligence by navigating using only local sensory information and simple reactive behaviors."}),"\n",(0,o.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Launch a Gazebo simulation with a simple wheeled robot in an environment with obstacles."}),"\n",(0,o.jsx)(n.li,{children:"The robot should have basic sensors (e.g., forward, left, right distance sensors) and differential drive control."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create a ROS 2 node that subscribes to sensor data and publishes velocity commands"}),"\n",(0,o.jsxs)(n.li,{children:["Implement a simple reactive controller that:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Moves forward when no obstacles are near"}),"\n",(0,o.jsx)(n.li,{children:"Turns right when obstacle is ahead"}),"\n",(0,o.jsx)(n.li,{children:"Turns left when obstacles are ahead and right"}),"\n",(0,o.jsx)(n.li,{children:"Turns right when only left has an obstacle"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"analysis",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Observe how the robot behavior emerges from the interaction between the body (wheels), environment (obstacles), and simple control rules"}),"\n",(0,o.jsx)(n.li,{children:"Compare the robustness of this approach to a path-planning approach"}),"\n",(0,o.jsx)(n.li,{children:"Discuss how the robot's physical embodiment contributes to its behavior"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"code-template",children:"Code Template"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist\n\nclass EmbodiedController(Node):\n    def __init__(self):\n        super().__init__('embodied_controller')\n        self.subscription = self.create_subscription(\n            LaserScan,\n            'laser_scan',\n            self.scan_callback,\n            10)\n        self.publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n        \n    def scan_callback(self, msg):\n        # Get distance readings in front, left, right\n        front = min(msg.ranges[0:10] + msg.ranges[-10:])\n        left = min(msg.ranges[80:100])\n        right = min(msg.ranges[260:280])\n        \n        # Simple reactive controller\n        cmd = Twist()\n        if front > 1.0:  # No obstacle ahead\n            cmd.linear.x = 0.5\n            cmd.angular.z = 0.0\n        elif right > 1.0:  # Turn right if clear\n            cmd.linear.x = 0.0\n            cmd.angular.z = -0.5\n        elif left > 1.0:  # Turn left if clear\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.5\n        else:  # Turn left as default\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.5\n            \n        self.publisher.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = EmbodiedController()\n    rclpy.spin(controller)\n    controller.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"lab-2-morphological-computation",children:"Lab 2: Morphological Computation"}),"\n",(0,o.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Demonstrate morphological computation by creating controllers that take advantage of the physical properties of the robot's body rather than relying on complex algorithms."}),"\n",(0,o.jsx)(n.h3,{id:"setup-1",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Create two simulated robots in Gazebo:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"A simple manipulator with passive compliance at the end-effector"}),"\n",(0,o.jsx)(n.li,{children:"A manipulator with precise, non-compliant joints"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"Set up the same pick-and-place task for both robots"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-1",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create a simple position controller for the compliant robot that doesn't need to account for object compliance"}),"\n",(0,o.jsx)(n.li,{children:"Create a complex force-position controller for the non-compliant robot that must carefully manage contact forces"}),"\n",(0,o.jsx)(n.li,{children:"Compare the performance of both approaches"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"analysis-1",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Document the differences in control complexity between the two approaches"}),"\n",(0,o.jsx)(n.li,{children:"Explain how the physical compliance of the first robot performs computational work"}),"\n",(0,o.jsx)(n.li,{children:"Discuss the trade-offs between morphological computation and algorithmic computation"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"code-template-1",children:"Code Template"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\nclass MorphologicalComputation(Node):\n    def __init__(self):\n        super().__init__('morphological_comp')\n        self.joint_pub = self.create_publisher(Float64MultiArray, 'joint_commands', 10)\n        self.state_sub = self.create_subscription(JointState, 'joint_states', self.state_callback, 10)\n        \n        # Target position for pick-and-place task\n        self.target_pos = np.array([0.5, 0.2, 0.1])  # x, y, z in meters\n        \n    def state_callback(self, msg):\n        # Get current end-effector position (simplified)\n        # In practice, you'd use forward kinematics\n        current_pos = np.array([msg.position[0], msg.position[1], msg.position[2]])\n        \n        # Simple PD controller for compliant robot (the compliance handles force control)\n        error = self.target_pos - current_pos\n        cmd = Float64MultiArray()\n        cmd.data = [0.1 * e + 0.02 * de for e, de in zip(error, self.prev_error - error)]\n        \n        self.joint_pub.publish(cmd)\n        self.prev_error = error\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = MorphologicalComputation()\n    rclpy.spin(controller)\n    controller.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"lab-3-sensorimotor-learning",children:"Lab 3: Sensorimotor Learning"}),"\n",(0,o.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Implement a basic sensorimotor learning system where a robot learns to interact with its environment through trial and error."}),"\n",(0,o.jsx)(n.h3,{id:"setup-2",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Create a simulation environment with a simple manipulator arm"}),"\n",(0,o.jsx)(n.li,{children:"Configure the environment to provide a reward signal based on task performance"}),"\n",(0,o.jsx)(n.li,{children:"Set up sensors (position, force, tactile)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-2",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a simple neural network controller for the robot"}),"\n",(0,o.jsx)(n.li,{children:"Use a reinforcement learning algorithm (such as Q-learning or SARSA) to learn a reaching task"}),"\n",(0,o.jsx)(n.li,{children:"Compare learning with and without different sensory modalities"}),"\n",(0,o.jsx)(n.li,{children:"Analyze how the sensorimotor loop affects learning"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"analysis-2",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Plot learning curves showing how performance improves over time"}),"\n",(0,o.jsx)(n.li,{children:"Compare the effectiveness of different sensory inputs for learning"}),"\n",(0,o.jsx)(n.li,{children:"Discuss how the physical embodiment affects the learning process"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"code-template-2",children:"Code Template"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, PointCloud2\nfrom geometry_msgs.msg import Twist\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nclass SensorimotorNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nclass SensorimotorLearner(Node):\n    def __init__(self):\n        super().__init__('sensorimotor_learner')\n        self.subscription = self.create_subscription(\n            JointState, 'joint_states', self.joint_callback, 10)\n        \n        self.reward_sub = self.create_subscription(\n            Twist, 'reward_signal', self.reward_callback, 10)\n        \n        self.pub = self.create_publisher(JointState, 'joint_commands', 10)\n        \n        # Neural network and learning parameters\n        self.network = SensorimotorNetwork(input_size=10, hidden_size=20, output_size=3)\n        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=0.001)\n        self.criterion = nn.MSELoss()\n        \n        self.state_memory = []\n        self.action_memory = []\n        self.reward_memory = []\n        \n    def joint_callback(self, msg):\n        # Create state vector from joint states and other sensors\n        state = np.array([\n            msg.position[0], msg.position[1], msg.position[2],\n            msg.velocity[0], msg.velocity[1], msg.velocity[2],\n            # Add other sensor values\n        ])\n        \n        # Convert to tensor and get action\n        state_tensor = torch.tensor(state, dtype=torch.float32)\n        action = self.network(state_tensor)\n        \n        # Convert action to ROS message and publish\n        cmd = JointState()\n        cmd.position = [0.0, float(action[0]), float(action[1]), float(action[2])]\n        self.pub.publish(cmd)\n        \n        # Store for learning\n        self.state_memory.append(state_tensor)\n        self.action_memory.append(action)\n        \n    def reward_callback(self, msg):\n        # Store reward for learning\n        self.reward_memory.append(msg.linear.x)  # Assuming reward in x component\n\ndef main(args=None):\n    rclpy.init(args=args)\n    learner = SensorimotorLearner()\n    \n    # Training loop (in practice, this would run periodically)\n    # for episode in range(1000):\n    #     # Training code here\n    #     pass\n    \n    rclpy.spin(learner)\n    learner.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"lab-4-balance-and-dynamic-control",children:"Lab 4: Balance and Dynamic Control"}),"\n",(0,o.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Implement a simple balance controller for a bipedal robot, demonstrating the principles of dynamic balance using embodied intelligence."}),"\n",(0,o.jsx)(n.h3,{id:"setup-3",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Load a simplified bipedal robot model in simulation"}),"\n",(0,o.jsx)(n.li,{children:"Configure sensors for balance (IMU, joint encoders, force/torque sensors)"}),"\n",(0,o.jsx)(n.li,{children:"Set up a challenge course with small obstacles or uneven terrain"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps-3",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a basic balance controller using the inverted pendulum model"}),"\n",(0,o.jsx)(n.li,{children:"Use sensor feedback to maintain the Zero Moment Point (ZMP) within the support polygon"}),"\n",(0,o.jsx)(n.li,{children:"Implement basic stepping strategies to maintain balance when disturbed"}),"\n",(0,o.jsx)(n.li,{children:"Test the controller with different disturbances (pushes, uneven terrain)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"analysis-3",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Analyze the relationship between sensor feedback and balance maintenance"}),"\n",(0,o.jsx)(n.li,{children:"Document the robot's response to different types of disturbances"}),"\n",(0,o.jsx)(n.li,{children:"Discuss how the physical embodiment (compliance, inertia, etc.) contributes to stability"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"code-template-3",children:"Code Template"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import Vector3\nimport numpy as np\n\nclass BalanceController(Node):\n    def __init__(self):\n        super().__init__('balance_controller')\n        \n        # Subscriptions\n        self.imu_sub = self.create_subscription(Imu, 'imu/data', self.imu_callback, 10)\n        self.joint_sub = self.create_subscription(JointState, 'joint_states', self.joint_callback, 10)\n        \n        # Publishers\n        self.cmd_pub = self.create_publisher(JointState, 'joint_commands', 10)\n        \n        # Balance control parameters\n        self.com_height = 0.8  # Center of mass height (meters)\n        self.gravity = 9.81\n        self.prev_error_x = 0.0\n        self.prev_error_y = 0.0\n        self.integral_error_x = 0.0\n        self.integral_error_y = 0.0\n        \n    def imu_callback(self, msg):\n        # Extract orientation and angular velocity\n        orientation = msg.orientation\n        angular_velocity = msg.angular_velocity\n        \n        # Calculate roll and pitch angles (simplified)\n        pitch = np.arcsin(2.0 * (orientation.w * orientation.y - orientation.z * orientation.x))\n        roll = np.arctan2(2.0 * (orientation.w * orientation.x + orientation.y * orientation.z), \n                         1.0 - 2.0 * (orientation.x**2 + orientation.y**2))\n        \n        # Calculate desired ZMP based on orientation\n        desired_zmp_x = -self.com_height / self.gravity * pitch\n        desired_zmp_y = -self.com_height / self.gravity * roll\n        \n        # Current ZMP (simplified - in a real system, you'd use force/torque sensors)\n        current_zmp_x = 0.0  # Would come from force sensors\n        current_zmp_y = 0.0  # Would come from force sensors\n        \n        # Control law using PID for ZMP tracking\n        error_x = desired_zmp_x - current_zmp_x\n        error_y = desired_zmp_y - current_zmp_y\n        \n        # PID control\n        kp = 100.0  # Proportional gain\n        ki = 1.0    # Integral gain\n        kd = 5.0    # Derivative gain\n        \n        self.integral_error_x += error_x\n        derivative_error_x = (error_x - self.prev_error_x)\n        \n        self.integral_error_y += error_y\n        derivative_error_y = (error_y - self.prev_error_y)\n        \n        control_output_x = kp * error_x + ki * self.integral_error_x + kd * derivative_error_x\n        control_output_y = kp * error_y + ki * self.integral_error_y + kd * derivative_error_y\n        \n        self.prev_error_x = error_x\n        self.prev_error_y = error_y\n        \n        # Generate joint commands based on control outputs\n        cmd = JointState()\n        # This is a simplified representation - in reality you'd map these to actual joint torques\n        # based on your robot's specific kinematics and dynamics\n        cmd.position = [0.0, control_output_x, control_output_y, 0.0]\n        self.cmd_pub.publish(cmd)\n\n    def joint_callback(self, msg):\n        # Process joint positions and velocities for balance control if needed\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = BalanceController()\n    rclpy.spin(controller)\n    controller.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"lab-report-requirements",children:"Lab Report Requirements"}),"\n",(0,o.jsx)(n.p,{children:"For each lab exercise, students must submit:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"A written report describing their approach and implementation"}),"\n",(0,o.jsx)(n.li,{children:"Code files with proper documentation"}),"\n",(0,o.jsx)(n.li,{children:"Analysis of results, including plots where appropriate"}),"\n",(0,o.jsx)(n.li,{children:"Discussion of how the experiments demonstrated embodied intelligence principles"}),"\n",(0,o.jsx)(n.li,{children:"Suggestions for improvements or extensions"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implementation quality and correctness (30%)"}),"\n",(0,o.jsx)(n.li,{children:"Understanding of embodied intelligence principles (30%)"}),"\n",(0,o.jsx)(n.li,{children:"Analysis and interpretation of results (25%)"}),"\n",(0,o.jsx)(n.li,{children:"Documentation and code quality (15%)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"lab-extension-ideas",children:"Lab Extension Ideas"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Advanced Embodiment:"})," Implement controllers that use more sophisticated morphological computation techniques"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Soft Robotics:"})," Experiment with soft-bodied robots and their control challenges"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Collective Embodiment:"})," Develop multiple agents that exhibit collective intelligence through embodiment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Learning Algorithms:"})," Implement more sophisticated learning algorithms for sensorimotor tasks"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-tips",children:"Troubleshooting Tips"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation Issues:"})," Ensure ROS 2 and Gazebo are properly installed and sourced"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Controller Instability:"})," Check gains in PID controllers and simulation time constants"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor Noise:"})," Implement basic filtering if sensor signals are too noisy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance:"})," Optimize algorithms if running slowly in simulation"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"references-and-further-reading",children:"References and Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Dudek, G., & Jenkin, M. (2010). Computational Principles of Mobile Robotics."}),"\n",(0,o.jsx)(n.li,{children:"Pfeifer, R., & Bongard, J. (2006). How the Body Shapes the Way We Think."}),"\n",(0,o.jsx)(n.li,{children:"Siciliano, B., & Khatib, O. (2016). Springer Handbook of Robotics."}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>l});var r=i(6540);const o={},t=r.createContext(o);function s(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);