"use strict";(globalThis.webpackChunkhumanoid_robotics_course=globalThis.webpackChunkhumanoid_robotics_course||[]).push([[5346],{4945:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-ai-robot-brain/practical-lab","title":"Module 4: Practical Lab - Isaac Platform for Perception and Control","description":"Lab Overview","source":"@site/docs/module-4-ai-robot-brain/practical-lab.md","sourceDirName":"module-4-ai-robot-brain","slug":"/module-4-ai-robot-brain/practical-lab","permalink":"/humanoid-robotics-course/docs/module-4-ai-robot-brain/practical-lab","draft":false,"unlisted":false,"editUrl":"https://github.com/RizSheik/humanoid-robotics-course/edit/main/docs/module-4-ai-robot-brain/practical-lab.md","tags":[],"version":"current","frontMatter":{}}');var s=i(4848),o=i(8453);const a={},r="Module 4: Practical Lab - Isaac Platform for Perception and Control",l={},c=[{value:"Lab Overview",id:"lab-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Required Software/Tools",id:"required-softwaretools",level:3},{value:"Lab Duration",id:"lab-duration",level:3},{value:"Lab 1: Isaac Sim Setup and AI Training Environment",id:"lab-1-isaac-sim-setup-and-ai-training-environment",level:2},{value:"Objective",id:"objective",level:3},{value:"Setup Instructions",id:"setup-instructions",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Code Template",id:"code-template",level:3},{value:"Analysis and Documentation",id:"analysis-and-documentation",level:3},{value:"Lab 2: Isaac ROS Perception Pipeline",id:"lab-2-isaac-ros-perception-pipeline",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Setup",id:"setup",level:3},{value:"Implementation Steps",id:"implementation-steps-1",level:3},{value:"Code Template",id:"code-template-1",level:3},{value:"Analysis and Documentation",id:"analysis-and-documentation-1",level:3},{value:"Lab 3: Isaac ROS Control Systems",id:"lab-3-isaac-ros-control-systems",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Setup",id:"setup-1",level:3},{value:"Implementation Steps",id:"implementation-steps-2",level:3},{value:"Code Template",id:"code-template-2",level:3},{value:"Analysis and Documentation",id:"analysis-and-documentation-2",level:3},{value:"Lab 4: AI Integration and Deployment",id:"lab-4-ai-integration-and-deployment",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Setup",id:"setup-2",level:3},{value:"Implementation Steps",id:"implementation-steps-3",level:3},{value:"Code Template",id:"code-template-3",level:3},{value:"Analysis and Documentation",id:"analysis-and-documentation-3",level:3},{value:"Lab Report Requirements",id:"lab-report-requirements",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Troubleshooting Tips",id:"troubleshooting-tips",level:2},{value:"Extensions and Advanced Challenges",id:"extensions-and-advanced-challenges",level:2},{value:"References and Further Reading",id:"references-and-further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-practical-lab---isaac-platform-for-perception-and-control",children:"Module 4: Practical Lab - Isaac Platform for Perception and Control"})}),"\n",(0,s.jsx)(n.h2,{id:"lab-overview",children:"Lab Overview"}),"\n",(0,s.jsx)(n.p,{children:"This practical lab provides hands-on experience with the NVIDIA Isaac Platform for implementing AI-powered perception and control systems. Students will work with Isaac Sim for AI training, Isaac ROS for perception and control pipelines, and integrate these into complete robotic systems. The lab emphasizes practical implementation of advanced AI techniques for real-world robotics applications."}),"\n",(0,s.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this lab, students will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up and configure Isaac Sim for AI training and validation"}),"\n",(0,s.jsx)(n.li,{children:"Implement perception pipelines using Isaac ROS packages"}),"\n",(0,s.jsx)(n.li,{children:"Design and deploy control systems with AI integration"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception and control components into complete robotic systems"}),"\n",(0,s.jsx)(n.li,{children:"Deploy and validate AI models on edge devices using Isaac Platform"}),"\n",(0,s.jsx)(n.li,{children:"Debug and optimize AI robotics systems"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"required-softwaretools",children:"Required Software/Tools"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NVIDIA Isaac Sim"}),": Version 2023.1 or later"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Packages"}),": Latest stable release"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Humble Hawksbill"}),": With Isaac ROS extensions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NVIDIA GPU"}),": With CUDA 11.4+ support (RTX 3070 or equivalent)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Python 3.11+"}),": For implementing custom nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Docker Environment"}),": For Isaac Sim and Isaac ROS deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac Navigation and Manipulation Apps"}),": Pre-built applications"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"lab-duration",children:"Lab Duration"}),"\n",(0,s.jsx)(n.p,{children:"This lab is designed for 18-20 hours of hands-on work, typically spread over 3-4 weeks with 6 hours per week."}),"\n",(0,s.jsx)(n.h2,{id:"lab-1-isaac-sim-setup-and-ai-training-environment",children:"Lab 1: Isaac Sim Setup and AI Training Environment"}),"\n",(0,s.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Set up Isaac Sim and create an AI training environment for a mobile robot navigation task."}),"\n",(0,s.jsx)(n.h3,{id:"setup-instructions",children:"Setup Instructions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Install Isaac Sim using Docker:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Pull Isaac Sim image\ndocker pull nvcr.io/nvidia/isaac-sim:2023.1.1\n\n# Run Isaac Sim container\ndocker run --gpus all -it --rm \\\n  --network=host \\\n  --env="DISPLAY" \\\n  --env="QT_X11_NO_MITSHM=1" \\\n  --volume="/tmp/.X11-unix:/tmp/.X11-unix:rw" \\\n  --volume="${PWD}/projects:/isaac-sim/projects" \\\n  --volume="${HOME}/.nvidia-omniverse:/root/.nvidia-omniverse" \\\n  --volume="${HOME}/.cache/ov:/root/.cache/ov" \\\n  --device=/dev/dri:/dev/dri \\\n  --name="isaac-sim" \\\n  nvcr.io/nvidia/isaac-sim:2023.1.1\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Verify Isaac Sim installation by launching:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./isaac-sim-launch.sh\n"})}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create a new Isaac Sim project"})," for mobile robot navigation:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use the Isaac Sim Project Wizard or manually create a new project"}),"\n",(0,s.jsx)(n.li,{children:"Select appropriate scene template (e.g., Office, Warehouse)"}),"\n",(0,s.jsx)(n.li,{children:"Configure physics properties with realistic parameters"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Set up robot model"})," (using Carter robot as example):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Add a differential drive robot to the scene"}),"\n",(0,s.jsx)(n.li,{children:"Configure robot with accurate kinematic and dynamic properties"}),"\n",(0,s.jsx)(n.li,{children:"Add sensors (camera, IMU, LIDAR) with realistic parameters"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Design training environments"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Create multiple environments with varying complexity"}),"\n",(0,s.jsx)(n.li,{children:"Include different lighting conditions and textures"}),"\n",(0,s.jsx)(n.li,{children:"Add dynamic objects for training robustness"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Generate synthetic training data"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement data collection pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Generate labeled perception data (images, depth, semantic segmentation)"}),"\n",(0,s.jsx)(n.li,{children:"Create navigation scenarios for RL training"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"code-template",children:"Code Template"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# lab1_isaac_sim_setup.py\nimport omni\nfrom pxr import Gf, Usd, UsdGeom\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.prims import RigidPrimView\nfrom omni.isaac.core.utils.prims import get_prim_at_path, create_prim\nfrom omni.isaac.sensor import _sensor as _sensor\nimport numpy as np\nimport carb\n\nclass Lab1IsaacSimEnvironment:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.robot = None\n        self.robot_name = "carter"\n        self.setup_complete = False\n        \n    def setup_environment(self):\n        """Set up the initial Isaac Sim environment"""\n        print("Setting up Isaac Sim environment...")\n        \n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n        \n        # Add lighting\n        self._add_lighting()\n        \n        # Load Carter robot\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            carb.log_error("Could not find Isaac Sim assets folder")\n            return\n        \n        # Add Carter robot\n        robot_path = f"/World/{self.robot_name}"\n        carter_path = assets_root_path + "/Isaac/Robots/Carter/carter_nucleus.usd"\n        add_reference_to_stage(usd_path=carter_path, prim_path=robot_path)\n        \n        # Create robot view for control\n        from omni.isaac.core.articulations import ArticulationView\n        self.robot = ArticulationView(\n            prim_path=robot_path,\n            name="carter_view",\n            reset_xform_properties=False,\n        )\n        self.world.scene.add(self.robot)\n        \n        # Add objects for interaction\n        self._add_training_objects()\n        \n        self.setup_complete = True\n        print("Environment setup complete!")\n    \n    def _add_lighting(self):\n        """Add realistic lighting to the environment"""\n        # Add dome light for ambient illumination\n        dome_light = create_prim(\n            prim_path="/World/DomeLight",\n            prim_type="DomeLight",\n            attributes={"color": (0.2, 0.2, 0.2), "intensity": 500}\n        )\n        \n        # Add distant light for shadows\n        distant_light = create_prim(\n            prim_path="/World/DistantLight",\n            prim_type="DistantLight", \n            attributes={\n                "color": (0.9, 0.9, 0.9), \n                "intensity": 500,\n                "angle": 0.5\n            }\n        )\n    \n    def _add_training_objects(self):\n        """Add objects for navigation training"""\n        from omni.isaac.core.objects import DynamicCuboid\n        \n        # Add navigation targets\n        target = DynamicCuboid(\n            prim_path="/World/Target",\n            name="target_cube",\n            position=np.array([5.0, 0.0, 0.2]),\n            size=np.array([0.2, 0.2, 0.2]),\n            color=np.array([1.0, 0.0, 0.0])  # Red\n        )\n        self.world.scene.add(target)\n        \n        # Add obstacles\n        obstacles = []\n        for i in range(5):\n            obstacle = DynamicCuboid(\n                prim_path=f"/World/Obstacle_{i}",\n                name=f"obstacle_{i}",\n                position=np.array([\n                    np.random.uniform(1.0, 4.0),\n                    np.random.uniform(-2.0, 2.0), \n                    0.2\n                ]),\n                size=np.array([0.3, 0.3, 0.4]),\n                color=np.array([0.5, 0.5, 0.5])  # Gray\n            )\n            self.world.scene.add(obstacle)\n            obstacles.append(obstacle)\n        \n        print(f"Added target and {len(obstacles)} obstacles for training")\n    \n    def generate_training_scenarios(self):\n        """Generate multiple training scenarios"""\n        print("Generating training scenarios...")\n        \n        # For each scenario, randomize:\n        # - Robot starting position\n        # - Target position\n        # - Obstacle positions\n        # - Lighting conditions\n        # - Floor textures\n        \n        scenarios = []\n        for i in range(20):  # Generate 20 different scenarios\n            scenario = self._create_scenario(f"scenario_{i}")\n            scenarios.append(scenario)\n        \n        print(f"Generated {len(scenarios)} training scenarios")\n        return scenarios\n    \n    def _create_scenario(self, name):\n        """Create a single training scenario"""\n        return {\n            "name": name,\n            "robot_start_pos": np.array([\n                np.random.uniform(-3.0, 3.0), \n                np.random.uniform(-3.0, 3.0), \n                0.5\n            ]),\n            "target_pos": np.array([\n                np.random.uniform(2.0, 6.0),\n                np.random.uniform(-2.0, 2.0),\n                0.2\n            ]),\n            "obstacles": [\n                {\n                    "pos": np.array([\n                        np.random.uniform(0.5, 5.0),\n                        np.random.uniform(-2.5, 2.5),\n                        0.2\n                    ]),\n                    "size": np.array([0.3, 0.3, 0.4])\n                } for _ in range(np.random.randint(3, 8))\n            ]\n        }\n    \n    def run_training_simulation(self, num_steps=1000, scenario=None):\n        """Run simulation for AI training"""\n        if not self.setup_complete:\n            print("Environment not set up. Run setup_environment first.")\n            return\n        \n        self.world.reset()\n        \n        if scenario:\n            self._apply_scenario(scenario)\n        \n        print(f"Running training simulation for {num_steps} steps...")\n        \n        for step in range(num_steps):\n            # In a real implementation, you would:\n            # 1. Get robot state and sensor data\n            # 2. Apply AI control policy\n            # 3. Calculate reward\n            # 4. Store experience for training\n            \n            # For this lab, we\'ll just step the simulation\n            self.world.step(render=True)\n            \n            if step % 100 == 0:\n                robot_pos, robot_orn = self.robot.get_world_poses()\n                print(f"Step {step}: Robot at {robot_pos[0]}")\n    \n    def _apply_scenario(self, scenario):\n        """Apply scenario configuration to environment"""\n        # Move robot to start position\n        self.robot.set_world_poses(positions=scenario["robot_start_pos"].reshape(1, 3))\n        \n        # Move target to position (this would be implemented based on your scene setup)\n        target_prim = get_prim_at_path("/World/Target")\n        # Implementation to move target would go here\n        \n        # Move obstacles\n        for i, obstacle_data in enumerate(scenario["obstacles"]):\n            obstacle_prim = get_prim_at_path(f"/World/Obstacle_{i}")\n            if obstacle_prim.IsValid():\n                # Move obstacle to position (implementation depends on your setup)\n                pass\n    \n    def cleanup(self):\n        """Clean up the environment"""\n        if hasattr(self, \'world\'):\n            self.world.clear()\n        print("Environment cleaned up.")\n\ndef main():\n    """Main entry point for Lab 1"""\n    env = Lab1IsaacSimEnvironment()\n    \n    try:\n        # Set up the environment\n        env.setup_environment()\n        \n        # Generate training scenarios\n        scenarios = env.generate_training_scenarios()\n        \n        # Run a short simulation with a scenario\n        if scenarios:\n            env.run_training_simulation(num_steps=500, scenario=scenarios[0])\n        \n        print("Lab 1 completed successfully!")\n        \n    except Exception as e:\n        print(f"Error in Lab 1: {e}")\n        carb.log_error(f"Error in Lab 1: {e}")\n    finally:\n        env.cleanup()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"analysis-and-documentation",children:"Analysis and Documentation"}),"\n",(0,s.jsx)(n.p,{children:"Document your results in the lab report:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Verify Isaac Sim installation and basic functionality"}),"\n",(0,s.jsx)(n.li,{children:"Record the configuration of your robot model (sensors, actuator limits, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Document the variety of training scenarios created"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the realism of your simulation environment"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-2-isaac-ros-perception-pipeline",children:"Lab 2: Isaac ROS Perception Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Implement a perception pipeline using Isaac ROS packages for object detection and environment understanding."}),"\n",(0,s.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Verify Isaac ROS installation:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check if Isaac ROS packages are installed\nros2 pkg list | grep isaac\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Source the Isaac ROS overlay:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash\nsource /usr/local/share/isaac_ros_common/setup.bash\n"})}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps-1",children:"Implementation Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Set up camera calibration"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use Isaac ROS Image Pipeline for camera calibration"}),"\n",(0,s.jsx)(n.li,{children:"Implement rectification and undistortion"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implement object detection"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use Isaac ROS DNN Inference for object detection"}),"\n",(0,s.jsx)(n.li,{children:"Configure model for your specific objects"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Create sensor fusion"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combine data from camera, IMU, and LIDAR"}),"\n",(0,s.jsx)(n.li,{children:"Implement spatial and temporal alignment"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implement tracking"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Create object tracking pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Associate detections across frames"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"code-template-1",children:"Code Template"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# lab2_isaac_ros_perception.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, Imu, LaserScan\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom geometry_msgs.msg import Pose, Point\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\nimport numpy as np\nimport cv2\nimport torch\n\nclass Lab2IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('lab2_isaac_perception_pipeline')\n        \n        # Initialize CV bridge\n        self.bridge = CvBridge()\n        \n        # Publishers\n        self.detection_pub = self.create_publisher(Detection2DArray, '/detections', 10)\n        self.visualization_pub = self.create_publisher(Image, '/detection_visualization', 10)\n        \n        # Subscribers with approximate time synchronization\n        self.image_sub = Subscriber(self, Image, '/camera/image_raw')\n        self.camera_info_sub = Subscriber(self, CameraInfo, '/camera/camera_info') \n        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\n        \n        # Synchronize image and camera info\n        self.ts = ApproximateTimeSynchronizer(\n            [self.image_sub, self.camera_info_sub], \n            queue_size=10, \n            slop=0.1\n        )\n        self.ts.registerCallback(self.camera_callback)\n        \n        # Internal state\n        self.latest_imu = None\n        self.latest_camera_info = None\n        self.detection_model = None  # Will be loaded later\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        \n        # Initialize detection model\n        self._load_detection_model()\n        \n        self.get_logger().info('Isaac ROS Perception Pipeline initialized')\n\n    def _load_detection_model(self):\n        \"\"\"Load object detection model\"\"\"\n        # In a real implementation, you would load a TensorRT-optimized model\n        # For this lab, we'll create a simple placeholder\n        self.get_logger().info('Loading object detection model...')\n        \n        # Create a simple detection model (in practice, use Isaac ROS DNN Inference)\n        # self.detection_model = torch_tensorrt.compile(loaded_model, ...)  \n        \n    def camera_callback(self, image_msg, camera_info_msg):\n        \"\"\"Process synchronized camera image and info\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\n            \n            # Update camera parameters\n            self.camera_matrix = np.array(camera_info_msg.k).reshape(3, 3)\n            self.distortion_coeffs = np.array(camera_info_msg.d)\n            \n            # Run object detection\n            detections = self._run_object_detection(cv_image)\n            \n            # Create and publish detections\n            detection_array_msg = self._create_detection_array_msg(detections, image_msg.header)\n            self.detection_pub.publish(detection_array_msg)\n            \n            # Create and publish visualization\n            vis_image = self._create_visualization(cv_image, detections)\n            vis_msg = self.bridge.cv2_to_imgmsg(vis_image, encoding='bgr8')\n            vis_msg.header = image_msg.header\n            self.visualization_pub.publish(vis_msg)\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing camera data: {str(e)}')\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data\"\"\"\n        self.latest_imu = msg\n        \n        # In a real system, you might use IMU data for:\n        # - Sensor fusion\n        # - Motion compensation\n        # - State estimation\n        self.get_logger().debug(f'IMU data received: linear_acceleration=({msg.linear_acceleration.x:.2f}, {msg.linear_acceleration.y:.2f}, {msg.linear_acceleration.z:.2f})')\n\n    def _run_object_detection(self, image):\n        \"\"\"Run object detection on image using Isaac ROS DNN Inference\"\"\"\n        # In real Isaac ROS, this would call the actual detection node\n        # For this example, we'll simulate detection\n        \n        # Convert image to tensor (in real implementation, use Isaac ROS)\n        height, width = image.shape[:2]\n        \n        # Simulate detection results\n        # In practice, you'd run the actual model\n        simulated_detections = [\n            {\n                'class_id': 0,\n                'class_name': 'person',\n                'confidence': 0.85,\n                'bbox': [int(width*0.3), int(height*0.2), int(width*0.2), int(height*0.4)]  # [x, y, w, h]\n            },\n            {\n                'class_id': 1,\n                'class_name': 'chair',\n                'confidence': 0.72,\n                'bbox': [int(width*0.6), int(height*0.4), int(width*0.25), int(height*0.3)]\n            }\n        ]\n        \n        return simulated_detections\n\n    def _create_detection_array_msg(self, detections, header):\n        \"\"\"Create Detection2DArray message from detection results\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n        \n        for detection in detections:\n            if detection['confidence'] > 0.5:  # Confidence threshold\n                detection_msg = Detection2D()\n                \n                # Bounding box center and size\n                bbox = detection['bbox']\n                detection_msg.bbox.center.x = bbox[0] + bbox[2] / 2  # center_x\n                detection_msg.bbox.center.y = bbox[1] + bbox[3] / 2  # center_y\n                detection_msg.bbox.size_x = bbox[2]  # width\n                detection_msg.bbox.size_y = bbox[3]  # height\n                \n                # Classification result\n                from vision_msgs.msg import ObjectHypothesisWithPose\n                hypothesis = ObjectHypothesisWithPose()\n                hypothesis.hypothesis.class_id = str(detection['class_id'])\n                hypothesis.hypothesis.score = detection['confidence']\n                \n                detection_msg.results.append(hypothesis)\n                detection_array.detections.append(detection_msg)\n        \n        return detection_array\n\n    def _create_visualization(self, image, detections):\n        \"\"\"Create visualization of detections on image\"\"\"\n        vis_image = image.copy()\n        \n        for detection in detections:\n            if detection['confidence'] > 0.5:  # Only visualize confident detections\n                bbox = detection['bbox']\n                x, y, w, h = bbox\n                \n                # Draw bounding box\n                start_point = (x, y)\n                end_point = (x + w, y + h)\n                color = (0, 255, 0)  # Green\n                thickness = 2\n                cv2.rectangle(vis_image, start_point, end_point, color, thickness)\n                \n                # Draw label and confidence\n                label = f\"{detection['class_name']}: {detection['confidence']:.2f}\"\n                label_position = (x, y - 10)\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                font_scale = 0.6\n                cv2.putText(vis_image, label, label_position, font, font_scale, color, 1)\n        \n        return vis_image\n\nclass IsaacPerceptionValidator(Node):\n    \"\"\"Validate perception pipeline performance\"\"\"\n    def __init__(self):\n        super().__init__('isaac_perception_validator')\n        \n        # Subscribe to perception output\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.detection_validation_callback, 10\n        )\n        \n        # Performance metrics\n        self.detection_counts = []\n        self.processing_times = []\n        self.confidence_scores = []\n        \n        # Timer for periodic evaluation\n        self.eval_timer = self.create_timer(5.0, self.periodic_evaluation)\n        \n        self.get_logger().info('Perception Validator initialized')\n\n    def detection_validation_callback(self, msg):\n        \"\"\"Validate incoming detections\"\"\"\n        import time\n        start_time = time.time()\n        \n        # Count detections\n        num_detections = len(msg.detections)\n        self.detection_counts.append(num_detections)\n        \n        # Collect confidence scores\n        for detection in msg.detections:\n            for result in detection.results:\n                self.confidence_scores.append(result.hypothesis.score)\n        \n        # Calculate processing time\n        processing_time = time.time() - start_time\n        self.processing_times.append(processing_time)\n        \n        self.get_logger().debug(f'Detections received: {num_detections}, Processing time: {processing_time:.4f}s')\n\n    def periodic_evaluation(self):\n        \"\"\"Periodically evaluate perception performance\"\"\"\n        if len(self.detection_counts) == 0:\n            self.get_logger().info('No detections received yet')\n            return\n        \n        # Calculate metrics\n        avg_detections = sum(self.detection_counts) / len(self.detection_counts)\n        avg_processing_time = sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0\n        avg_confidence = sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0\n        \n        self.get_logger().info(\n            f'Perception Performance - Avg Detections: {avg_detections:.2f}, '\n            f'Avg Processing Time: {avg_processing_time:.4f}s, '\n            f'Avg Confidence: {avg_confidence:.4f}'\n        )\n        \n        # Reset for next period\n        self.detection_counts = []\n        self.processing_times = []\n        self.confidence_scores = []\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    # Create perception pipeline and validator\n    perception_pipeline = Lab2IsaacPerceptionPipeline()\n    validator = IsaacPerceptionValidator()\n    \n    try:\n        # Combine both nodes in a single executor\n        executor = rclpy.executors.SingleThreadedExecutor()\n        executor.add_node(perception_pipeline)\n        executor.add_node(validator)\n        \n        executor.spin()\n    except KeyboardInterrupt:\n        perception_pipeline.get_logger().info('Shutting down Isaac Perception Pipeline')\n    finally:\n        perception_pipeline.destroy_node()\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"analysis-and-documentation-1",children:"Analysis and Documentation"}),"\n",(0,s.jsx)(n.p,{children:"Document your results:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Measure detection accuracy and performance"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the quality of sensor fusion"}),"\n",(0,s.jsx)(n.li,{children:"Assess real-time performance (FPS, latency)"}),"\n",(0,s.jsx)(n.li,{children:"Document any calibration or alignment issues"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-3-isaac-ros-control-systems",children:"Lab 3: Isaac ROS Control Systems"}),"\n",(0,s.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Implement AI-powered control systems using Isaac ROS for navigation and manipulation tasks."}),"\n",(0,s.jsx)(n.h3,{id:"setup-1",children:"Setup"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure Isaac ROS navigation and manipulation packages are installed:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo apt-get install ros-humble-isaac-ros-nav2-benchmarks\nsudo apt-get install ros-humble-isaac-ros-manipulation\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Set up robot control interface:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Source ROS and Isaac ROS\nsource /opt/ros/humble/setup.bash\nsource /usr/local/share/isaac_ros_common/setup.bash\n"})}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps-2",children:"Implementation Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implement navigation stack"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure Isaac ROS Navigation with GPU acceleration"}),"\n",(0,s.jsx)(n.li,{children:"Set up costmaps and planners"}),"\n",(0,s.jsx)(n.li,{children:"Implement dynamic obstacle avoidance"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Integrate AI for navigation"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use Isaac ROS Perception output for navigation decisions"}),"\n",(0,s.jsx)(n.li,{children:"Implement learning-based planning"}),"\n",(0,s.jsx)(n.li,{children:"Add semantic navigation capabilities"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Set up manipulation control"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement Isaac ROS manipulation pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Integrate perception for object recognition and pose estimation"}),"\n",(0,s.jsx)(n.li,{children:"Create grasping and manipulation policies"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implement high-level control"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Create task planning system"}),"\n",(0,s.jsx)(n.li,{children:"Integrate multiple AI components"}),"\n",(0,s.jsx)(n.li,{children:"Implement human-robot interaction"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"code-template-2",children:"Code Template"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# lab3_isaac_control_systems.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport tf2_ros\n\nclass IsaacNavigationController(Node):\n    def __init__(self):\n        super().__init__('isaac_navigation_controller')\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)\n        \n        # Subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10)\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.detection_callback, 10)\n        \n        # TF2 broadcaster and listener\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n        \n        # Robot state\n        self.current_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta\n        self.current_velocity = np.array([0.0, 0.0])   # linear, angular\n        self.target_pose = np.array([0.0, 0.0, 0.0])\n        self.navigation_state = 'idle'  # idle, navigating, avoiding\n        self.detected_objects = []\n        \n        # Control parameters\n        self.linear_gain = 1.0\n        self.angular_gain = 2.0\n        self.collision_threshold = 0.5  # meters\n        self.arrival_threshold = 0.2    # meters\n        \n        # Timer for control loop\n        self.control_timer = self.create_timer(0.05, self.control_loop)  # 20 Hz\n        \n        self.get_logger().info('Isaac Navigation Controller initialized')\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot pose from odometry\"\"\"\n        # Extract position\n        self.current_pose[0] = msg.pose.pose.position.x\n        self.current_pose[1] = msg.pose.pose.position.y\n        \n        # Extract orientation\n        quat = msg.pose.pose.orientation\n        rot = R.from_quat([quat.x, quat.y, quat.z, quat.w])\n        euler = rot.as_euler('xyz')\n        self.current_pose[2] = euler[2]  # Only care about yaw\n        \n        # Extract velocity\n        self.current_velocity[0] = msg.twist.twist.linear.x\n        self.current_velocity[1] = msg.twist.twist.angular.z\n\n    def detection_callback(self, msg):\n        \"\"\"Process object detections\"\"\"\n        self.detected_objects = []\n        \n        for detection in msg.detections:\n            # In a real implementation, you would need to convert 2D detections to 3D\n            # using depth information or other methods\n            \n            obj_info = {\n                'class': detection.results[0].hypothesis.class_id if detection.results else 'unknown',\n                'confidence': detection.results[0].hypothesis.score if detection.results else 0.0,\n                'bbox_center': (detection.bbox.center.x, detection.bbox.center.y),\n                'bbox_size': (detection.bbox.size_x, detection.bbox.size_y)\n            }\n            \n            self.detected_objects.append(obj_info)\n\n    def set_navigation_goal(self, x, y, theta=0.0):\n        \"\"\"Set navigation goal\"\"\"\n        self.target_pose = np.array([x, y, theta])\n        self.navigation_state = 'navigating'\n        self.get_logger().info(f'Navigation goal set: ({x}, {y}, {theta})')\n\n    def control_loop(self):\n        \"\"\"Main navigation control loop\"\"\"\n        if self.navigation_state == 'idle':\n            # Stop the robot\n            self._stop_robot()\n            return\n        \n        if self.navigation_state == 'navigating':\n            # Check for obstacles\n            if self._detect_collision_risk():\n                self.navigation_state = 'avoiding'\n                self.get_logger().info('Collision risk detected, switching to avoidance mode')\n            else:\n                # Calculate navigation commands\n                cmd_vel = self._compute_navigation_command()\n                self.cmd_vel_pub.publish(cmd_vel)\n                \n                # Check if arrived\n                distance = np.linalg.norm(self.current_pose[:2] - self.target_pose[:2])\n                if distance < self.arrival_threshold:\n                    self.navigation_state = 'arrived'\n                    self._stop_robot()\n                    self.get_logger().info('Destination reached!')\n\n    def _detect_collision_risk(self):\n        \"\"\"Detect collision risk based on object detections\"\"\"\n        for obj in self.detected_objects:\n            if obj['confidence'] > 0.7:  # High confidence detection\n                # This is simplified - in real implementation, you'd use depth data\n                # to estimate distance to obstacles\n                if obj['class'] in ['person', 'obstacle', 'furniture']:\n                    # Assume obstacle is close enough to be risky\n                    return True\n        return False\n\n    def _compute_navigation_command(self):\n        \"\"\"Compute navigation command to reach target\"\"\"\n        cmd = Twist()\n        \n        # Calculate direction to target\n        direction = self.target_pose[:2] - self.current_pose[:2]\n        distance = np.linalg.norm(direction)\n        \n        if distance > 0.1:  # If not already at target\n            # Normalize direction\n            direction_norm = direction / distance\n            \n            # Calculate desired heading\n            desired_theta = np.arctan2(direction[1], direction[0])\n            \n            # Calculate heading error\n            heading_error = desired_theta - self.current_pose[2]\n            \n            # Normalize angle to [-\u03c0, \u03c0]\n            while heading_error > np.pi:\n                heading_error -= 2 * np.pi\n            while heading_error < -np.pi:\n                heading_error += 2 * np.pi\n            \n            # Proportional control\n            cmd.linear.x = min(0.5, distance * self.linear_gain)  # Cap speed\n            cmd.angular.z = heading_error * self.angular_gain\n            \n            # Adjust for robot kinematics if needed\n            # For differential drive robots, adjust angular velocity based on linear velocity\n            cmd.angular.z = max(min(cmd.angular.z, 1.0), -1.0)  # Cap angular velocity\n        else:\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n        \n        return cmd\n\n    def _stop_robot(self):\n        \"\"\"Stop the robot\"\"\"\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd)\n\nclass IsaacManipulationController(Node):\n    def __init__(self):\n        super().__init__('isaac_manipulation_controller')\n        \n        # Publishers and subscribers for manipulation\n        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.object_detection_callback, 10)\n        \n        # Manipulation state\n        self.detected_objects = []\n        self.grasp_target = None\n        self.manipulation_state = 'idle'  # idle, planning_grasp, executing_grasp, holding\n        \n        # Timer for manipulation control\n        self.manip_timer = self.create_timer(0.1, self.manipulation_loop)\n        \n        self.get_logger().info('Isaac Manipulation Controller initialized')\n\n    def object_detection_callback(self, msg):\n        \"\"\"Process object detections for manipulation\"\"\"\n        # Process detections for graspable objects\n        graspable_objects = []\n        \n        for detection in msg.detections:\n            # Check if this is a graspable object (based on class, size, etc.)\n            if detection.results and detection.results[0].hypothesis.score > 0.8:\n                class_id = detection.results[0].hypothesis.class_id\n                \n                # For now, assume anything that's not background could be graspable\n                if class_id in ['0', '1', '2']:  # These would be object categories\n                    graspable_objects.append({\n                        'bbox': detection.bbox,\n                        'class': class_id,\n                        'confidence': detection.results[0].hypothesis.score\n                    })\n        \n        self.detected_objects = graspable_objects\n\n    def manipulation_loop(self):\n        \"\"\"Main manipulation control loop\"\"\"\n        if self.manipulation_state == 'idle':\n            # Look for graspable objects\n            if self.detected_objects:\n                # Select highest confidence object to grasp\n                best_obj = max(self.detected_objects, key=lambda o: o['confidence'])\n                self.grasp_target = best_obj\n                self.manipulation_state = 'planning_grasp'\n                self.get_logger().info(f'Planning grasp for object {best_obj[\"class\"]}')\n        \n        elif self.manipulation_state == 'planning_grasp':\n            # In a real implementation, plan grasp trajectory\n            # For this lab, just transition to execution\n            self.manipulation_state = 'executing_grasp'\n            self.get_logger().info('Executing grasp plan')\n            \n        elif self.manipulation_state == 'executing_grasp':\n            # Execute grasp (in real implementation)\n            # For this lab, just transition to holding\n            self.manipulation_state = 'holding'\n            self.get_logger().info('Object grasped successfully')\n            \n        elif self.manipulation_state == 'holding':\n            # Hold object until commanded otherwise\n            pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    # Create navigation and manipulation controllers\n    nav_controller = IsaacNavigationController()\n    manip_controller = IsaacManipulationController()\n    \n    try:\n        # Set a navigation goal for testing\n        nav_controller.set_navigation_goal(2.0, 2.0)\n        \n        # Combine nodes in single executor\n        executor = rclpy.executors.SingleThreadedExecutor()\n        executor.add_node(nav_controller)\n        executor.add_node(manip_controller)\n        \n        executor.spin()\n    except KeyboardInterrupt:\n        nav_controller.get_logger().info('Shutting down Isaac Control Systems')\n    finally:\n        nav_controller.destroy_node()\n        manip_controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"analysis-and-documentation-2",children:"Analysis and Documentation"}),"\n",(0,s.jsx)(n.p,{children:"Document your results:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Measure navigation performance (accuracy, time to goal, collision avoidance)"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate manipulation success rates"}),"\n",(0,s.jsx)(n.li,{children:"Assess integration between perception and control"}),"\n",(0,s.jsx)(n.li,{children:"Document any tuning of control parameters needed"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-4-ai-integration-and-deployment",children:"Lab 4: AI Integration and Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,s.jsx)(n.p,{children:"Deploy and integrate the AI systems developed in previous labs onto an edge device, simulating real-world deployment conditions."}),"\n",(0,s.jsx)(n.h3,{id:"setup-2",children:"Setup"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Jetson or other edge device:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS on Jetson\nsudo apt-get update\nsudo apt-get install ros-humble-isaac-ros-common\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Prepare models for deployment:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Convert PyTorch models to TensorRT (simplified)\npython3 -c \"\nimport torch\nimport torch_tensorrt\n\n# Load your trained model\nmodel = torch.load('model.pth')\nmodel.eval()\n\n# Trace the model\ntraced_model = torch.jit.trace(model, torch.randn(1, 3, 224, 224))\n\n# Compile with TensorRT\ncompiled_model = torch_tensorrt.compile(\n    traced_model,\n    inputs=[torch_tensorrt.Input((1, 3, 224, 224))],\n    enabled_precisions={torch.float, torch.half},\n    refit_enabled=True,\n    debug=False\n)\n\n# Save compiled model\ntorch.jit.save(compiled_model, 'compiled_model.ts')\n\"\n"})}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps-3",children:"Implementation Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Optimize models for edge deployment"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Apply quantization"}),"\n",(0,s.jsx)(n.li,{children:"Optimize model architecture for real-time performance"}),"\n",(0,s.jsx)(n.li,{children:"Test on edge hardware"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implement deployment pipeline"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Create Docker containers for edge deployment"}),"\n",(0,s.jsx)(n.li,{children:"Set up remote monitoring and logging"}),"\n",(0,s.jsx)(n.li,{children:"Implement automatic model updates"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Validate deployment"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Test performance in edge environment"}),"\n",(0,s.jsx)(n.li,{children:"Validate accuracy is maintained"}),"\n",(0,s.jsx)(n.li,{children:"Monitor resource usage"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implement fallback mechanisms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Create safety fallbacks for AI failures"}),"\n",(0,s.jsx)(n.li,{children:"Implement graceful degradation"}),"\n",(0,s.jsx)(n.li,{children:"Add manual override capabilities"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"code-template-3",children:"Code Template"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# lab4_ai_deployment.py\nimport torch\nimport numpy as np\nimport time\nimport threading\nfrom collections import deque\nfrom dataclasses import dataclass\nimport json\nimport logging\n\n@dataclass\nclass DeploymentConfig:\n    """Configuration for AI model deployment"""\n    model_path: str\n    input_shape: tuple\n    output_shape: tuple\n    batch_size: int = 1\n    precision: str = \'fp16\'  # fp32, fp16, int8\n    max_latency: float = 0.1  # seconds\n    min_throughput: float = 10  # FPS\n\nclass ModelOptimizer:\n    def __init__(self, config: DeploymentConfig):\n        self.config = config\n        self.original_model = None\n        self.optimized_model = None\n    \n    def load_model(self):\n        """Load the original model"""\n        self.original_model = torch.load(self.config.model_path)\n        self.original_model.eval()\n        print(f"Loaded model from {self.config.model_path}")\n    \n    def optimize_for_jetson(self):\n        """Optimize model for Jetson deployment using TensorRT"""\n        import torch_tensorrt\n        \n        # Trace the model\n        dummy_input = torch.randn(self.config.batch_size, *self.config.input_shape[1:])\n        traced_model = torch.jit.trace(self.original_model, dummy_input)\n        \n        # Determine precision\n        if self.config.precision == \'fp16\':\n            precision_set = {torch.half}\n        else:\n            precision_set = {torch.float}\n        \n        # Compile with TensorRT\n        self.optimized_model = torch_tensorrt.compile(\n            traced_model,\n            inputs=[torch_tensorrt.Input(\n                shape=[self.config.batch_size, *self.config.input_shape[1:]]\n            )],\n            enabled_precisions=precision_set,\n            refit_enabled=True,\n            debug=False\n        )\n        \n        print(f"Model optimized for {self.config.precision} precision")\n        return self.optimized_model\n    \n    def quantize_model(self):\n        """Apply post-training quantization"""\n        # Create quantizable version of model\n        quantized_model = torch.quantization.quantize_dynamic(\n            self.original_model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8\n        )\n        self.optimized_model = quantized_model\n        print("Model quantized to INT8")\n        return quantized_model\n\nclass EdgeDeploymentManager:\n    def __init__(self, config: DeploymentConfig):\n        self.config = config\n        self.optimizer = ModelOptimizer(config)\n        self.model = None\n        self.is_ready = False\n        \n        # Performance monitoring\n        self.inference_times = deque(maxlen=100)\n        self.throughput_history = deque(maxlen=50)\n        \n        # Logging\n        self.logger = self._setup_logger()\n    \n    def _setup_logger(self):\n        """Setup performance logger"""\n        logging.basicConfig(level=logging.INFO)\n        logger = logging.getLogger(\'EdgeDeploy\')\n        \n        # File handler for persistent logs\n        file_handler = logging.FileHandler(\'edge_deployment.log\')\n        file_handler.setLevel(logging.DEBUG)\n        \n        # Console handler\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n        \n        # Formatter\n        formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n        file_handler.setFormatter(formatter)\n        console_handler.setFormatter(formatter)\n        \n        logger.addHandler(file_handler)\n        logger.addHandler(console_handler)\n        \n        return logger\n    \n    def deploy_model(self):\n        """Deploy model to edge device"""\n        try:\n            # Load and optimize model\n            self.optimizer.load_model()\n            self.model = self.optimizer.optimize_for_jetson()\n            \n            # Validate model performance\n            self._validate_performance()\n            \n            # Check if meets requirements\n            if self._meets_requirements():\n                self.is_ready = True\n                self.logger.info("Model successfully deployed and validated")\n                return True\n            else:\n                self.logger.error("Model does not meet deployment requirements")\n                return False\n                \n        except Exception as e:\n            self.logger.error(f"Deployment failed: {str(e)}")\n            return False\n    \n    def _validate_performance(self):\n        """Validate model performance on edge device"""\n        # Run multiple inference iterations to measure performance\n        test_inputs = torch.randn(100, *self.config.input_shape[1:])\n        \n        inference_times = []\n        for i in range(100):\n            start_time = time.time()\n            with torch.no_grad():\n                _ = self.model(test_inputs[i:i+1])\n            end_time = time.time()\n            \n            inference_time = end_time - start_time\n            inference_times.append(inference_time)\n            \n            # Update performance metrics\n            self.inference_times.append(inference_time)\n            self.throughput_history.append(1.0 / inference_time if inference_time > 0 else 0)\n        \n        avg_inference_time = np.mean(inference_times)\n        avg_throughput = np.mean(list(self.throughput_history)[-20:])  # Last 20 measurements\n        \n        self.logger.info(f"Performance validation: avg_inference_time={avg_inference_time:.4f}s, "\n                        f"avg_throughput={avg_throughput:.2f} FPS")\n    \n    def _meets_requirements(self):\n        """Check if model meets deployment requirements"""\n        if len(self.inference_times) == 0:\n            return False\n        \n        avg_inference_time = np.mean(list(self.inference_times)[-20:])  # Last 20 measurements\n        \n        meets_latency = avg_inference_time <= self.config.max_latency\n        avg_throughput = np.mean(list(self.throughput_history)[-20:]) if self.throughput_history else 0\n        meets_throughput = avg_throughput >= self.config.min_throughput\n        \n        return meets_latency and meets_throughput\n    \n    def run_inference(self, input_tensor):\n        """Run optimized inference on edge device"""\n        if not self.is_ready:\n            raise RuntimeError("Model not ready for inference")\n        \n        start_time = time.time()\n        with torch.no_grad():\n            output = self.model(input_tensor)\n        end_time = time.time()\n        \n        inference_time = end_time - start_time\n        self.inference_times.append(inference_time)\n        \n        # Log slow inference times\n        if inference_time > self.config.max_latency:\n            self.logger.warning(f"Slow inference: {inference_time:.4f}s (threshold: {self.config.max_latency}s)")\n        \n        return output\n    \n    def get_performance_stats(self):\n        """Get performance statistics"""\n        if len(self.inference_times) == 0:\n            return {\n                \'average_inference_time\': 0.0,\n                \'average_throughput\': 0.0,\n                \'latency_percentiles\': [0.0, 0.0],\n                \'last_10_avg\': 0.0\n            }\n        \n        times_list = list(self.inference_times)\n        avg_time = np.mean(times_list)\n        avg_throughput = np.mean(list(self.throughput_history)) if self.throughput_history else 0\n        \n        p50 = np.percentile(times_list, 50) if len(times_list) > 1 else avg_time\n        p95 = np.percentile(times_list, 95) if len(times_list) > 1 else avg_time\n        \n        last_10_avg = np.mean(times_list[-10:]) if len(times_list) >= 10 else avg_time\n        \n        return {\n            \'average_inference_time\': avg_time,\n            \'average_throughput\': avg_throughput,\n            \'latency_percentiles\': [float(p50), float(p95)],\n            \'last_10_avg\': last_10_avg\n        }\n\nclass AIIntegrationValidator:\n    def __init__(self, deployment_manager: EdgeDeploymentManager):\n        self.deployment_manager = deployment_manager\n        self.validation_results = {}\n        \n    def validate_integration(self):\n        """Validate AI system integration"""\n        try:\n            # Test model with edge-specific inputs\n            dummy_input = torch.randn(1, *self.deployment_manager.config.input_shape[1:])\n            \n            # Run inference\n            start_time = time.time()\n            output = self.deployment_manager.run_inference(dummy_input)\n            end_time = time.time()\n            \n            # Validate output shape\n            expected_shape = (1, *self.deployment_manager.config.output_shape[1:])\n            actual_shape = tuple(output.shape)\n            \n            if actual_shape == expected_shape:\n                validation_passed = True\n                validation_message = f"Output shape validation passed: {actual_shape}"\n            else:\n                validation_passed = False\n                validation_message = f"Output shape mismatch: expected {expected_shape}, got {actual_shape}"\n            \n            # Performance validation\n            inference_time = end_time - start_time\n            latency_met = inference_time <= self.deployment_manager.config.max_latency\n            \n            # Store validation results\n            self.validation_results = {\n                \'passed\': validation_passed,\n                \'message\': validation_message,\n                \'inference_time\': inference_time,\n                \'latency_requirement_met\': latency_met,\n                \'timestamp\': time.time()\n            }\n            \n            return self.validation_results\n            \n        except Exception as e:\n            self.validation_results = {\n                \'passed\': False,\n                \'message\': f"Validation failed with error: {str(e)}",\n                \'timestamp\': time.time()\n            }\n            return self.validation_results\n\ndef run_deployment_lab():\n    """Run the AI deployment lab"""\n    print("Starting AI Deployment Lab...")\n    \n    # Create deployment configuration\n    config = DeploymentConfig(\n        model_path="./models/perception_model.pth",\n        input_shape=(1, 3, 224, 224),\n        output_shape=(1, 10),  # Example: 10 object classes\n        batch_size=1,\n        precision=\'fp16\',\n        max_latency=0.05,  # 50ms\n        min_throughput=15  # 15 FPS\n    )\n    \n    # Create and start deployment manager\n    deploy_manager = EdgeDeploymentManager(config)\n    \n    # Deploy model to edge device\n    if deploy_manager.deploy_model():\n        print("Model deployed successfully!")\n        \n        # Validate integration\n        validator = AIIntegrationValidator(deploy_manager)\n        validation_results = validator.validate_integration()\n        \n        print(f"Integration validation: {validation_results[\'message\']}")\n        print(f"Inference time: {validation_results[\'inference_time\']:.4f}s")\n        print(f"Latency requirement met: {validation_results[\'latency_requirement_met\']}")\n        \n        # Get performance statistics\n        perf_stats = deploy_manager.get_performance_stats()\n        print(f"Performance Stats: {perf_stats}")\n        \n        # Simulate running inference for a while\n        print("\\nRunning inference simulation...")\n        for i in range(10):\n            dummy_input = torch.randn(1, *config.input_shape[1:])\n            try:\n                output = deploy_manager.run_inference(dummy_input)\n                print(f"Step {i+1}: Inference completed, output shape: {tuple(output.shape)}")\n            except Exception as e:\n                print(f"Step {i+1}: Inference failed: {str(e)}")\n            time.sleep(0.1)  # Small delay to simulate real-time operation\n        \n    else:\n        print("Model deployment failed!")\n\nif __name__ == "__main__":\n    run_deployment_lab()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"analysis-and-documentation-3",children:"Analysis and Documentation"}),"\n",(0,s.jsx)(n.p,{children:"Document your results:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Measure inference performance (latency, throughput)"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate resource usage (CPU, GPU, memory)"}),"\n",(0,s.jsx)(n.li,{children:"Assess accuracy preservation after optimization"}),"\n",(0,s.jsx)(n.li,{children:"Document deployment process and any challenges"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lab-report-requirements",children:"Lab Report Requirements"}),"\n",(0,s.jsx)(n.p,{children:"For each lab exercise, students must submit:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implementation Documentation"})," (25%):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Complete code with proper documentation"}),"\n",(0,s.jsx)(n.li,{children:"Explanation of design decisions"}),"\n",(0,s.jsx)(n.li,{children:"Configuration files and environment setup"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance Analysis"})," (40%):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Measurements of latency, throughput, and accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Analysis of resource utilization"}),"\n",(0,s.jsx)(n.li,{children:"Comparison to baseline implementations"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Integration Report"})," (25%):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How different Isaac components work together"}),"\n",(0,s.jsx)(n.li,{children:"Challenges encountered and solutions"}),"\n",(0,s.jsx)(n.li,{children:"Recommendations for improvements"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Reflection and Learning"})," (10%):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"What was learned about Isaac Platform"}),"\n",(0,s.jsx)(n.li,{children:"How concepts apply to real-world robotics"}),"\n",(0,s.jsx)(n.li,{children:"Future directions for improvement"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implementation quality and correctness (40%)"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of Isaac Platform components (30%)"}),"\n",(0,s.jsx)(n.li,{children:"Performance analysis and optimization (20%)"}),"\n",(0,s.jsx)(n.li,{children:"Documentation and code quality (10%)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-tips",children:"Troubleshooting Tips"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac Sim Issues"}),": Check Omniverse connection and GPU drivers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS Integration"}),": Verify proper namespace and topic mapping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Issues"}),": Monitor GPU utilization and memory usage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Deployment"}),": Validate TensorRT compatibility with hardware"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Calibration Problems"}),": Re-run sensor calibration procedures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Synchronization Issues"}),": Check message timestamps and queue sizes"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"extensions-and-advanced-challenges",children:"Extensions and Advanced Challenges"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-Robot Coordination"}),": Extend to multi-robot systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Perception"}),": Implement 3D object detection and tracking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning Systems"}),": Add reinforcement learning for navigation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-Robot Interaction"}),": Implement gesture recognition interfaces"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cloud Integration"}),": Connect with cloud AI services for heavy computation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Systems"}),": Implement comprehensive safety monitoring"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references-and-further-reading",children:"References and Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["NVIDIA Isaac Sim Documentation: ",(0,s.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/",children:"https://docs.omniverse.nvidia.com/isaacsim/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Isaac ROS Documentation: ",(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/ros/",children:"https://docs.nvidia.com/isaac/ros/"})]}),"\n",(0,s.jsx)(n.li,{children:"Isaac Navigation Documentation"}),"\n",(0,s.jsx)(n.li,{children:"Isaac Manipulation Documentation"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 Documentation for reference implementations"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);