"use strict";(globalThis.webpackChunkhumanoid_robotics_course=globalThis.webpackChunkhumanoid_robotics_course||[]).push([[745],{6573:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module-3-digital-twin-simulation/simulation","title":"Module 3: Simulation - Digital Twin Environments for Robotics","description":"Simulation Overview","source":"@site/docs/module-3-digital-twin-simulation/simulation.md","sourceDirName":"module-3-digital-twin-simulation","slug":"/module-3-digital-twin-simulation/simulation","permalink":"/humanoid-robotics-course/docs/module-3-digital-twin-simulation/simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/RizSheik/humanoid-robotics-course/edit/main/docs/module-3-digital-twin-simulation/simulation.md","tags":[],"version":"current","frontMatter":{}}');var t=i(4848),o=i(8453);const r={},s="Module 3: Simulation - Digital Twin Environments for Robotics",l={},m=[{value:"Simulation Overview",id:"simulation-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Required Simulation Tools",id:"required-simulation-tools",level:3},{value:"Gazebo Simulation Environment",id:"gazebo-simulation-environment",level:2},{value:"Gazebo Configuration and Setup",id:"gazebo-configuration-and-setup",level:3},{value:"Robot Model Configuration in Gazebo",id:"robot-model-configuration-in-gazebo",level:3},{value:"Gazebo Simulation Implementation",id:"gazebo-simulation-implementation",level:3},{value:"Unity Simulation Environment",id:"unity-simulation-environment",level:2},{value:"Unity Scene Configuration",id:"unity-scene-configuration",level:3},{value:"Advanced Unity Perception Simulation",id:"advanced-unity-perception-simulation",level:3},{value:"Isaac Sim Simulation Environment",id:"isaac-sim-simulation-environment",level:2},{value:"Isaac Sim Robot Configuration",id:"isaac-sim-robot-configuration",level:3},{value:"Advanced Simulation Techniques",id:"advanced-simulation-techniques",level:2},{value:"Domain Randomization Implementation",id:"domain-randomization-implementation",level:3},{value:"System Identification for Simulation Calibration",id:"system-identification-for-simulation-calibration",level:3},{value:"Simulation Validation and Performance Analysis",id:"simulation-validation-and-performance-analysis",level:2},{value:"Simulation Validation Techniques",id:"simulation-validation-techniques",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Advanced Exercises",id:"advanced-exercises",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"module-3-simulation---digital-twin-environments-for-robotics",children:"Module 3: Simulation - Digital Twin Environments for Robotics"})}),"\n",(0,t.jsx)(e.h2,{id:"simulation-overview",children:"Simulation Overview"}),"\n",(0,t.jsx)(e.p,{children:"This simulation module focuses on implementing and testing digital twin simulation environments for robotics using three major platforms: Gazebo, Unity, and Isaac Sim. Students will gain practical experience with creating realistic simulation environments, implementing advanced simulation techniques, and validating simulation models against real-world behavior."}),"\n",(0,t.jsx)(e.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"After completing this simulation module, students will be able to:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement and configure simulation environments in Gazebo, Unity, and Isaac Sim"}),"\n",(0,t.jsx)(e.li,{children:"Create realistic robot models with accurate physics and sensor simulation"}),"\n",(0,t.jsx)(e.li,{children:"Apply advanced simulation techniques like domain randomization and system identification"}),"\n",(0,t.jsx)(e.li,{children:"Validate simulation models and evaluate sim-to-real transfer potential"}),"\n",(0,t.jsx)(e.li,{children:"Optimize simulation performance and realism for specific applications"}),"\n",(0,t.jsx)(e.li,{children:"Design simulation experiments that accelerate robotics development"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"required-simulation-tools",children:"Required Simulation Tools"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gazebo Harmonic"})," or later with ROS 2 integration"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Unity Hub"})," with Unity 2021.3 LTS and robotics packages"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"NVIDIA Isaac Sim"})," with Omniverse platform"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 Humble Hawksbill"})," for robotics communication"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Python 3.11+"})," and ",(0,t.jsx)(e.strong,{children:"C++17"})," for custom implementations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Docker"})," for Isaac Sim container deployment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"RViz2"})," and ",(0,t.jsx)(e.strong,{children:"rqt"})," for visualization and debugging"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"gazebo-simulation-environment",children:"Gazebo Simulation Environment"}),"\n",(0,t.jsx)(e.h3,{id:"gazebo-configuration-and-setup",children:"Gazebo Configuration and Setup"}),"\n",(0,t.jsx)(e.p,{children:"For Gazebo-based simulations, proper configuration is essential for achieving realistic physics and sensor simulation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Example Gazebo world configuration with realistic physics --\x3e\n<sdf version="1.7">\n  <world name="realistic_world">\n    \x3c!-- Physics engine configuration --\x3e\n    <physics type="dartsim">\n      <max_step_size>0.001</max_step_size>        \x3c!-- Time step: 1ms (1000Hz) --\x3e\n      <real_time_factor>1.0</real_time_factor>    \x3c!-- Real-time simulation --\x3e\n      <real_time_update_rate>1000</real_time_update_rate>\n      \n      <dart>\n        <solver>\n          <type>PGS</type>\n          <iterations>100</iterations>\n          <collision_filter_mode>all</collision_filter_mode>\n        </solver>\n        <constraints>\n          <contact_surface_layer>0.001</contact_surface_layer>\n          <contact_max_correcting_vel>100.0</contact_max_correcting_vel>\n        </constraints>\n        <collision_detector>bullet</collision_detector>\n        <impact_capture_model>\n          <type>None</type>\n        </impact_capture_model>\n        <soft_body_solver>\n          <velocity_damping>0.0</velocity_damping>\n          <position_damping>1.0</position_damping>\n          <drift_correction>1.0</drift_correction>\n          <cluster_stiffness>1e+06</cluster_stiffness>\n        </soft_body_solver>\n      </dart>\n    </physics>\n\n    \x3c!-- Include a realistic ground plane --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n      <pose>0 0 0 0 0 0</pose>\n    </include>\n\n    \x3c!-- Add lighting for realistic rendering --\x3e\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    \x3c!-- Add ambient light --\x3e\n    <light type="directional" name="ambient_light">\n      <cast_shadows>false</cast_shadows>\n      <pose>0 0 10 0 0 0</pose>\n      <diffuse>0.8 0.8 0.8 1</diffuse>\n      <specular>0.2 0.2 0.2 1</specular>\n      <attenuation>\n        <range>100</range>\n        <constant>0.9</constant>\n        <linear>0.01</linear>\n        <quadratic>0.001</quadratic>\n      </attenuation>\n    </light>\n    \n    \x3c!-- Include robot model --\x3e\n    <include>\n      <name>mobile_robot</name>\n      <uri>model://differential_drive_robot</uri>\n      <pose>0 0 0.5 0 0 0</pose>\n    </include>\n\n    \x3c!-- Add objects for interaction --\x3e\n    <model name="table">\n      <pose>2 2 0 0 0 0</pose>\n      <link name="table_link">\n        <pose>0 0 0.4 0 0 0</pose>\n        <collision name="collision">\n          <geometry>\n            <box><size>1.0 0.6 0.8</size></box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box><size>1.0 0.6 0.8</size></box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.6 0.4 1</ambient>\n            <diffuse>0.8 0.6 0.4 1</diffuse>\n          </material>\n        </visual>\n        <inertial>\n          <mass>10.0</mass>\n          <inertia><ixx>1</ixx><ixy>0</ixy><ixz>0</ixz><iyy>1</iyy><iyz>0</iyz><izz>1</izz></inertia>\n        </inertial>\n      </link>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"robot-model-configuration-in-gazebo",children:"Robot Model Configuration in Gazebo"}),"\n",(0,t.jsx)(e.p,{children:"Creating realistic robot models for simulation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Advanced robot model with realistic sensors and actuators --\x3e\n<sdf version="1.7">\n  <model name="advanced_robot">\n    \x3c!-- Base chassis --\x3e\n    <link name="base_link">\n      <inertial>\n        <mass>20.0</mass>\n        <inertia>\n          <ixx>1.0</ixx><ixy>0.0</ixy><ixz>0.0</ixz>\n          <iyy>1.5</iyy><iyz>0.0</iyz><izz>2.0</izz>\n        </inertia>\n      </inertial>\n      \n      <collision name="collision">\n        <geometry><box><size>0.8 0.5 0.3</size></box></geometry>\n      </collision>\n      \n      <visual name="visual">\n        <geometry><mesh><uri>model://robot/meshes/base.dae</uri></mesh></geometry>\n        <material>\n          <ambient>0.1 0.1 0.8 1</ambient>\n          <diffuse>0.1 0.1 0.8 1</diffuse>\n        </material>\n      </visual>\n    </link>\n\n    \x3c!-- Add realistic sensors --\x3e\n    <sensor name="camera_front" type="camera">\n      <pose>0.3 0 0.2 0 0 0</pose>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n        <image>\n          <width>800</width>\n          <height>600</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>30.0</far>\n        </clip>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.007</stddev>\n        </noise>\n      </camera>\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <visualize>true</visualize>\n    </sensor>\n\n    \x3c!-- IMU sensor --\x3e\n    <sensor name="imu_sensor" type="imu">\n      <pose>0 0 0.1 0 0 0</pose>\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n    </sensor>\n\n    \x3c!-- LIDAR sensor --\x3e\n    <sensor name="3d_lidar" type="ray">\n      <pose>0.2 0 0.4 0 0 0</pose>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>720</samples>\n            <resolution>1</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n          <vertical>\n            <samples>16</samples>\n            <resolution>1</resolution>\n            <min_angle>-0.2618</min_angle> \x3c!-- -15 degrees --\x3e\n            <max_angle>0.2618</max_angle>   \x3c!-- 15 degrees --\x3e\n          </vertical>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <always_on>true</always_on>\n      <update_rate>10</update_rate>\n      <visualize>true</visualize>\n    </sensor>\n\n    \x3c!-- ROS 2 integration --\x3e\n    <plugin filename="libignition-gazebo-diff-drive-system.so" name="ignition::gazebo::systems::DiffDrive">\n      <left_joint>left_wheel_joint</left_joint>\n      <right_joint>right_wheel_joint</right_joint>\n      <wheel_separation>0.4</wheel_separation>\n      <wheel_radius>0.15</wheel_radius>\n      <odom_publish_frequency>30</odom_publish_frequency>\n      <topic>cmd_vel</topic>\n      <odom_topic>odom</odom_topic>\n      <tf_topic>tf</tf_topic>\n    </plugin>\n\n    <plugin filename="libignition-gazebo-joint-state-publisher-system.so" name="ignition::gazebo::systems::JointStatePublisher">\n      <ros>\n        <namespace>/robot1</namespace>\n        <remapping>joint_states:=/robot1/joint_states</remapping>\n      </ros>\n    </plugin>\n  </model>\n</sdf>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"gazebo-simulation-implementation",children:"Gazebo Simulation Implementation"}),"\n",(0,t.jsx)(e.p,{children:"Implementing simulation with advanced physics and control:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n# Advanced Gazebo simulation implementation for robotics\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, Pose, Point\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan, Image, Imu, JointState\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\n\nclass GazeboRobotSimulator(Node):\n    def __init__(self):\n        super().__init__(\'gazebo_robot_simulator\')\n        \n        # Initialize CV bridge\n        self.bridge = CvBridge()\n        \n        # Robot state variables\n        self.x = 0.0\n        self.y = 0.0\n        self.theta = 0.0\n        self.linear_velocity = 0.0\n        self.angular_velocity = 0.0\n        \n        # Robot physical parameters\n        self.wheel_separation = 0.4\n        self.wheel_radius = 0.15\n        self.robot_radius = 0.25\n        \n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, \'odom\', 10)\n        self.laser_pub = self.create_publisher(LaserScan, \'scan\', 10)\n        self.imu_pub = self.create_publisher(Imu, \'imu/data\', 10)\n        self.camera_pub = self.create_publisher(Image, \'camera/image_raw\', 10)\n        \n        # Subscribers\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'cmd_vel\', self.cmd_vel_callback, 10)\n        \n        # Transform broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n        \n        # Create timer for simulation update\n        self.timer = self.create_timer(0.01, self.update_simulation)  # 100Hz\n        \n        # Initialize random number generator for sensor noise\n        self.rng = np.random.default_rng(seed=42)\n        \n        self.get_logger().info(\'Gazebo Robot Simulator initialized\')\n\n    def cmd_vel_callback(self, msg):\n        """Handle velocity commands"""\n        self.linear_velocity = msg.linear.x\n        self.angular_velocity = msg.angular.z\n\n    def update_simulation(self):\n        """Update robot state based on physics model"""\n        dt = 0.01  # Time step based on timer frequency\n        \n        # Update robot pose using differential drive kinematics\n        dx = self.linear_velocity * math.cos(self.theta) * dt\n        dy = self.linear_velocity * math.sin(self.theta) * dt\n        dtheta = self.angular_velocity * dt\n        \n        self.x += dx\n        self.y += dy\n        self.theta += dtheta\n        \n        # Normalize theta to [-pi, pi]\n        self.theta = math.atan2(math.sin(self.theta), math.cos(self.theta))\n        \n        # Publish odometry\n        self.publish_odometry()\n        \n        # Publish laser scan with simulated environment\n        self.publish_laser_scan()\n        \n        # Publish IMU data with simulated noise\n        self.publish_imu_data()\n        \n        # Publish camera image\n        self.publish_camera_image()\n        \n        # Publish transform\n        self.publish_transform()\n\n    def publish_odometry(self):\n        """Publish odometry data"""\n        odom = Odometry()\n        odom.header = Header()\n        odom.header.stamp = self.get_clock().now().to_msg()\n        odom.header.frame_id = \'odom\'\n        odom.child_frame_id = \'base_link\'\n        \n        # Position\n        odom.pose.pose.position.x = self.x\n        odom.pose.pose.position.y = self.y\n        odom.pose.pose.position.z = 0.0\n        \n        # Orientation (convert from angle to quaternion)\n        from scipy.spatial.transform import Rotation as R\n        quat = R.from_euler(\'z\', self.theta).as_quat()\n        odom.pose.pose.orientation.x = quat[0]\n        odom.pose.pose.orientation.y = quat[1]\n        odom.pose.pose.orientation.z = quat[2]\n        odom.pose.pose.orientation.w = quat[3]\n        \n        # Velocity\n        odom.twist.twist.linear.x = self.linear_velocity\n        odom.twist.twist.angular.z = self.angular_velocity\n        \n        self.odom_pub.publish(odom)\n\n    def publish_laser_scan(self):\n        """Publish simulated laser scan"""\n        scan = LaserScan()\n        scan.header = Header()\n        scan.header.stamp = self.get_clock().now().to_msg()\n        scan.header.frame_id = \'laser_link\'\n        \n        # Laser parameters\n        scan.angle_min = -math.pi\n        scan.angle_max = math.pi\n        scan.angle_increment = 2 * math.pi / 720  # 720 rays\n        scan.time_increment = 0.0\n        scan.scan_time = 0.1  # 10Hz\n        scan.range_min = 0.1\n        scan.range_max = 30.0\n        \n        # Generate simulated ranges based on environment\n        ranges = []\n        for i in range(720):\n            angle = scan.angle_min + i * scan.angle_increment\n            \n            # Simulate a simple environment with walls and objects\n            # In a real implementation, this would be based on the scene geometry\n            distance = self.simulate_laser_ray(angle)\n            \n            # Add realistic noise\n            noise = self.rng.normal(0, 0.02)  # 2cm noise\n            distance_with_noise = max(scan.range_min, min(scan.range_max, distance + noise))\n            \n            ranges.append(distance_with_noise)\n        \n        scan.ranges = ranges\n        scan.intensities = []  # No intensity information\n        \n        self.laser_pub.publish(scan)\n\n    def simulate_laser_ray(self, angle):\n        """Simulate a laser ray in the environment"""\n        # For this simulation, create a simple environment\n        # with walls at x = -5, x = 5, y = -5, y = 5\n        robot_x = self.x\n        robot_y = self.y\n        robot_angle = self.theta\n        \n        # Global angle of the laser beam\n        global_angle = robot_angle + angle\n        \n        # Calculate intersection with walls\n        # Wall at x = -5\n        t_x_neg = float(\'inf\')\n        if math.cos(global_angle) != 0:\n            t_x_neg = (-5 - robot_x) / math.cos(global_angle)\n        if t_x_neg > 0:\n            y_at_x_neg = robot_y + t_x_neg * math.sin(global_angle)\n            if -5 <= y_at_x_neg <= 5:\n                return t_x_neg\n        \n        # Wall at x = 5\n        t_x_pos = float(\'inf\')\n        if math.cos(global_angle) != 0:\n            t_x_pos = (5 - robot_x) / math.cos(global_angle)\n        if t_x_pos > 0:\n            y_at_x_pos = robot_y + t_x_pos * math.sin(global_angle)\n            if -5 <= y_at_x_pos <= 5:\n                return t_x_pos\n        \n        # Wall at y = -5\n        t_y_neg = float(\'inf\')\n        if math.sin(global_angle) != 0:\n            t_y_neg = (-5 - robot_y) / math.sin(global_angle)\n        if t_y_neg > 0:\n            x_at_y_neg = robot_x + t_y_neg * math.cos(global_angle)\n            if -5 <= x_at_y_neg <= 5:\n                return t_y_neg\n        \n        # Wall at y = 5\n        t_y_pos = float(\'inf\')\n        if math.sin(global_angle) != 0:\n            t_y_pos = (5 - robot_y) / math.sin(global_angle)\n        if t_y_pos > 0:\n            x_at_y_pos = robot_x + t_y_pos * math.cos(global_angle)\n            if -5 <= x_at_y_pos <= 5:\n                return t_y_pos\n        \n        # Return max range if no intersection\n        return 30.0\n\n    def publish_imu_data(self):\n        """Publish simulated IMU data"""\n        imu = Imu()\n        imu.header = Header()\n        imu.header.stamp = self.get_clock().now().to_msg()\n        imu.header.frame_id = \'imu_link\'\n        \n        # Simulate IMU readings with realistic noise\n        # Linear acceleration (simulating gravity and movement)\n        imu.linear_acceleration.x = self.rng.normal(0, 0.05)\n        imu.linear_acceleration.y = self.rng.normal(0, 0.05)\n        imu.linear_acceleration.z = 9.81 + self.rng.normal(0, 0.05)\n        \n        # Angular velocity (including current rotation and noise)\n        imu.angular_velocity.x = self.rng.normal(0, 0.001)\n        imu.angular_velocity.y = self.rng.normal(0, 0.001)\n        imu.angular_velocity.z = self.angular_velocity + self.rng.normal(0, 0.005)\n        \n        # Orientation (simplified - in real implementation should be integrated)\n        from scipy.spatial.transform import Rotation as R\n        quat = R.from_euler(\'z\', self.theta).as_quat()\n        imu.orientation.x = quat[0] + self.rng.normal(0, 0.001)\n        imu.orientation.y = quat[1] + self.rng.normal(0, 0.001)\n        imu.orientation.z = quat[2] + self.rng.normal(0, 0.001)\n        imu.orientation.w = quat[3] + self.rng.normal(0, 0.001)\n        \n        # Normalize quaternion\n        norm = np.linalg.norm([imu.orientation.x, imu.orientation.y, \n                              imu.orientation.z, imu.orientation.w])\n        imu.orientation.x /= norm\n        imu.orientation.y /= norm\n        imu.orientation.z /= norm\n        imu.orientation.w /= norm\n        \n        # Set covariance values (information about measurement uncertainty)\n        imu.orientation_covariance[0] = 0.01\n        imu.orientation_covariance[4] = 0.01\n        imu.orientation_covariance[8] = 0.01\n        imu.angular_velocity_covariance[0] = 0.01\n        imu.angular_velocity_covariance[4] = 0.01\n        imu.angular_velocity_covariance[8] = 0.01\n        imu.linear_acceleration_covariance[0] = 0.01\n        imu.linear_acceleration_covariance[4] = 0.01\n        imu.linear_acceleration_covariance[8] = 0.01\n        \n        self.imu_pub.publish(imu)\n\n    def publish_camera_image(self):\n        """Publish simulated camera image"""\n        # Create a synthetic image with simulated environment\n        height, width = 480, 640\n        img = np.zeros((height, width, 3), dtype=np.uint8)\n        \n        # Add elements to simulate environment\n        # Center of image corresponds to robot\'s forward direction\n        center_x, center_y = width // 2, height // 2\n        \n        # Add simulated obstacles based on position and orientation\n        # For this example, let\'s just add some static features\n        cv2.rectangle(img, (center_x-50, center_y-100), (center_x+50, center_y-80), (100, 100, 100), -1)  # Ground feature\n        cv2.circle(img, (center_x + int(100*math.cos(self.theta)), center_y + int(100*math.sin(self.theta))), 20, (0, 255, 0), -1)  # Green obstacle\n        \n        # Add simulated camera distortion and noise\n        img = self.add_camera_effects(img)\n        \n        # Convert to ROS message\n        img_msg = self.bridge.cv2_to_imgmsg(img, encoding="bgr8")\n        img_msg.header.stamp = self.get_clock().now().to_msg()\n        img_msg.header.frame_id = \'camera_link\'\n        \n        self.camera_pub.publish(img_msg)\n\n    def add_camera_effects(self, img):\n        """Add realistic camera effects like noise and distortion"""\n        # Add noise\n        noise = self.rng.normal(0, 15, img.shape).astype(np.int16)\n        img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n        \n        return img\n\n    def publish_transform(self):\n        """Publish TF transform"""\n        t = TransformStamped()\n        \n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = \'odom\'\n        t.child_frame_id = \'base_link\'\n        \n        t.transform.translation.x = self.x\n        t.transform.translation.y = self.y\n        t.transform.translation.z = 0.0\n        \n        from scipy.spatial.transform import Rotation as R\n        quat = R.from_euler(\'z\', self.theta).as_quat()\n        t.transform.rotation.x = quat[0]\n        t.transform.rotation.y = quat[1]\n        t.transform.rotation.z = quat[2]\n        t.transform.rotation.w = quat[3]\n        \n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    simulator = GazeboRobotSimulator()\n    \n    try:\n        rclpy.spin(simulator)\n    except KeyboardInterrupt:\n        simulator.get_logger().info(\'Simulation stopped by user\')\n    finally:\n        simulator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    import cv2  # Required for image processing\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"unity-simulation-environment",children:"Unity Simulation Environment"}),"\n",(0,t.jsx)(e.h3,{id:"unity-scene-configuration",children:"Unity Scene Configuration"}),"\n",(0,t.jsx)(e.p,{children:"Configuring Unity for advanced robotics simulation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'// UnityRobotSimulation.cs\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Geometry;\nusing RosMessageTypes.Nav;\nusing System.Collections;\n\npublic class UnityRobotSimulation : MonoBehaviour\n{\n    [Header("Robot Configuration")]\n    public GameObject robotBase;\n    public WheelCollider[] wheelColliders;\n    public GameObject[] wheelVisuals;\n    public Transform cameraMount;\n    public float maxMotorTorque = 100f;\n    public float maxSteeringAngle = 45f;\n\n    [Header("Physics Configuration")]\n    public float robotMass = 20f;\n    public float wheelRadius = 0.3f;\n    public float wheelWidth = 0.1f;\n\n    [Header("ROS Connection")]\n    public string rosIP = "127.0.0.1";\n    public int rosPort = 10000;\n    public string cmdVelTopic = "/cmd_vel";\n    public string odomTopic = "/odom";\n    public string laserTopic = "/scan";\n\n    [Header("Simulation Configuration")]\n    public float simulationSpeed = 1.0f;\n    public int physicsIterations = 10;\n    public int velocityIterations = 10;\n\n    private ROSConnection ros;\n    private Rigidbody robotRigidbody;\n    private float motorTorque = 0f;\n    private float steering = 0f;\n\n    void Start()\n    {\n        // Configure physics parameters\n        ConfigurePhysics();\n        \n        // Initialize ROS connection\n        InitializeROS();\n        \n        // Initialize robot components\n        InitializeRobot();\n    }\n\n    void ConfigurePhysics()\n    {\n        // Set physics parameters for accurate simulation\n        Physics.defaultSolverIterations = physicsIterations;\n        Physics.defaultSolverVelocityIterations = velocityIterations;\n        Physics.sleepThreshold = 0.001f;\n        Physics.defaultContactOffset = 0.01f;\n        Physics.bounceThreshold = 2f;\n    }\n\n    void InitializeROS()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Initialize(rosIP, rosPort);\n        \n        // Subscribe to command topics\n        ros.Subscribe<TwistMsg>(cmdVelTopic, ReceiveVelocityCommand);\n    }\n\n    void InitializeRobot()\n    {\n        robotRigidbody = GetComponent<Rigidbody>();\n        if (robotRigidbody == null)\n        {\n            robotRigidbody = gameObject.AddComponent<Rigidbody>();\n        }\n        \n        robotRigidbody.mass = robotMass;\n        robotRigidbody.drag = 0.1f;\n        robotRigidbody.angularDrag = 0.2f;\n        robotRigidbody.interpolation = RigidbodyInterpolation.Interpolate;\n    }\n\n    void ReceiveVelocityCommand(TwistMsg cmd)\n    {\n        // Convert ROS velocity command to Unity controls\n        // Linear x becomes forward movement\n        motorTorque = cmd.linear.x * maxMotorTorque;\n        \n        // Angular z becomes steering\n        steering = -cmd.angular.z * maxSteeringAngle;  // Negative for correct direction\n    }\n\n    void Update()\n    {\n        // Update wheel rotations for visual representation\n        UpdateWheelVisuals();\n    }\n\n    void FixedUpdate()\n    {\n        // Apply physics-based movement\n        ApplyWheelForces();\n        \n        // Publish odometry and sensor data\n        PublishSensorData();\n    }\n\n    void ApplyWheelForces()\n    {\n        for (int i = 0; i < wheelColliders.Length; i++)\n        {\n            if (i < 2) // Front wheels - steering\n            {\n                wheelColliders[i].steerAngle = steering;\n            }\n            \n            if (i >= 2) // Rear wheels - motor\n            {\n                wheelColliders[i].motorTorque = motorTorque;\n            }\n            \n            // Apply handbrake effect if needed\n            wheelColliders[i].brakeTorque = 0f;\n        }\n    }\n\n    void UpdateWheelVisuals()\n    {\n        for (int i = 0; i < wheelColliders.Length && i < wheelVisuals.Length; i++)\n        {\n            Quaternion q;\n            Vector3 p;\n            \n            wheelColliders[i].GetWorldPose(out p, out q);\n            \n            // Update visual wheel positions and rotations\n            wheelVisuals[i].transform.position = p;\n            wheelVisuals[i].transform.rotation = q;\n        }\n    }\n\n    void PublishSensorData()\n    {\n        // Publish odometry data\n        PublishOdometry();\n        \n        // In a complete implementation, also publish laser data, IMU, etc.\n    }\n\n    void PublishOdometry()\n    {\n        var odom = new OdometryMsg();\n        odom.header = new std_msgs.HeaderMsg();\n        odom.header.stamp = new TimeMsg();\n        odom.header.frame_id = "odom";\n        odom.child_frame_id = "base_link";\n        \n        // Position\n        odom.pose.pose.position = new Vector3Msg(\n            transform.position.x,\n            transform.position.y,\n            transform.position.z\n        );\n        \n        // Orientation\n        odom.pose.pose.orientation = new QuaternionMsg(\n            transform.rotation.x,\n            transform.rotation.y,\n            transform.rotation.z,\n            transform.rotation.w\n        );\n        \n        // Linear velocity\n        odom.twist.twist.linear = new Vector3Msg(\n            robotRigidbody.velocity.x,\n            robotRigidbody.velocity.y,\n            robotRigidbody.velocity.z\n        );\n        \n        // Angular velocity\n        odom.twist.twist.angular = new Vector3Msg(\n            robotRigidbody.angularVelocity.x,\n            robotRigidbody.angularVelocity.y,\n            robotRigidbody.angularVelocity.z\n        );\n        \n        ros.Publish(odomTopic, odom);\n    }\n\n    // Visualization methods for debugging\n    void OnValidate()\n    {\n        if (wheelColliders.Length != wheelVisuals.Length)\n        {\n            Debug.LogWarning("Number of wheel colliders doesn\'t match number of wheel visuals");\n        }\n    }\n\n    void OnDrawGizmos()\n    {\n        // Draw robot coordinate system\n        Gizmos.color = Color.red;\n        Gizmos.DrawLine(transform.position, transform.position + transform.right * 0.5f);\n        \n        Gizmos.color = Color.green;\n        Gizmos.DrawLine(transform.position, transform.position + transform.up * 0.5f);\n        \n        Gizmos.color = Color.blue;\n        Gizmos.DrawLine(transform.position, transform.position + transform.forward * 0.5f);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"advanced-unity-perception-simulation",children:"Advanced Unity Perception Simulation"}),"\n",(0,t.jsx)(e.p,{children:"Implementing advanced perception simulation in Unity:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'// UnityPerceptionSimulator.cs\nusing UnityEngine;\nusing UnityEngine.Rendering;\nusing System.Collections;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Geometry;\n\npublic class UnityPerceptionSimulator : MonoBehaviour\n{\n    [Header("Camera Configuration")]\n    public Camera perceptionCamera;\n    public int imageWidth = 640;\n    public int imageHeight = 480;\n    public float cameraFov = 60f;\n    public float cameraNear = 0.1f;\n    public float cameraFar = 100f;\n\n    [Header("LIDAR Configuration")]\n    public Transform lidarSensor;\n    public float lidarRange = 30f;\n    public int horizontalResolution = 720;\n    public int verticalResolution = 1;\n    public float lidarUpdateRate = 10f; // Hz\n\n    [Header("Noise Configuration")]\n    public float cameraNoiseIntensity = 0.01f;\n    public float lidarNoise = 0.02f; // meters\n\n    [Header("ROS Topics")]\n    public string imageTopic = "/camera/image_raw";\n    public string cameraInfoTopic = "/camera/camera_info";\n    public string laserTopic = "/scan";\n\n    private RenderTexture renderTexture;\n    private float lidarUpdateInterval;\n    private float lastLidarUpdate;\n    private float lastImageUpdate;\n    \n    private ROSConnection ros;\n    private float[] lidarData;\n\n    void Start()\n    {\n        InitializePerception();\n    }\n\n    void InitializePerception()\n    {\n        // Camera setup\n        if (perceptionCamera == null)\n        {\n            perceptionCamera = GetComponent<Camera>();\n        }\n        \n        if (perceptionCamera == null)\n        {\n            perceptionCamera = gameObject.AddComponent<Camera>();\n        }\n\n        perceptionCamera.fieldOfView = cameraFov;\n        perceptionCamera.nearClipPlane = cameraNear;\n        perceptionCamera.farClipPlane = cameraFar;\n        perceptionCamera.enabled = true;\n\n        // Create render texture for camera\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        perceptionCamera.targetTexture = renderTexture;\n\n        // LIDAR setup\n        lidarUpdateInterval = 1.0f / lidarUpdateRate;\n        lastLidarUpdate = 0;\n        lidarData = new float[horizontalResolution];\n\n        // ROS setup\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Initialize(ROSConnection.Instance.RosIPAddress, ROSConnection.Instance.RosPort);\n\n        // Initialize camera info\n        PublishCameraInfo();\n    }\n\n    void Update()\n    {\n        // Update LIDAR simulation\n        if (Time.time - lastLidarUpdate >= lidarUpdateInterval)\n        {\n            SimulateLidarScan();\n            lastLidarUpdate = Time.time;\n        }\n    }\n\n    void SimulateLidarScan()\n    {\n        float horizontalAngleIncrement = 360.0f / horizontalResolution;\n\n        for (int i = 0; i < horizontalResolution; i++)\n        {\n            float angle = i * horizontalAngleIncrement * Mathf.Deg2Rad;\n            float distance = PerformLidarRaycast(angle);\n            \n            // Add noise to the measurement\n            distance += Random.Range(-lidarNoise, lidarNoise);\n            \n            // Apply range limits\n            lidarData[i] = Mathf.Clamp(distance, cameraNear, lidarRange);\n        }\n\n        PublishLaserScan();\n    }\n\n    float PerformLidarRaycast(float angle)\n    {\n        Vector3 direction = new Vector3(\n            Mathf.Cos(angle),\n            0f, // No elevation in 2D LIDAR\n            Mathf.Sin(angle)\n        );\n\n        direction = lidarSensor.TransformDirection(direction);\n\n        if (Physics.Raycast(lidarSensor.position, direction, out RaycastHit hit, lidarRange))\n        {\n            return hit.distance;\n        }\n        else\n        {\n            return lidarRange; // Max range if no object detected\n        }\n    }\n\n    void PublishLaserScan()\n    {\n        var laserScan = new LaserScanMsg();\n        laserScan.header = new std_msgs.HeaderMsg();\n        laserScan.header.stamp = new TimeMsg();\n        laserScan.header.frame_id = "laser_link";\n\n        laserScan.angle_min = -Mathf.PI;\n        laserScan.angle_max = Mathf.PI;\n        laserScan.angle_increment = (2 * Mathf.PI) / horizontalResolution;\n        laserScan.time_increment = 0.0f; // Not using time increment for this example\n        laserScan.scan_time = 1.0f / lidarUpdateRate;\n        laserScan.range_min = cameraNear;\n        laserScan.range_max = lidarRange;\n\n        laserScan.ranges = new float[horizontalResolution];\n        for (int i = 0; i < horizontalResolution; i++)\n        {\n            laserScan.ranges[i] = lidarData[i];\n        }\n\n        laserScan.intensities = new float[horizontalResolution]; // No intensity data\n\n        ros.Publish(laserTopic, laserScan);\n    }\n\n    public void CaptureAndPublishImage()\n    {\n        // Render the camera image to the render texture\n        perceptionCamera.Render();\n\n        // Create temporary texture to read from render texture\n        Texture2D imageTexture = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n\n        // Remember the currently active render texture\n        RenderTexture currentRT = RenderTexture.active;\n        RenderTexture.active = renderTexture;\n\n        // Read pixels from the active render texture\n        imageTexture.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        imageTexture.Apply();\n\n        // Restore the original render texture\n        RenderTexture.active = currentRT;\n\n        // Add noise to the image\n        AddImageNoise(imageTexture);\n\n        // Convert to ROS image message and publish\n        PublishImageMessage(imageTexture);\n\n        // Clean up\n        Destroy(imageTexture);\n    }\n\n    void AddImageNoise(Texture2D texture)\n    {\n        // Apply noise to the image\n        Color[] pixels = texture.GetPixels();\n\n        for (int i = 0; i < pixels.Length; i++)\n        {\n            Color original = pixels[i];\n            float noise = Random.Range(-cameraNoiseIntensity, cameraNoiseIntensity);\n\n            pixels[i] = new Color(\n                Mathf.Clamp01(original.r + noise),\n                Mathf.Clamp01(original.g + noise),\n                Mathf.Clamp01(original.b + noise),\n                original.a\n            );\n        }\n\n        texture.SetPixels(pixels);\n        texture.Apply();\n    }\n\n    void PublishImageMessage(Texture2D imageTexture)\n    {\n        // In a real implementation, convert Unity texture to ROS sensor_msgs/Image\n        // This would involve encoding the image data properly\n        \n        var imageMsg = new ImageMsg();\n        imageMsg.header = new std_msgs.HeaderMsg();\n        imageMsg.header.stamp = new TimeMsg();\n        imageMsg.header.frame_id = "camera_link";\n        \n        imageMsg.height = (uint)imageTexture.height;\n        imageMsg.width = (uint)imageTexture.width;\n        imageMsg.encoding = "rgb8";\n        imageMsg.is_bigendian = 0;\n        imageMsg.step = (uint)(imageTexture.width * 3); // 3 bytes per pixel for RGB\n        \n        // For this example, we\'re not actually transferring image data\n        // In a real implementation, encode the texture data\n        \n        ros.Publish(imageTopic, imageMsg);\n    }\n\n    void PublishCameraInfo()\n    {\n        var cameraInfo = new CameraInfoMsg();\n        cameraInfo.header = new std_msgs.HeaderMsg();\n        cameraInfo.header.stamp = new TimeMsg();\n        cameraInfo.header.frame_id = "camera_link";\n        \n        cameraInfo.width = (uint)imageWidth;\n        cameraInfo.height = (uint)imageHeight;\n        \n        // Calculate camera matrix values\n        float fx = (imageWidth / 2.0f) / Mathf.Tan(Mathf.Deg2Rad * cameraFov / 2.0f);\n        float fy = fx; // Assuming square pixels\n        float cx = imageWidth / 2.0f;\n        float cy = imageHeight / 2.0f;\n        \n        cameraInfo.K = new double[] { fx, 0, cx, 0, fy, cy, 0, 0, 1 };\n        cameraInfo.R = new double[] { 1, 0, 0, 0, 1, 0, 0, 0, 1 };\n        cameraInfo.P = new double[] { fx, 0, cx, 0, 0, fy, cy, 0, 0, 0, 1, 0 };\n        \n        cameraInfo.distortion_model = "plumb_bob";\n        cameraInfo.D = new double[] { 0, 0, 0, 0, 0 }; // No distortion for this example\n\n        ros.Publish(cameraInfoTopic, cameraInfo);\n    }\n\n    // Visualization for debugging LIDAR rays\n    void OnDrawGizmosSelected()\n    {\n        if (lidarSensor == null) return;\n\n        float horizontalAngleIncrement = 360.0f / horizontalResolution;\n        Gizmos.color = Color.red;\n\n        for (int i = 0; i < horizontalResolution; i += 20) // Draw every 20th ray for visibility\n        {\n            float angle = i * horizontalAngleIncrement * Mathf.Deg2Rad;\n            Vector3 direction = new Vector3(Mathf.Cos(angle), 0, Mathf.Sin(angle));\n            direction = lidarSensor.TransformDirection(direction);\n\n            if (i < lidarData.Length)\n            {\n                Gizmos.DrawRay(lidarSensor.position, direction * lidarData[i]);\n            }\n            else\n            {\n                Gizmos.DrawRay(lidarSensor.position, direction * lidarRange);\n            }\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"isaac-sim-simulation-environment",children:"Isaac Sim Simulation Environment"}),"\n",(0,t.jsx)(e.h3,{id:"isaac-sim-robot-configuration",children:"Isaac Sim Robot Configuration"}),"\n",(0,t.jsx)(e.p,{children:"Setting up advanced Isaac Sim robot models:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# isaac_sim_advanced_robot.py\nimport omni\nfrom pxr import Gf, Usd, UsdGeom, Sdf\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage, get_stage_units\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path, create_prim\nfrom omni.isaac.core.robots.robot import Robot\nfrom omni.isaac.core.articulations import ArticulationView\nfrom omni.isaac.core.utils.viewports import set_camera_view\nfrom omni.isaac.sensor import _sensor as _sensor\nimport numpy as np\nimport carb\n\nclass IsaacSimAdvancedRobot:\n    def __init__(self, robot_name: str = "carter", position: np.ndarray = np.array([0, 0, 0.5])):\n        self.world = World(stage_units_in_meters=1.0)\n        self.robot_name = robot_name\n        self.position = position\n        self.robot = None\n        \n        self._setup_scene()\n        self._setup_robot()\n        \n    def _setup_scene(self):\n        """Set up the Isaac Sim scene with realistic environment"""\n        # Add default ground plane\n        self.world.scene.add_default_ground_plane()\n        \n        # Add lighting\n        self._add_lighting()\n        \n        # Add textured environment\n        self._add_environment_objects()\n\n    def _add_lighting(self):\n        """Add realistic lighting to the scene"""\n        # Add a dome light for environment illumination\n        dome_light_path = "/World/DomeLight"\n        create_prim(\n            prim_path=dome_light_path,\n            prim_type="DomeLight",\n            position=np.array([0, 0, 0]),\n            orientation=np.array([0, 0, 0, 1]),\n            attributes={"color": (0.4, 0.4, 0.4), "intensity": 3000}\n        )\n        \n        # Add a distant light for shadows\n        distant_light_path = "/World/DistantLight"\n        create_prim(\n            prim_path=distant_light_path,\n            prim_type="DistantLight",\n            position=np.array([0, 0, 10]),\n            orientation=np.array([0, 0, 0, 1]),\n            attributes={"color": (0.9, 0.9, 0.9), "intensity": 1000, "angle": 0.5}\n        )\n\n    def _add_environment_objects(self):\n        """Add objects to create a realistic environment"""\n        from omni.isaac.core.objects import DynamicCuboid\n        \n        # Add some static objects\n        table = DynamicCuboid(\n            prim_path="/World/Table",\n            name="table",\n            position=np.array([2, 2, 0.4]),\n            size=np.array([1.0, 0.6, 0.8]),\n            color=np.array([0.5, 0.5, 0.5])\n        )\n        self.world.scene.add(table)\n        \n        # Add a wall\n        wall = DynamicCuboid(\n            prim_path="/World/Wall",\n            name="wall",\n            position=np.array([3, 0, 1]),\n            size=np.array([0.1, 5, 2]),\n            color=np.array([0.7, 0.7, 0.7])\n        )\n        self.world.scene.add(wall)\n\n    def _setup_robot(self):\n        """Set up the robot with realistic configuration"""\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            carb.log_error("Could not find Isaac Sim assets folder")\n            return\n            \n        # Load the Carter robot\n        robot_path = f"/World/{self.robot_name}"\n        carter_path = assets_root_path + "/Isaac/Robots/Carter/carter_nucleus.usd"\n        add_reference_to_stage(usd_path=carter_path, prim_path=robot_path)\n        \n        # Set the robot position\n        robot_prim = get_prim_at_path(robot_path)\n        if robot_prim.IsValid():\n            # Set position\n            import omni.kit.commands\n            omni.kit.commands.execute(\n                "TransformMultiPrimsSIPrimsCommand",\n                count=1,\n                paths=[robot_path],\n                new_positions=[self.position[0], self.position[1], self.position[2]],\n                usd_context=omni.usd.get_context()\n            )\n        \n        # Create robot view for control and observation\n        self.robot = ArticulationView(\n            prim_path=robot_path,\n            name="carter_view",\n            reset_xform_properties=False,\n        )\n        self.world.scene.add(self.robot)\n\n    def setup_sensors(self):\n        """Configure advanced sensors on the robot"""\n        from omni.isaac.range_sensor import _range_sensor\n        from omni.isaac.core.utils.prims import is_prim_path_valid\n        \n        # Get sensor interface\n        self.sensor_interface = _range_sensor.acquire_imu_sensor_interface()\n        \n        # Add LIDAR sensor\n        self._add_lidar_sensor()\n        \n        # Add camera sensor\n        self._add_camera_sensor()\n        \n        # Add IMU sensor\n        self._add_imu_sensor()\n\n    def _add_lidar_sensor(self):\n        """Add a 3D LIDAR sensor to the robot"""\n        # Use Isaac Sim\'s built-in LIDAR creation\n        lidar_path = f"/World/{self.robot_name}/Lidar"\n        \n        try:\n            # Create LIDAR using Omniverse commands (this is a simplified representation)\n            # In actual Isaac Sim, you would use the LIDAR sensor creation tools\n            carb.log_info(f"Creating LIDAR sensor at {lidar_path}")\n            \n            # For this example, we\'ll note that we would add a LIDAR sensor\n            # The actual implementation would depend on the Isaac Sim version\n            pass\n        except Exception as e:\n            carb.log_error(f"Could not create LIDAR sensor: {e}")\n\n    def _add_camera_sensor(self):\n        """Add a camera sensor to the robot"""\n        # Create camera sensor (simplified for the example)\n        camera_path = f"/World/{self.robot_name}/Camera"\n        \n        # In practice, add a camera using Isaac Sim tools\n        carb.log_info(f"Camera sensor placeholder at {camera_path}")\n\n    def _add_imu_sensor(self):\n        """Add an IMU sensor to the robot"""\n        # Create IMU sensor (simplified for the example)\n        imu_path = f"/World/{self.robot_name}/Imu"\n        \n        # In practice, add an IMU using Isaac Sim tools\n        carb.log_info(f"IMU sensor placeholder at {imu_path}")\n\n    def run_simulation(self, num_steps: int = 1000):\n        """Run the simulation for a specified number of steps"""\n        self.world.reset()\n        \n        for step in range(num_steps):\n            # Apply some simple control (e.g., move forward)\n            if step < 200:\n                # Move forward for first 200 steps\n                self.apply_velocity_commands(1.0, 0.0)  # linear vel = 1.0, angular vel = 0.0\n            elif step < 400:\n                # Turn left\n                self.apply_velocity_commands(0.5, 0.5)  # forward and turn\n            else:\n                # Stop\n                self.apply_velocity_commands(0.0, 0.0)\n            \n            # Step the world\n            self.world.step(render=True)\n            \n            # Print progress every 100 steps\n            if step % 100 == 0:\n                robot_pos, robot_orn = self.robot.get_world_poses()\n                carb.log_info(f"Step {step}: Position = {robot_pos[0]}, Orientation = {robot_orn[0]}")\n\n    def apply_velocity_commands(self, linear_vel: float, angular_vel: float):\n        """Apply velocity commands to the robot"""\n        # For Carter robot, we need to convert differential drive commands to joint velocities\n        # Wheel separation for Carter is approximately 0.44m, wheel radius 0.115m\n        wheel_separation = 0.44  # meters\n        wheel_radius = 0.115     # meters\n        \n        # Convert linear and angular velocities to wheel velocities\n        left_wheel_vel = (linear_vel - angular_vel * wheel_separation / 2.0) / wheel_radius\n        right_wheel_vel = (linear_vel + angular_vel * wheel_separation / 2.0) / wheel_radius\n        \n        # Apply joint velocities\n        joint_indices = [0, 1]  # Assuming first two joints are wheels\n        velocities = np.array([[left_wheel_vel, right_wheel_vel]])\n        \n        # In a real implementation, set the joint velocities\n        # self.robot.set_joint_velocity_targets(velocities)\n\n    def get_robot_state(self):\n        """Get the current state of the robot"""\n        positions, orientations = self.robot.get_world_poses()\n        linear_vels, angular_vels = self.robot.get_velocities()\n        \n        return {\n            \'position\': positions[0],\n            \'orientation\': orientations[0],\n            \'linear_velocity\': linear_vels[0],\n            \'angular_velocity\': angular_vels[0]\n        }\n\n    def cleanup(self):\n        """Clean up resources"""\n        self.world.clear()\n        carb.log_info("Isaac Sim Advanced Robot cleaned up")\n\ndef run_advanced_robot_simulation():\n    """Run the advanced Isaac Sim robot simulation"""\n    sim_robot = IsaacSimAdvancedRobot(position=np.array([0, 0, 0.5]))\n    sim_robot.setup_sensors()\n    \n    carb.log_info("Starting advanced robot simulation...")\n    sim_robot.run_simulation(num_steps=1000)\n    sim_robot.cleanup()\n\nif __name__ == "__main__":\n    run_advanced_robot_simulation()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"advanced-simulation-techniques",children:"Advanced Simulation Techniques"}),"\n",(0,t.jsx)(e.h3,{id:"domain-randomization-implementation",children:"Domain Randomization Implementation"}),"\n",(0,t.jsx)(e.p,{children:"Implementing domain randomization for robust sim-to-real transfer:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# domain_randomization.py\nimport numpy as np\nimport random\nfrom scipy.stats import truncnorm\nimport carb\n\nclass DomainRandomization:\n    def __init__(self):\n        self.parameters = {\n            'robot_mass': (0.8, 1.2),  # 80% to 120% of nominal\n            'friction_coeff': (0.1, 1.0),\n            'camera_noise_multiplier': (0.5, 2.0),\n            'lighting_intensity': (0.5, 2.0),\n            'material_roughness': (0.0, 1.0),\n            'gravity': (9.7, 9.9),  # Variation in gravity\n            'actuator_dynamics': (0.9, 1.1),  # Delay/bandwidth variation\n        }\n        \n        self.randomization_schedule = {\n            'initial': 0.1,      # Apply 10% of range initially\n            'final': 1.0,        # Apply full range eventually\n            'schedule_type': 'linear',  # linear, exponential, etc.\n            'episodes_for_full_randomization': 1000\n        }\n    \n    def get_randomized_parameters(self, episode_number: int = 0):\n        \"\"\"Get randomized parameters based on episode number\"\"\"\n        randomized_params = {}\n        \n        # Calculate schedule factor\n        schedule_factor = min(\n            1.0, \n            episode_number / self.randomization_schedule['episodes_for_full_randomization']\n        )\n        \n        if self.randomization_schedule['schedule_type'] == 'linear':\n            applied_range = self.randomization_schedule['initial'] + \\\n                           schedule_factor * (self.randomization_schedule['final'] - \n                                            self.randomization_schedule['initial'])\n        elif self.randomization_schedule['schedule_type'] == 'exponential':\n            applied_range = self.randomization_schedule['initial'] * \\\n                           (self.randomization_schedule['final'] / \n                            self.randomization_schedule['initial']) ** schedule_factor\n        \n        for param_name, (min_val, max_val) in self.parameters.items():\n            # Calculate the range to apply based on schedule\n            center = (min_val + max_val) / 2.0\n            range_to_apply = (max_val - min_val) * applied_range / 2.0\n            \n            # Sample from the range\n            randomized_params[param_name] = np.random.uniform(\n                center - range_to_apply,\n                center + range_to_apply\n            )\n        \n        return randomized_params\n    \n    def get_truncated_normal_random(self, mean: float, std: float, min_val: float, max_val: float):\n        \"\"\"Get random value from truncated normal distribution\"\"\"\n        a, b = (min_val - mean) / std, (max_val - mean) / std\n        return truncnorm.rvs(a, b, loc=mean, scale=std)\n    \n    def systematic_randomization(self, episode_number: int, n_dims: int = 10):\n        \"\"\"Use systematic sampling instead of purely random\"\"\"\n        # Use Halton sequence for good coverage of parameter space\n        def halton_sequence(index, base):\n            result = 0\n            f = 1.0\n            i = index\n            while i > 0:\n                f = f / base\n                result = result + f * (i % base)\n                i = int(i / base)\n            return result\n        \n        # Generate Halton sequence for each dimension\n        bases = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29][:n_dims]  # Prime bases\n        \n        values = []\n        for i, base in enumerate(bases):\n            halton_val = halton_sequence(episode_number, base)\n            values.append(halton_val)\n        \n        return values\n\nclass PhysicsParameterRandomizer:\n    def __init__(self, sim_environment):\n        self.sim_env = sim_environment\n        self.domain_rand = DomainRandomization()\n        \n    def apply_randomization(self, episode_number: int):\n        \"\"\"Apply domain randomization to simulation parameters\"\"\"\n        params = self.domain_rand.get_randomized_parameters(episode_number)\n        \n        # Apply parameters to simulation\n        self._apply_mass_randomization(params)\n        self._apply_friction_randomization(params)\n        self._apply_dynamics_randomization(params)\n        \n        return params\n    \n    def _apply_mass_randomization(self, params):\n        \"\"\"Apply mass randomization to robot links\"\"\"\n        # In a real implementation, update mass properties of robot\n        # This would use the physics API of the simulation environment\n        carb.log_info(f\"Applying mass multiplier: {params['robot_mass']}\")\n    \n    def _apply_friction_randomization(self, params):\n        \"\"\"Apply friction randomization to contacts\"\"\"\n        # In a real implementation, update friction coefficients\n        carb.log_info(f\"Applying friction coefficient: {params['friction_coeff']}\")\n    \n    def _apply_dynamics_randomization(self, params):\n        \"\"\"Apply actuator dynamics randomization\"\"\"\n        # In a real implementation, update actuator parameters\n        carb.log_info(f\"Applying actuator dynamics multiplier: {params['actuator_dynamics']}\")\n"})}),"\n",(0,t.jsx)(e.h3,{id:"system-identification-for-simulation-calibration",children:"System Identification for Simulation Calibration"}),"\n",(0,t.jsx)(e.p,{children:"Implementing system identification to improve simulation accuracy:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# system_identification.py\nimport numpy as np\nfrom scipy.optimize import minimize, least_squares\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\nimport matplotlib.pyplot as plt\n\nclass SimulationSystemIdentification:\n    def __init__(self, simulation_model, real_robot_interface):\n        self.sim_model = simulation_model\n        self.real_robot = real_robot_interface\n        self.collected_data = []\n        self.correction_model = None\n        \n    def collect_training_data(self, n_samples: int = 500):\n        """Collect data for system identification"""\n        carb.log_info(f"Collecting {n_samples} training data samples...")\n        \n        for i in range(n_samples):\n            # Generate random control input\n            control_input = self._generate_random_control()\n            \n            # Apply to real robot and get response\n            real_state = self.real_robot.apply_control_and_measure(control_input)\n            \n            # Apply to simulation and get response\n            sim_state = self.sim_model.apply_control_and_predict(control_input)\n            \n            # Store the data point with error\n            data_point = {\n                \'control_input\': control_input,\n                \'real_state\': real_state,\n                \'sim_state\': sim_state,\n                \'error\': real_state - sim_state\n            }\n            \n            self.collected_data.append(data_point)\n            \n            if (i + 1) % 100 == 0:\n                carb.log_info(f"Collected {i + 1}/{n_samples} samples")\n    \n    def _generate_random_control(self):\n        """Generate random control input for system ID"""\n        # For a differential drive robot, random linear and angular velocities\n        linear_vel = np.random.uniform(-1.0, 1.0)\n        angular_vel = np.random.uniform(-0.5, 0.5)\n        return np.array([linear_vel, angular_vel])\n    \n    def train_correction_model(self, model_type: str = \'gaussian_process\'):\n        """Train a model to predict simulation corrections"""\n        if len(self.collected_data) == 0:\n            raise ValueError("No training data collected")\n        \n        # Prepare training data\n        X = np.array([data[\'sim_state\'] for data in self.collected_data])\n        y = np.array([data[\'error\'] for data in self.collected_data])\n        \n        if model_type == \'gaussian_process\':\n            # Define kernel for Gaussian Process\n            kernel = ConstantKernel(1.0) * RBF(1.0)\n            self.correction_model = GaussianProcessRegressor(\n                kernel=kernel,\n                n_restarts_optimizer=10,\n                alpha=1e-6,\n                normalize_y=True\n            )\n        \n        # Train the model\n        self.correction_model.fit(X, y)\n        carb.log_info("Correction model trained successfully")\n        \n        # Evaluate model performance\n        score = self.correction_model.score(X, y)\n        carb.log_info(f"Model R\xb2 score: {score:.4f}")\n    \n    def corrected_prediction(self, state, control):\n        """Get simulation prediction with learned correction"""\n        # Get base simulation prediction\n        sim_prediction = self.sim_model.apply_control_and_predict(control)\n        \n        # Apply learned correction if model exists\n        if self.correction_model is not None:\n            correction = self.correction_model.predict(sim_prediction.reshape(1, -1))\n            corrected_prediction = sim_prediction + correction.flatten()\n        else:\n            corrected_prediction = sim_prediction\n        \n        return corrected_prediction\n    \n    def optimize_simulation_parameters(self):\n        """Optimize simulation parameters by minimizing error"""\n        def objective_function(params):\n            # Apply parameters to simulation\n            self.sim_model.set_parameters(params)\n            \n            # Calculate total error\n            total_error = 0\n            for data_point in self.collected_data:\n                predicted_state = self.sim_model.apply_control_and_predict(\n                    data_point[\'control_input\']\n                )\n                error = np.mean((predicted_state - data_point[\'real_state\']) ** 2)\n                total_error += error\n            \n            # Reset parameters for next iteration\n            # self.sim_model.reset_parameters()  # Implement as appropriate\n            \n            return total_error / len(self.collected_data)\n        \n        # Get initial parameters\n        initial_params = self.sim_model.get_parameters()\n        \n        # Optimize using scipy\n        result = minimize(\n            objective_function,\n            initial_params,\n            method=\'BFGS\',\n            options={\'disp\': True}\n        )\n        \n        # Apply optimized parameters\n        self.sim_model.set_parameters(result.x)\n        \n        carb.log_info(f"Optimization completed. Final parameters: {result.x}")\n        carb.log_info(f"Final error: {result.fun}")\n        \n        return result\n\nclass AdaptiveSimulation:\n    def __init__(self, base_simulator, system_id_module):\n        self.base_sim = base_simulator\n        self.sys_id = system_id_module\n        self.performance_threshold = 0.05\n        \n    def adaptive_simulation(self, control_sequence, initial_state):\n        """Run simulation with adaptive correction"""\n        states = [initial_state]\n        current_state = initial_state\n        \n        for t, control in enumerate(control_sequence):\n            # Get prediction with correction\n            predicted_state = self.sys_id.corrected_prediction(current_state, control)\n            \n            states.append(predicted_state)\n            current_state = predicted_state\n            \n            # Periodically validate and update model\n            if t % 50 == 0 and t > 0:\n                self._validate_and_update_model(states[-50:])\n        \n        return states\n    \n    def _validate_and_update_model(self, recent_states):\n        """Validate model and retrain if necessary"""\n        # Implementation for online model validation and updating\n        pass\n\ndef run_system_identification_example():\n    """Run a system identification example"""\n    # This would typically connect to real simulation and robot interfaces\n    # sim_model = MySimulationModel()\n    # real_robot = MyRealRobotInterface()\n    # sys_id = SimulationSystemIdentification(sim_model, real_robot)\n    \n    # Collect training data\n    # sys_id.collect_training_data(n_samples=500)\n    \n    # Train correction model\n    # sys_id.train_correction_model()\n    \n    # Optimize parameters\n    # result = sys_id.optimize_simulation_parameters()\n    \n    # Evaluate performance\n    carb.log_info("System identification example completed")\n'})}),"\n",(0,t.jsx)(e.h2,{id:"simulation-validation-and-performance-analysis",children:"Simulation Validation and Performance Analysis"}),"\n",(0,t.jsx)(e.h3,{id:"simulation-validation-techniques",children:"Simulation Validation Techniques"}),"\n",(0,t.jsx)(e.p,{children:"Validating simulation accuracy and performance:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# simulation_validation.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nclass SimulationValidator:\n    def __init__(self):\n        self.metrics = {}\n        \n    def validate_trajectory_tracking(self, sim_trajectory, real_trajectory):\n        \"\"\"Validate how well simulation matches real trajectory\"\"\"\n        if len(sim_trajectory) != len(real_trajectory):\n            raise ValueError(\"Trajectory lengths must match\")\n        \n        # Calculate error metrics\n        mse = mean_squared_error(real_trajectory, sim_trajectory)\n        mae = mean_absolute_error(real_trajectory, sim_trajectory)\n        r2 = r2_score(real_trajectory, sim_trajectory)\n        \n        # Calculate RMSE\n        rmse = np.sqrt(mse)\n        \n        # Calculate maximum error\n        max_error = np.max(np.abs(real_trajectory - sim_trajectory))\n        \n        metrics = {\n            'mse': mse,\n            'mae': mae,\n            'rmse': rmse,\n            'r2': r2,\n            'max_error': max_error,\n            'mean_error': np.mean(real_trajectory - sim_trajectory),\n            'std_error': np.std(real_trajectory - sim_trajectory)\n        }\n        \n        return metrics\n    \n    def validate_sensor_data(self, sim_sensor_data, real_sensor_data):\n        \"\"\"Validate sensor data from simulation vs reality\"\"\"\n        # Apply statistical tests\n        ks_statistic, p_value = stats.ks_2samp(\n            sim_sensor_data.flatten(), \n            real_sensor_data.flatten()\n        )\n        \n        # Calculate correlation\n        correlation = np.corrcoef(\n            sim_sensor_data.flatten(), \n            real_sensor_data.flatten()\n        )[0, 1]\n        \n        return {\n            'ks_statistic': ks_statistic,\n            'p_value': p_value,\n            'correlation': correlation,\n            'sim_mean': np.mean(sim_sensor_data),\n            'real_mean': np.mean(real_sensor_data),\n            'sim_std': np.std(sim_sensor_data),\n            'real_std': np.std(real_sensor_data)\n        }\n    \n    def validate_dynamic_response(self, sim_input_output, real_input_output):\n        \"\"\"Validate dynamic system response\"\"\"\n        # For each input-output pair, validate the dynamic response\n        errors = []\n        for (sim_in, sim_out), (real_in, real_out) in zip(sim_input_output, real_input_output):\n            # Ensure inputs are the same\n            if not np.allclose(sim_in, real_in):\n                raise ValueError(\"Input signals must match for dynamic validation\")\n            \n            error = np.mean(np.abs(sim_out - real_out))\n            errors.append(error)\n        \n        return {\n            'mean_response_error': np.mean(errors),\n            'std_response_error': np.std(errors),\n            'max_response_error': np.max(errors)\n        }\n    \n    def plot_validation_results(self, sim_data, real_data, title=\"Validation Results\"):\n        \"\"\"Plot simulation vs real data for visual validation\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n        \n        # Plot 1: Trajectory comparison\n        axes[0, 0].plot(real_data, label='Real', linewidth=2)\n        axes[0, 0].plot(sim_data, label='Simulation', linestyle='--', linewidth=2)\n        axes[0, 0].set_title('Trajectory Comparison')\n        axes[0, 0].set_xlabel('Time')\n        axes[0, 0].set_ylabel('Value')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True)\n        \n        # Plot 2: Error over time\n        error = real_data - sim_data\n        axes[0, 1].plot(error)\n        axes[0, 1].set_title('Error Over Time')\n        axes[0, 1].set_xlabel('Time')\n        axes[0, 1].set_ylabel('Error')\n        axes[0, 1].grid(True)\n        \n        # Plot 3: Scatter plot (sim vs real)\n        axes[1, 0].scatter(sim_data, real_data, alpha=0.5)\n        min_val = min(np.min(sim_data), np.min(real_data))\n        max_val = max(np.max(sim_data), np.max(real_data))\n        axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n        axes[1, 0].set_xlabel('Simulation')\n        axes[1, 0].set_ylabel('Real')\n        axes[1, 0].set_title('Simulation vs Real Scatter')\n        axes[1, 0].grid(True)\n        \n        # Plot 4: Histogram of errors\n        axes[1, 1].hist(error, bins=50, density=True, alpha=0.7)\n        axes[1, 1].set_title('Error Distribution')\n        axes[1, 1].set_xlabel('Error')\n        axes[1, 1].set_ylabel('Density')\n        axes[1, 1].grid(True)\n        \n        plt.tight_layout()\n        plt.suptitle(title, fontsize=14, y=1.02)\n        plt.savefig('validation_results.png', dpi=300, bbox_inches='tight')\n        plt.show()\n\ndef conduct_comprehensive_validation():\n    \"\"\"Conduct comprehensive simulation validation\"\"\"\n    validator = SimulationValidator()\n    \n    # Example: validating a simple trajectory\n    time_steps = 100\n    time = np.linspace(0, 10, time_steps)\n    \n    # Real data (simulated for this example)\n    real_trajectory = np.sin(time) + 0.1 * np.random.normal(size=time_steps)\n    \n    # Simulation data (with some error)\n    sim_trajectory = np.sin(time) * 0.95 + 0.05  # Simulated with slight offset and scale error\n    \n    # Validate trajectory tracking\n    trajectory_metrics = validator.validate_trajectory_tracking(\n        sim_trajectory, real_trajectory\n    )\n    \n    print(\"Trajectory Validation Metrics:\")\n    for name, value in trajectory_metrics.items():\n        print(f\"  {name}: {value:.4f}\")\n    \n    # Plot results\n    validator.plot_validation_results(\n        sim_trajectory, real_trajectory, \n        title=\"Trajectory Validation: Simulation vs Real\"\n    )\n    \n    return trajectory_metrics\n\nif __name__ == \"__main__\":\n    conduct_comprehensive_validation()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,t.jsx)(e.p,{children:"This simulation module provided comprehensive coverage of digital twin simulation environments for robotics using Gazebo, Unity, and Isaac Sim. Students implemented realistic robot models with accurate physics and sensor simulation, applied advanced techniques like domain randomization and system identification, and validated simulation models against real-world behavior. The module emphasized practical implementation skills needed to create effective digital twins that can accelerate robotics development and enable successful sim-to-real transfer."}),"\n",(0,t.jsx)(e.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Digital Twin Simulation"}),"\n",(0,t.jsx)(e.li,{children:"Physics-Based Simulation"}),"\n",(0,t.jsx)(e.li,{children:"Sensor Simulation and Calibration"}),"\n",(0,t.jsx)(e.li,{children:"Domain Randomization"}),"\n",(0,t.jsx)(e.li,{children:"System Identification"}),"\n",(0,t.jsx)(e.li,{children:"Simulation Validation"}),"\n",(0,t.jsx)(e.li,{children:"Multi-Platform Simulation"}),"\n",(0,t.jsx)(e.li,{children:"Sim-to-Real Transfer"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"advanced-exercises",children:"Advanced Exercises"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement a reinforcement learning task in each simulation environment and compare performance"}),"\n",(0,t.jsx)(e.li,{children:"Design and apply domain randomization techniques for a specific robotic manipulation task"}),"\n",(0,t.jsx)(e.li,{children:"Perform system identification to calibrate simulation parameters based on real robot data"}),"\n",(0,t.jsx)(e.li,{children:"Validate sim-to-real transfer by comparing results across all three simulation platforms"}),"\n"]})]})}function c(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>s});var a=i(6540);const t={},o=a.createContext(t);function r(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);