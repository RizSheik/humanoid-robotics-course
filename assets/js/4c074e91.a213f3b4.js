"use strict";(globalThis.webpackChunkhumanoid_robotics_course=globalThis.webpackChunkhumanoid_robotics_course||[]).push([[2448],{2032:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-1-physical-ai-foundations/simulation","title":"Module 1: Simulation - Physical AI and Embodied Intelligence in Practice","description":"Simulation Overview","source":"@site/docs/module-1-physical-ai-foundations/simulation.md","sourceDirName":"module-1-physical-ai-foundations","slug":"/module-1-physical-ai-foundations/simulation","permalink":"/humanoid-robotics-course/docs/module-1-physical-ai-foundations/simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/RizSheik/humanoid-robotics-course/edit/main/docs/module-1-physical-ai-foundations/simulation.md","tags":[],"version":"current","frontMatter":{}}');var t=i(4848),o=i(8453);const r={},l="Module 1: Simulation - Physical AI and Embodied Intelligence in Practice",a={},c=[{value:"Simulation Overview",id:"simulation-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Required Simulation Tools",id:"required-simulation-tools",level:3},{value:"Simulation Environment Setup",id:"simulation-environment-setup",level:2},{value:"Gazebo Configuration",id:"gazebo-configuration",level:3},{value:"Isaac Sim Configuration",id:"isaac-sim-configuration",level:3},{value:"Simulation 1: Passive Dynamic Walking",id:"simulation-1-passive-dynamic-walking",level:2},{value:"Objective",id:"objective",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Simulation Code",id:"simulation-code",level:3},{value:"Analysis",id:"analysis",level:3},{value:"Simulation 2: Morphological Computation in Manipulation",id:"simulation-2-morphological-computation-in-manipulation",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Setup",id:"setup",level:3},{value:"Implementation",id:"implementation-1",level:3},{value:"Analysis",id:"analysis-1",level:3},{value:"Simulation 3: Sensorimotor Learning in Navigation",id:"simulation-3-sensorimotor-learning-in-navigation",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Setup",id:"setup-1",level:3},{value:"Implementation",id:"implementation-2",level:3},{value:"Analysis",id:"analysis-2",level:3},{value:"Simulation 4: Dynamic Balance and Control",id:"simulation-4-dynamic-balance-and-control",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Implementation",id:"implementation-3",level:3},{value:"Analysis",id:"analysis-3",level:3},{value:"Simulation Evaluation and Transfer",id:"simulation-evaluation-and-transfer",level:2},{value:"Simulation-to-Reality Gap Analysis",id:"simulation-to-reality-gap-analysis",level:3},{value:"Metrics for Evaluation",id:"metrics-for-evaluation",level:3},{value:"Advanced Simulation Techniques",id:"advanced-simulation-techniques",level:2},{value:"Differentiable Physics Simulation",id:"differentiable-physics-simulation",level:3},{value:"Hardware-in-the-Loop Simulation",id:"hardware-in-the-loop-simulation",level:3},{value:"Multi-Physics Simulation",id:"multi-physics-simulation",level:3},{value:"Troubleshooting Simulation Issues",id:"troubleshooting-simulation-issues",level:2},{value:"Simulation Extensions",id:"simulation-extensions",level:2},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"References",id:"references",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-1-simulation---physical-ai-and-embodied-intelligence-in-practice",children:"Module 1: Simulation - Physical AI and Embodied Intelligence in Practice"})}),"\n",(0,t.jsx)(n.h2,{id:"simulation-overview",children:"Simulation Overview"}),"\n",(0,t.jsx)(n.p,{children:"This simulation module focuses on implementing and testing Physical AI and embodied intelligence concepts in virtual environments. Students will work with simulation tools to understand how physical embodiment, sensorimotor loops, and environmental interaction contribute to intelligent behavior."}),"\n",(0,t.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this simulation module, students will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up and configure simulation environments for embodied robotics"}),"\n",(0,t.jsx)(n.li,{children:"Implement embodied agents that demonstrate morphological computation"}),"\n",(0,t.jsx)(n.li,{children:"Test sensorimotor learning algorithms in simulated environments"}),"\n",(0,t.jsx)(n.li,{children:"Analyze the effects of physical embodiment on robot behavior and learning"}),"\n",(0,t.jsx)(n.li,{children:"Transfer knowledge from simulation to potential real-world applications"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"required-simulation-tools",children:"Required Simulation Tools"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gazebo Harmonic"})," or ",(0,t.jsx)(n.strong,{children:"Isaac Sim"})," for physics simulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Humble Hawksbill"})," for robot control and communication"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Python 3.11+"})," for implementing controllers and learning algorithms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PyTorch/TensorFlow"})," for neural network implementations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI Gym/Gymnasium"})," or custom environments for learning tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"simulation-environment-setup",children:"Simulation Environment Setup"}),"\n",(0,t.jsx)(n.h3,{id:"gazebo-configuration",children:"Gazebo Configuration"}),"\n",(0,t.jsx)(n.p,{children:"For Gazebo-based simulations:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a simulation world with appropriate physics properties:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Set gravity (",(0,t.jsx)(n.code,{children:"<gravity>9.81 0 0</gravity>"}),")"]}),"\n",(0,t.jsx)(n.li,{children:"Configure physical properties (friction, restitution)"}),"\n",(0,t.jsx)(n.li,{children:"Define sensor parameters (noise, update rates)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Define robot models with accurate physical properties:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Mass, inertia, and center of mass for each link"}),"\n",(0,t.jsx)(n.li,{children:"Joint limits and dynamics (friction, damping)"}),"\n",(0,t.jsx)(n.li,{children:"Sensor placement and parameters"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create appropriate environments for testing:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Simple environments for basic testing"}),"\n",(0,t.jsx)(n.li,{children:"Complex environments for advanced challenges"}),"\n",(0,t.jsx)(n.li,{children:"Environments with varied terrain for locomotion"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"isaac-sim-configuration",children:"Isaac Sim Configuration"}),"\n",(0,t.jsx)(n.p,{children:"For Isaac Sim-based simulations:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up the environment with accurate physics parameters"}),"\n",(0,t.jsx)(n.li,{children:"Import robot models with proper articulation"}),"\n",(0,t.jsx)(n.li,{children:"Configure sensors (camera, LiDAR, IMU, force/torque)"}),"\n",(0,t.jsx)(n.li,{children:"Set up reward functions for learning tasks"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"simulation-1-passive-dynamic-walking",children:"Simulation 1: Passive Dynamic Walking"}),"\n",(0,t.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Demonstrate how physical embodiment can produce complex behaviors (walking) without active control, using only the interaction between body dynamics and environment."}),"\n",(0,t.jsx)(n.h3,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(n.p,{children:"Create a simple 2D passive dynamic walker in simulation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Passive Walker URDF --\x3e\n<?xml version="1.0"?>\n<robot name="passive_walker">\n  <link name="torso">\n    <inertial>\n      <mass value="5.0"/>\n      <origin xyz="0 0 0"/>\n      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 0"/>\n      <geometry>\n        <box size="0.3 0.1 0.5"/>\n      </geometry>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0"/>\n      <geometry>\n        <box size="0.3 0.1 0.5"/>\n      </geometry>\n    </collision>\n  </link>\n  \n  <link name="left_thigh">\n    <inertial>\n      <mass value="1.0"/>\n      <origin xyz="0 0 -0.25"/>\n      <inertia ixx="0.05" ixy="0" ixz="0" iyy="0.05" iyz="0" izz="0.001"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 -0.25"/>\n      <geometry>\n        <cylinder radius="0.05" length="0.5"/>\n      </geometry>\n    </visual>\n    <collision>\n      <origin xyz="0 0 -0.25"/>\n      <geometry>\n        <cylinder radius="0.05" length="0.5"/>\n      </geometry>\n    </collision>\n  </link>\n  \n  \x3c!-- Similar definition for right leg --\x3e\n  <joint name="left_hip" type="continuous">\n    <parent link="torso"/>\n    <child link="left_thigh"/>\n    <origin xyz="0 0.1 0.1"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n</robot>\n'})}),"\n",(0,t.jsx)(n.h3,{id:"simulation-code",children:"Simulation Code"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import Twist\nimport numpy as np\n\nclass PassiveWalkerSim(Node):\n    def __init__(self):\n        super().__init__('passive_walker_sim')\n        \n        # Physics parameters\n        self.gravity = 9.81\n        self.foot_length = 0.1  # Length of feet\n        self.time_step = 0.001  # Simulation time step\n        \n        # For a passive walker, we only need to set initial conditions\n        # The walking emerges from the physical interaction\n        \n        # Initial small push to start walking\n        self.initial_push = True\n        self.push_time = 0\n        \n        self.joint_pub = self.create_publisher(JointState, 'joint_commands', 10)\n        \n        # Timer for simulation loop\n        self.timer = self.create_timer(self.time_step, self.simulation_step)\n        \n    def simulation_step(self):\n        # For a truly passive walker, no active control is applied\n        # The walking emerges from the interaction between gravity, \n        # the slope of the surface, and the physical configuration\n        \n        # However, for simulation purposes, we might apply a small initial push\n        if self.initial_push and self.push_time < 0.5:  # Push for 0.5 seconds\n            self.push_time += self.time_step\n            # Apply a small horizontal force to initiate motion\n            # In a real simulation, this would be handled by the physics engine\n            pass\n        elif self.push_time >= 0.5:\n            self.initial_push = False\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sim = PassiveWalkerSim()\n    rclpy.spin(sim)\n    sim.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"analysis",children:"Analysis"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Observe how different leg angles and masses affect walking stability"}),"\n",(0,t.jsx)(n.li,{children:"Analyze the energy efficiency of the passive walk"}),"\n",(0,t.jsx)(n.li,{children:"Compare with active control approaches"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"simulation-2-morphological-computation-in-manipulation",children:"Simulation 2: Morphological Computation in Manipulation"}),"\n",(0,t.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Implement and test how physical properties of a robot can perform computational work that would otherwise require complex algorithms."}),"\n",(0,t.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Create two simulated grippers in Gazebo:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Rigid gripper with precise force control"}),"\n",(0,t.jsx)(n.li,{children:"Underactuated compliant gripper (e.g., tendon-driven or spring-loaded)"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Place various objects of different shapes and stiffnesses"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, ForceTorqueSensor\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\nclass MorphologicalComputationSim(Node):\n    def __init__(self):\n        super().__init__('morph_comp_sim')\n        \n        # Publishers for each gripper type\n        self.rigid_cmd_pub = self.create_publisher(Float64MultiArray, 'rigid_gripper/commands', 10)\n        self.compliant_cmd_pub = self.create_publisher(Float64MultiArray, 'compliant_gripper/commands', 10)\n        \n        # Subscribers for gripper states\n        self.rigid_state_sub = self.create_subscription(JointState, 'rigid_gripper/states', self.rigid_state_callback, 10)\n        self.compliant_state_sub = self.create_subscription(JointState, 'compliant_gripper/states', self.compliant_state_callback, 10)\n        \n        # Force feedback for rigid gripper\n        self.rigid_force_sub = self.create_subscription(ForceTorqueSensor, 'rigid_gripper/force', self.rigid_force_callback, 10)\n        \n        self.rigid_force = 0.0\n        self.compliant_grasp_success = False\n        \n        # Simulation step timer\n        self.timer = self.create_timer(0.01, self.control_step)\n        \n    def rigid_state_callback(self, msg):\n        # Store rigid gripper state\n        self.rigid_position = msg.position\n        self.rigid_velocity = msg.velocity\n        \n    def compliant_state_callback(self, msg):\n        # Store compliant gripper state\n        self.compliant_position = msg.position\n        self.compliant_velocity = msg.velocity\n        \n    def rigid_force_callback(self, msg):\n        # Store force feedback\n        self.rigid_force = msg.wrench.force.x  # Simplified to x-axis force\n        \n    def control_step(self):\n        # Control for rigid gripper - precise force control needed\n        rigid_cmd = Float64MultiArray()\n        \n        # More complex control for rigid gripper to manage force\n        if self.rigid_force < 5.0:  # Target force\n            # Increase grip strength\n            rigid_cmd.data = [0.8]  # More aggressive approach needed\n        elif self.rigid_force > 10.0:  # Maximum safe force\n            # Decrease grip strength\n            rigid_cmd.data = [0.2]  # Reduce pressure\n        else:\n            # Maintain current grip\n            rigid_cmd.data = [0.5]\n        \n        self.rigid_cmd_pub.publish(rigid_cmd)\n        \n        # Control for compliant gripper - simpler control due to physical compliance\n        compliant_cmd = Float64MultiArray()\n        \n        # Simple position control for compliant gripper\n        # The physical compliance handles force regulation\n        compliant_cmd.data = [0.7]  # Close to object, let compliance handle force\n        \n        self.compliant_cmd_pub.publish(compliant_cmd)\n        \n    def evaluate_grasps(self):\n        # Evaluate grasp success based on force and object position\n        # This would be called periodically to assess performance\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sim = MorphologicalComputationSim()\n    rclpy.spin(sim)\n    sim.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"analysis-1",children:"Analysis"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Compare the control complexity between both approaches"}),"\n",(0,t.jsx)(n.li,{children:"Measure grasp success rates for different object types"}),"\n",(0,t.jsx)(n.li,{children:"Analyze energy efficiency of each approach"}),"\n",(0,t.jsx)(n.li,{children:"Document how physical compliance reduces control requirements"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"simulation-3-sensorimotor-learning-in-navigation",children:"Simulation 3: Sensorimotor Learning in Navigation"}),"\n",(0,t.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Implement and test a sensorimotor learning algorithm where a robot learns to navigate through environmental interaction."}),"\n",(0,t.jsx)(n.h3,{id:"setup-1",children:"Setup"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create a maze environment in the simulation"}),"\n",(0,t.jsx)(n.li,{children:"Configure a mobile robot with proximity sensors"}),"\n",(0,t.jsx)(n.li,{children:"Set up reward system based on distance to goal and collision penalties"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"implementation-2",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist, Pose\nfrom std_msgs.msg import Float64\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport random\nfrom collections import deque\n\nclass LinearQNetwork(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LinearQNetwork, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n        \n    def forward(self, x):\n        return self.fc(x)\n\nclass SensorimotorLearningSim(Node):\n    def __init__(self):\n        super().__init__('sensorimotor_learning_sim')\n        \n        self.subscription = self.create_subscription(\n            LaserScan, 'laser_scan', self.scan_callback, 10)\n        \n        self.cmd_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        self.reward_pub = self.create_publisher(Float64, 'reward', 10)\n        \n        # RL parameters\n        self.state_size = 5  # Number of sensor readings used\n        self.action_size = 4  # 4 discrete actions: forward, turn left, turn right, slight turn\n        self.learning_rate = 0.001\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.gamma = 0.95  # Discount factor\n        self.memory = deque(maxlen=10000)\n        \n        # Neural networks\n        self.q_network = LinearQNetwork(self.state_size, self.action_size)\n        self.target_network = LinearQNetwork(self.state_size, self.action_size)\n        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n        \n        # Action definitions\n        self.actions = {\n            0: (0.5, 0.0),    # Forward\n            1: (0.2, 0.5),    # Turn left\n            2: (0.2, -0.5),   # Turn right\n            3: (0.4, 0.1)     # Slight turn\n        }\n        \n        # Game state\n        self.current_state = None\n        self.previous_state = None\n        self.previous_action = None\n        self.previous_reward = None\n        self.steps = 0\n        \n        # Goal position (simplified - in real sim would get from TF or parameters)\n        self.goal_x = 5.0\n        self.goal_y = 5.0\n        \n        # Timer for decision making\n        self.timer = self.create_timer(0.1, self.decision_step)\n        \n    def scan_callback(self, msg):\n        # Process laser scan to get state representation\n        # Take readings from front, front-left, front-right, left, right\n        front = min(min(msg.ranges[0:10]), min(msg.ranges[350:359]))\n        front_left = min(msg.ranges[45:55])\n        front_right = min(msg.ranges[305:315])\n        left = min(msg.ranges[80:100])\n        right = min(msg.ranges[260:280])\n        \n        # Normalize distances to 0-1 range (max sensor range assumed to be 10m)\n        state = np.array([\n            min(1.0, front/10.0),\n            min(1.0, front_left/10.0), \n            min(1.0, front_right/10.0),\n            min(1.0, left/10.0),\n            min(1.0, right/10.0)\n        ])\n        \n        self.current_state = state\n        \n    def get_robot_position(self):\n        # In real implementation, get from TF or robot state\n        # For simulation, we might track this internally or get from odometry\n        return (0, 0)  # Placeholder\n    \n    def calculate_reward(self, action, sensor_data):\n        # Calculate reward based on sensor readings and action taken\n        # Positive reward for moving toward goal, negative for collisions\n        x, y = self.get_robot_position()\n        \n        # Distance to goal (simplified)\n        dist_to_goal = np.sqrt((x - self.goal_x)**2 + (y - self.goal_y)**2)\n        \n        # Calculate reward\n        reward = 0\n        \n        # Reward for moving closer to goal\n        reward += (10 - dist_to_goal) * 0.1  # Higher reward when closer\n        \n        # Penalty for being close to obstacles\n        if sensor_data[0] < 0.3:  # Front sensor close to obstacle\n            reward -= 5.0\n        elif sensor_data[0] < 0.5:\n            reward -= 1.0\n            \n        # Small penalty for taking any action (to encourage efficiency)\n        reward -= 0.01\n        \n        return reward\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def replay(self, batch_size=32):\n        if len(self.memory) < batch_size:\n            return\n            \n        batch = random.sample(self.memory, batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.LongTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch])\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch])\n        \n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n        \n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n    \n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n    \n    def decision_step(self):\n        if self.current_state is None:\n            return\n            \n        # Choose action using epsilon-greedy\n        if random.random() < self.epsilon:\n            # Explore\n            action = random.randrange(self.action_size)\n        else:\n            # Exploit\n            state_tensor = torch.FloatTensor(self.current_state).unsqueeze(0)\n            q_values = self.q_network(state_tensor)\n            action = q_values.max(1)[1].item()\n        \n        # Execute action\n        cmd = Twist()\n        cmd.linear.x, cmd.angular.z = self.actions[action]\n        self.cmd_pub.publish(cmd)\n        \n        # Calculate reward\n        reward = self.calculate_reward(action, self.current_state)\n        \n        # Store experience for learning\n        if self.previous_state is not None:\n            self.remember(\n                self.previous_state, \n                self.previous_action, \n                self.previous_reward, \n                self.current_state, \n                False\n            )\n        \n        # Update previous values for next iteration\n        self.previous_state = self.current_state.copy()\n        self.previous_action = action\n        self.previous_reward = reward\n        \n        # Publish reward for monitoring\n        reward_msg = Float64()\n        reward_msg.data = reward\n        self.reward_pub.publish(reward_msg)\n        \n        # Learn from experience\n        if len(self.memory) > 1000:\n            self.replay()\n            if self.steps % 100 == 0:\n                self.update_target_network()\n                \n        # Decay exploration rate\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n            \n        self.steps += 1\n        \n        # Reset if episode ends (simplified - would typically have proper episodes)\n        if self.steps > 10000:  # Reset after 10000 steps\n            self.steps = 0\n            self.reset_environment()\n\n    def reset_environment(self):\n        # In a real simulation, reset robot to starting position\n        # For this example, we just reset some parameters\n        self.previous_state = None\n        self.previous_action = None\n        self.previous_reward = None\n        self.get_logger().info(\"Environment reset\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sim = SensorimotorLearningSim()\n    rclpy.spin(sim)\n    sim.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"analysis-2",children:"Analysis"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Plot learning curves showing improvement over time"}),"\n",(0,t.jsx)(n.li,{children:"Analyze which sensorimotor patterns led to successful navigation"}),"\n",(0,t.jsx)(n.li,{children:"Compare performance with traditional path-planning approaches"}),"\n",(0,t.jsx)(n.li,{children:"Discuss the role of embodiment in the learning process"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"simulation-4-dynamic-balance-and-control",children:"Simulation 4: Dynamic Balance and Control"}),"\n",(0,t.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Simulate dynamic balance control for a bipedal robot, demonstrating how embodied intelligence principles can maintain stability."}),"\n",(0,t.jsx)(n.h3,{id:"implementation-3",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import Vector3, Point\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\nclass BalanceControllerSim(Node):\n    def __init__(self):\n        super().__init__('balance_controller_sim')\n        \n        # Subscriptions\n        self.imu_sub = self.create_subscription(Imu, 'imu/data', self.imu_callback, 10)\n        self.joint_sub = self.create_subscription(JointState, 'joint_states', self.joint_callback, 10)\n        \n        # Publishers\n        self.joint_cmd_pub = self.create_publisher(Float64MultiArray, 'joint_commands', 10)\n        self.zmp_pub = self.create_publisher(Point, 'zmp', 10)\n        \n        # Balance control parameters\n        self.com_height = 0.8  # Center of mass height (meters)\n        self.gravity = 9.81\n        self.control_frequency = 100  # Hz\n        self.time_step = 1.0 / self.control_frequency\n        \n        # LIPM parameters\n        self.desired_com_pos = np.array([0.0, 0.0])  # Desired CoM position in x-y plane\n        self.desired_com_vel = np.array([0.0, 0.0])\n        \n        # PID controller gains for ZMP control\n        self.kp = 100.0\n        self.ki = 10.0\n        self.kd = 50.0\n        \n        # Integral and derivative terms\n        self.zmp_error_integral = np.array([0.0, 0.0])\n        self.prev_zmp_error = np.array([0.0, 0.0])\n        \n        # Timer for control loop\n        self.timer = self.create_timer(1.0/self.control_frequency, self.balance_control_step)\n        \n        # Robot state\n        self.roll = 0.0\n        self.pitch = 0.0\n        self.yaw = 0.0\n        self.angular_velocity = np.array([0.0, 0.0, 0.0])\n        self.joint_positions = []\n        self.joint_velocities = []\n        \n    def imu_callback(self, msg):\n        # Extract orientation from quaternion\n        q = msg.orientation\n        self.roll = np.arctan2(2.0 * (q.w * q.x + q.y * q.z), 1.0 - 2.0 * (q.x**2 + q.y**2))\n        self.pitch = np.arcsin(2.0 * (q.w * q.y - q.z * q.x))\n        self.yaw = np.arctan2(2.0 * (q.w * q.z + q.x * q.y), 1.0 - 2.0 * (q.y**2 + q.z**2))\n        \n        # Extract angular velocity\n        self.angular_velocity[0] = msg.angular_velocity.x\n        self.angular_velocity[1] = msg.angular_velocity.y\n        self.angular_velocity[2] = msg.angular_velocity.z\n        \n    def joint_callback(self, msg):\n        self.joint_positions = msg.position\n        self.joint_velocities = msg.velocity\n        \n    def balance_control_step(self):\n        # Calculate current ZMP (simplified - in reality would use force/torque sensors)\n        # For this simulation, we'll estimate ZMP from IMU and kinematic data\n        estimated_zmp = self.estimate_zmp()\n        \n        # Calculate error between desired and actual ZMP\n        # For this simple case, we want to keep ZMP at (0,0) - center of support\n        zmp_error = self.desired_com_pos - estimated_zmp\n        \n        # PID control for ZMP tracking\n        self.zmp_error_integral += zmp_error * self.time_step\n        zmp_error_derivative = (zmp_error - self.prev_zmp_error) / self.time_step\n        \n        # Calculate control output\n        zmp_control_output = (\n            self.kp * zmp_error + \n            self.ki * self.zmp_error_integral + \n            self.kd * zmp_error_derivative\n        )\n        \n        self.prev_zmp_error = zmp_error.copy()\n        \n        # Convert control output to joint commands\n        # This is a simplified approach - in reality, this would involve\n        # inverse kinematics, whole-body control, or other advanced techniques\n        joint_commands = self.compute_joint_commands(zmp_control_output)\n        \n        # Publish commands\n        cmd_msg = Float64MultiArray()\n        cmd_msg.data = joint_commands\n        self.joint_cmd_pub.publish(cmd_msg)\n        \n        # Publish ZMP for visualization\n        zmp_msg = Point()\n        zmp_msg.x = estimated_zmp[0]\n        zmp_msg.y = estimated_zmp[1]\n        zmp_msg.z = 0.0  # ZMP is a point on the ground\n        self.zmp_pub.publish(zmp_msg)\n        \n        # Log balance status\n        self.get_logger().debug(f'ZMP Error: {zmp_error}, Control Output: {zmp_control_output}')\n    \n    def estimate_zmp(self):\n        \"\"\"\n        Estimate Zero Moment Point from IMU and kinematic data.\n        In a real robot, this would come from foot force/torque sensors.\n        \"\"\"\n        # Simplified estimation based on orientation\n        # In practice, this requires force/torque sensors in the feet\n        estimated_zmp_x = -self.com_height / self.gravity * self.pitch\n        estimated_zmp_y = self.com_height / self.gravity * self.roll\n        \n        return np.array([estimated_zmp_x, estimated_zmp_y])\n    \n    def compute_joint_commands(self, control_output):\n        \"\"\"\n        Convert ZMP control output to joint commands.\n        This is a simplified approach - in reality, this would be much more complex.\n        \"\"\"\n        # For this simulation, we'll map control outputs to simple hip and ankle adjustments\n        # This is greatly simplified - a real implementation would use whole-body control\n        \n        # Extract control commands\n        x_cmd = control_output[0]\n        y_cmd = control_output[1]\n        \n        # Map to joint adjustments (simplified)\n        commands = [0.0] * 12  # Assuming 12 joints for a simple biped\n        \n        # Hip adjustments\n        commands[0] = x_cmd * 0.1  # Left hip roll\n        commands[1] = y_cmd * 0.1  # Left hip pitch\n        commands[6] = -x_cmd * 0.1  # Right hip roll\n        commands[7] = y_cmd * 0.1  # Right hip pitch\n        \n        # Ankle adjustments for balance\n        commands[2] = -y_cmd * 0.05  # Left ankle pitch\n        commands[3] = x_cmd * 0.05   # Left ankle roll\n        commands[8] = -y_cmd * 0.05  # Right ankle pitch\n        commands[9] = -x_cmd * 0.05  # Right ankle roll\n        \n        return commands\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sim = BalanceControllerSim()\n    \n    # Update target network periodically during simulation\n    sim.timer2 = sim.create_timer(1.0, sim.update_target_network)\n    \n    rclpy.spin(sim)\n    sim.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"analysis-3",children:"Analysis"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Evaluate balance stability under different perturbations"}),"\n",(0,t.jsx)(n.li,{children:"Analyze the role of physical parameters (COM height, foot size) on stability"}),"\n",(0,t.jsx)(n.li,{children:"Test recovery from various disturbances (pushes, uneven terrain)"}),"\n",(0,t.jsx)(n.li,{children:"Compare with static balance approaches"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"simulation-evaluation-and-transfer",children:"Simulation Evaluation and Transfer"}),"\n",(0,t.jsx)(n.h3,{id:"simulation-to-reality-gap-analysis",children:"Simulation-to-Reality Gap Analysis"}),"\n",(0,t.jsx)(n.p,{children:"Discuss the challenges and methods for addressing the sim-to-real gap:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Domain Randomization"}),": Vary lighting, textures, and camera parameters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamics Randomization"}),": Randomize physical parameters (masses, frictions, etc.)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise Injection"}),": Add realistic sensor and actuator noise"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Identification"}),": Tune simulation parameters to match real robot behavior"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"metrics-for-evaluation",children:"Metrics for Evaluation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task Performance Metrics"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Success rate"}),"\n",(0,t.jsx)(n.li,{children:"Time to complete tasks"}),"\n",(0,t.jsx)(n.li,{children:"Energy efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Stability measures"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Embodied Intelligence Metrics"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Morphological computation index"}),"\n",(0,t.jsx)(n.li,{children:"Sensorimotor loop efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Adaptation speed to environmental changes"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Learning Performance Metrics"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Convergence rate"}),"\n",(0,t.jsx)(n.li,{children:"Final performance level"}),"\n",(0,t.jsx)(n.li,{children:"Generalization to new conditions"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-simulation-techniques",children:"Advanced Simulation Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"differentiable-physics-simulation",children:"Differentiable Physics Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Modern simulators support differentiable physics, allowing gradients to flow through the simulation for direct optimization of control policies."}),"\n",(0,t.jsx)(n.h3,{id:"hardware-in-the-loop-simulation",children:"Hardware-in-the-Loop Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Connect real sensors and computational units to the simulation for more realistic testing while maintaining safety."}),"\n",(0,t.jsx)(n.h3,{id:"multi-physics-simulation",children:"Multi-Physics Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Combine multiple physical models (mechanical, electrical, thermal) for comprehensive system analysis."}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-simulation-issues",children:"Troubleshooting Simulation Issues"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unstable Dynamics"}),": Check mass/inertia properties and joint limits"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Slow Performance"}),": Reduce physics update rate or simplify models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Failures"}),": Verify reward function and state representation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control Issues"}),": Check PID gains and control frequency"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"simulation-extensions",children:"Simulation Extensions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adversarial Environments"}),": Create challenging scenarios that test the limits of embodied approaches"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Agent Embodiment"}),": Simulate teams of embodied agents"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Soft Body Simulation"}),": Implement soft robotic systems in simulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Developmental Learning"}),": Simulate robots that learn over extended periods"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,t.jsx)(n.p,{children:"This simulation module provided hands-on experience with embodied intelligence concepts in virtual environments. Students implemented and tested passive dynamic walking, morphological computation, sensorimotor learning, and dynamic balance control. The simulations demonstrated how physical embodiment can enhance robot intelligence and efficiency, while also highlighting the challenges and opportunities in embodied AI."}),"\n",(0,t.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Simulation-to-Reality Gap"}),"\n",(0,t.jsx)(n.li,{children:"Zero Moment Point (ZMP)"}),"\n",(0,t.jsx)(n.li,{children:"Passive Dynamic Walking"}),"\n",(0,t.jsx)(n.li,{children:"Sensorimotor Learning"}),"\n",(0,t.jsx)(n.li,{children:"Morphological Computation"}),"\n",(0,t.jsx)(n.li,{children:"Linear Inverted Pendulum Model (LIPM)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Tedrake, R. (2023). Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation."}),"\n",(0,t.jsx)(n.li,{children:"Kubricht, J. R., Holyoak, K. J., & Lu, H. (2017). The power of mental simulation."}),"\n",(0,t.jsxs)(n.li,{children:["Gazebo Documentation: ",(0,t.jsx)(n.a,{href:"http://gazebosim.org/tutorials",children:"http://gazebosim.org/tutorials"})]}),"\n",(0,t.jsxs)(n.li,{children:["NVIDIA Isaac Sim Documentation: ",(0,t.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/",children:"https://docs.omniverse.nvidia.com/isaacsim/"})]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);