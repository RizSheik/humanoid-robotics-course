"use strict";(globalThis.webpackChunkhumanoid_robotics_course=globalThis.webpackChunkhumanoid_robotics_course||[]).push([[7987],{4123:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"module-4-ai-robot-brain/simulation","title":"Module 4: Simulation - Isaac Platform for Perception and Control","description":"Simulation Environment Configuration","source":"@site/docs/module-4-ai-robot-brain/simulation.md","sourceDirName":"module-4-ai-robot-brain","slug":"/module-4-ai-robot-brain/simulation","permalink":"/humanoid-robotics-course/docs/module-4-ai-robot-brain/simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/RizSheik/humanoid-robotics-course/edit/main/docs/module-4-ai-robot-brain/simulation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Module 4: Practical Lab - Isaac Platform for Perception and Control","permalink":"/humanoid-robotics-course/docs/module-4-ai-robot-brain/practical-lab"},"next":{"title":"Module 4: Assignment - Isaac Platform for Perception and Control","permalink":"/humanoid-robotics-course/docs/module-4-ai-robot-brain/assignment"}}');var o=t(4848),a=t(8453);const s={},r="Module 4: Simulation - Isaac Platform for Perception and Control",l={},m=[{value:"Simulation Environment Configuration",id:"simulation-environment-configuration",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Isaac Platform Architecture for Simulation",id:"isaac-platform-architecture-for-simulation",level:3},{value:"Required Simulation Tools",id:"required-simulation-tools",level:3},{value:"Simulation 1: Isaac Sim for AI Training",id:"simulation-1-isaac-sim-for-ai-training",level:2},{value:"Objective",id:"objective",level:3},{value:"Setup",id:"setup",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Analysis",id:"analysis",level:3},{value:"Simulation 2: Isaac ROS Perception Pipeline",id:"simulation-2-isaac-ros-perception-pipeline",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Setup",id:"setup-1",level:3},{value:"Implementation",id:"implementation-1",level:3},{value:"Analysis",id:"analysis-1",level:3},{value:"Simulation 3: Isaac ROS Control Systems",id:"simulation-3-isaac-ros-control-systems",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Setup",id:"setup-2",level:3},{value:"Implementation",id:"implementation-2",level:3},{value:"Analysis",id:"analysis-2",level:3},{value:"Simulation 4: AI Model Deployment and Performance",id:"simulation-4-ai-model-deployment-and-performance",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Setup",id:"setup-3",level:3},{value:"Implementation",id:"implementation-3",level:3},{value:"Analysis",id:"analysis-3",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-simulation---isaac-platform-for-perception-and-control",children:"Module 4: Simulation - Isaac Platform for Perception and Control"})}),"\n",(0,o.jsx)(n.h2,{id:"simulation-environment-configuration",children:"Simulation Environment Configuration"}),"\n",(0,o.jsx)(n.p,{children:"This simulation module provides hands-on experience with the NVIDIA Isaac Platform for developing AI-powered perception and control systems in robotics. Students will work with Isaac Sim for AI training, Isaac ROS for perception pipelines, and integrated control systems."}),"\n",(0,o.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this simulation, students will be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Configure and run Isaac Sim for training AI models"}),"\n",(0,o.jsx)(n.li,{children:"Implement GPU-accelerated perception pipelines using Isaac ROS"}),"\n",(0,o.jsx)(n.li,{children:"Design and test control systems in Isaac environments"}),"\n",(0,o.jsx)(n.li,{children:"Validate AI models trained in simulation for real-world transfer"}),"\n",(0,o.jsx)(n.li,{children:"Analyze performance and bottlenecks in AI robotics systems"}),"\n",(0,o.jsx)(n.li,{children:"Deploy and optimize AI models for edge robotics platforms"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"isaac-platform-architecture-for-simulation",children:"Isaac Platform Architecture for Simulation"}),"\n",(0,o.jsx)(n.p,{children:"The Isaac Platform simulation environment consists of three main components:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac Sim"})," - High-fidelity physics simulation and synthetic data generation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS"})," - GPU-accelerated perception and control packages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac Applications"})," - Pre-built AI applications for navigation and manipulation"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"required-simulation-tools",children:"Required Simulation Tools"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac Sim"})," (Omniverse-based simulation platform)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac ROS packages"})," (with GPU acceleration)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Isaac Navigation"})," and ",(0,o.jsx)(n.strong,{children:"Isaac Manipulation"})," applications"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 Humble Hawksbill"})," with Isaac extensions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"NVIDIA GPU"})," with CUDA support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Python 3.11+"})," for scripting and development"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"simulation-1-isaac-sim-for-ai-training",children:"Simulation 1: Isaac Sim for AI Training"}),"\n",(0,o.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Implement and test Isaac Sim for generating synthetic data and training AI models for robotics applications."}),"\n",(0,o.jsx)(n.h3,{id:"setup",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Launch Isaac Sim with appropriate robot and environment models"}),"\n",(0,o.jsx)(n.li,{children:"Configure synthetic data generation pipelines"}),"\n",(0,o.jsx)(n.li,{children:"Implement domain randomization for robust model training"}),"\n",(0,o.jsx)(n.li,{children:"Set up reinforcement learning environments for control policy learning"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation",children:"Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# isaac_sim_ai_training.py\nimport omni\nimport carb\nfrom pxr import Gf, Usd, UsdGeom\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.objects import DynamicCuboid\nfrom omni.isaac.core.prims import RigidPrimView\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.isaac.range_sensor import _range_sensor\nfrom omni.isaac.core.sensors import ImuSensor\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nclass IsaacSimAIGym:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.syn_data = SyntheticDataHelper()\n        \n        # Robot configuration\n        self.robot_name = "carter"\n        self.robot = None\n        self.robot_view = None\n        \n        # Environment configuration\n        self.scene_objects = []\n        self.domain_randomization_settings = {\n            \'lighting_variation\': (0.5, 2.0),\n            \'material_roughness\': (0.0, 1.0),\n            \'texture_randomization\': True,\n            \'dynamic_objects\': 3,\n            \'environment_complexity\': 5  # 1-10 scale\n        }\n        \n        # AI training configuration\n        self.observation_space = 24  # Example: 24-dimensional state space\n        self.action_space = 2        # Example: linear and angular velocity\n        self.episode_length = 500    # Steps per episode\n        self.current_step = 0\n        \n    def setup_environment(self):\n        """Set up the Isaac Sim environment for AI training"""\n        print("Setting up Isaac Sim environment for AI training...")\n        \n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n        \n        # Add lighting\n        self._add_lighting()\n        \n        # Load robot\n        self._load_robot()\n        \n        # Add objects for interaction\n        self._add_training_objects()\n        \n        # Setup sensors\n        self._setup_sensors()\n        \n        # Configure domain randomization\n        self._configure_domain_randomization()\n        \n        print("Isaac Sim environment setup complete!")\n    \n    def _add_lighting(self):\n        """Add realistic lighting to the environment"""\n        # Add dome light for ambient illumination\n        dome_light_path = "/World/DomeLight"\n        carb.kit.commands.execute(\n            "CreatePrim",\n            prim_type="DomeLight",\n            prim_path=dome_light_path,\n            attributes={"color": (0.2, 0.2, 0.2), "intensity": 500}\n        )\n        \n        # Add directional light for shadows\n        distant_light_path = "/World/DistantLight"\n        carb.kit.commands.execute(\n            "CreatePrim", \n            prim_type="DistantLight",\n            prim_path=distant_light_path,\n            attributes={"color": (0.9, 0.9, 0.9), "intensity": 500, "angle": 0.1}\n        )\n    \n    def _load_robot(self):\n        """Load and configure the robot for training"""\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            carb.log_error("Could not find Isaac Sim assets folder")\n            return\n        \n        robot_path = f"/World/{self.robot_name}"\n        carter_path = assets_root_path + "/Isaac/Robots/Carter/carter_nucleus.usd"\n        add_reference_to_stage(usd_path=carter_path, prim_path=robot_path)\n        \n        # Create robot view for control\n        from omni.isaac.core.articulations import ArticulationView\n        self.robot_view = ArticulationView(\n            prim_path=robot_path,\n            name="carter_view",\n            reset_xform_properties=False,\n        )\n        self.world.scene.add(self.robot_view)\n        \n        print(f"Robot {self.robot_name} loaded and added to scene")\n    \n    def _add_training_objects(self):\n        """Add objects for navigation and manipulation training"""\n        # Add target object\n        target = DynamicCuboid(\n            prim_path="/World/Target",\n            name="target",\n            position=np.array([3.0, 0.0, 0.2]),\n            size=np.array([0.2, 0.2, 0.2]),\n            color=np.array([1.0, 0.0, 0.0])  # Red target\n        )\n        self.world.scene.add(target)\n        self.scene_objects.append(target)\n        \n        # Add obstacles\n        for i in range(5):\n            obstacle = DynamicCuboid(\n                prim_path=f"/World/Obstacle_{i}",\n                name=f"obstacle_{i}",\n                position=np.array([\n                    np.random.uniform(1.0, 4.0),\n                    np.random.uniform(-2.0, 2.0),\n                    0.2\n                ]),\n                size=np.array([0.3, 0.3, 0.4]),\n                color=np.array([0.5, 0.5, 0.5])  # Gray obstacle\n            )\n            self.world.scene.add(obstacle)\n            self.scene_objects.append(obstacle)\n        \n        print(f"Added target and {len(self.scene_objects)-1} obstacles for training")\n    \n    def _setup_sensors(self):\n        """Set up robot sensors for training"""\n        # In a real implementation, we would add sensors to the robot\n        # For this simulation, we\'ll note the sensor configuration\n        print("Sensor configuration noted for AI training")\n    \n    def _configure_domain_randomization(self):\n        """Configure domain randomization for robust training"""\n        print("Domain randomization configured:")\n        for setting, value in self.domain_randomization_settings.items():\n            print(f"  {setting}: {value}")\n    \n    def reset_environment(self):\n        """Reset the environment to a new configuration"""\n        self.world.reset()\n        self.current_step = 0\n        \n        # Reset robot to random position\n        rand_x = np.random.uniform(-2.0, 2.0)\n        rand_y = np.random.uniform(-2.0, 2.0)\n        robot_position = np.array([rand_x, rand_y, 0.5])\n        self.robot_view.set_world_poses(positions=robot_position.reshape(1, 3))\n        \n        # Randomize other objects in the scene\n        self._randomize_scene()\n        \n        return self.get_observation()\n    \n    def _randomize_scene(self):\n        """Apply domain randomization to the scene"""\n        # Randomize lighting\n        dome_light = get_prim_at_path("/World/DomeLight")\n        if dome_light.IsValid():\n            intensity = np.random.uniform(\n                self.domain_randomization_settings[\'lighting_variation\'][0],\n                self.domain_randomization_settings[\'lighting_variation\'][1]\n            ) * 500\n            dome_light.GetAttribute("intensity").Set(intensity)\n        \n        # Randomize object positions\n        for i, obj in enumerate(self.scene_objects[1:], start=1):  # Skip target\n            new_pos = np.array([\n                np.random.uniform(0.5, 4.0),\n                np.random.uniform(-3.0, 3.0),\n                0.2\n            ])\n            obj.set_world_poses(positions=new_pos.reshape(1, 3))\n    \n    def get_observation(self):\n        """Get current observation from the environment"""\n        # Get robot state\n        robot_pos, robot_orn = self.robot_view.get_world_poses()\n        robot_lin_vel, robot_ang_vel = self.robot_view.get_velocities()\n        \n        # Get target position\n        target_pos, _ = self.scene_objects[0].get_world_poses()  # First object is target\n        \n        # Calculate relative position to target\n        rel_pos = target_pos[0] - robot_pos[0]\n        \n        # Combine into observation vector\n        observation = np.concatenate([\n            robot_pos[0],  # Robot position [x, y, z]\n            robot_orn[0],  # Robot orientation [qw, qx, qy, qz]\n            robot_lin_vel[0, :2],  # Robot linear velocity [x, y] (ignore z)\n            robot_ang_vel[0, 2:3],  # Robot angular velocity [z] (ignore x, y)\n            rel_pos[:2]  # Relative position to target [x, y]\n        ]).astype(np.float32)\n        \n        return observation\n    \n    def apply_action(self, action):\n        """Apply action to the robot"""\n        # Action format: [linear_velocity, angular_velocity]\n        linear_vel = np.clip(action[0], -1.0, 1.0)\n        angular_vel = np.clip(action[1], -1.0, 1.0)\n        \n        # Convert to wheel velocities for differential drive\n        wheel_separation = 0.44  # meters\n        wheel_radius = 0.115     # meters\n        \n        left_wheel_vel = (linear_vel - angular_vel * wheel_separation / 2.0) / wheel_radius\n        right_wheel_vel = (linear_vel + angular_vel * wheel_separation / 2.0) / wheel_radius\n        \n        # Apply velocities to wheels (in real implementation, set joint velocities)\n        # self.robot_view.set_joint_velocities(np.array([[left_wheel_vel, right_wheel_vel]]))\n        \n    def calculate_reward(self, action, obs):\n        """Calculate reward for the current step"""\n        # Get target and robot positions\n        target_pos, _ = self.scene_objects[0].get_world_poses()  # Target is first object\n        robot_pos, _ = self.robot_view.get_world_poses()\n        \n        # Calculate distance to target\n        distance = np.linalg.norm(target_pos[0, :2] - robot_pos[0, :2])\n        \n        # Dense reward based on distance to target\n        reward = -distance * 0.01  # Negative because closer is better\n        \n        # Bonus for getting very close to target\n        if distance < 0.3:\n            reward += 1.0\n        \n        # Penalty for large actions to encourage smooth movement\n        action_penalty = -0.001 * np.sum(np.abs(action))\n        reward += action_penalty\n        \n        return reward\n    \n    def is_done(self):\n        """Check if the episode is done"""\n        # Get positions\n        robot_pos, _ = self.robot_view.get_world_poses()\n        target_pos, _ = self.scene_objects[0].get_world_poses()\n        \n        # Calculate distance\n        distance = np.linalg.norm(target_pos[0, :2] - robot_pos[0, :2])\n        \n        # Episode is done if:\n        # 1. Reached target\n        # 2. Exceeded maximum steps\n        # 3. Robot is out of bounds\n        return (\n            distance < 0.3 or  # Reached target\n            self.current_step >= self.episode_length or  # Exceeded max steps\n            abs(robot_pos[0, 0]) > 10.0 or  # Out of bounds in X\n            abs(robot_pos[0, 1]) > 10.0     # Out of bounds in Y\n        )\n    \n    def step(self, action):\n        """Take a step in the environment"""\n        # Apply action to robot\n        self.apply_action(action)\n        \n        # Step the physics simulation\n        self.world.step(render=True)\n        self.current_step += 1\n        \n        # Get new observation\n        observation = self.get_observation()\n        \n        # Calculate reward\n        reward = self.calculate_reward(action, observation)\n        \n        # Check if episode is done\n        done = self.is_done()\n        \n        # Additional info\n        info = {\n            \'step\': self.current_step,\n            \'distance_to_target\': np.linalg.norm(observation[-2:]),\n            \'episode_done\': done\n        }\n        \n        return observation, reward, done, info\n    \n    def run_training_episode(self, policy):\n        """Run a single training episode using the given policy"""\n        obs = self.reset_environment()\n        total_reward = 0\n        step_count = 0\n        \n        print(f"Starting episode with initial observation shape: {obs.shape}")\n        \n        while not self.is_done() and step_count < self.episode_length:\n            # Get action from policy\n            if hasattr(policy, \'get_action\'):\n                action = policy.get_action(obs)\n            else:\n                # Default random policy for simulation\n                action = np.random.uniform(-1, 1, size=self.action_space).astype(np.float32)\n            \n            # Take step in environment\n            obs, reward, done, info = self.step(action)\n            \n            total_reward += reward\n            step_count += 1\n            \n            if step_count % 50 == 0:\n                print(f"Step {step_count}: Reward = {reward:.3f}, Total = {total_reward:.3f}")\n        \n        print(f"Episode completed: Steps = {step_count}, Total Reward = {total_reward:.3f}")\n        return total_reward, step_count\n\nclass AIPolicy:\n    """Simple AI policy for demonstration"""\n    def __init__(self, action_space):\n        self.action_space = action_space\n        self.network = self._build_network()\n    \n    def _build_network(self):\n        """Build a simple neural network for policy"""\n        # In practice, this would be a more complex network trained on the environment\n        return nn.Sequential(\n            nn.Linear(24, 128),  # Match observation space\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, self.action_space)\n        )\n    \n    def get_action(self, observation):\n        """Get action from policy"""\n        obs_tensor = torch.FloatTensor(observation).unsqueeze(0)\n        \n        with torch.no_grad():\n            action = self.network(obs_tensor)\n        \n        # Convert to numpy and clip to valid range\n        action_np = action.numpy()[0]\n        return np.clip(action_np, -1.0, 1.0)\n\ndef train_ai_model():\n    """Train an AI model using Isaac Sim environment"""\n    sim_env = IsaacSimAIGym()\n    sim_env.setup_environment()\n    \n    # Create a simple policy for demonstration\n    policy = AIPolicy(action_space=2)\n    \n    print("Starting training loop...")\n    \n    # Run several episodes to test the environment\n    for episode in range(10):\n        print(f"\\nRunning training episode {episode + 1}/10")\n        total_reward, steps = sim_env.run_training_episode(policy)\n        print(f"Episode {episode + 1}: Total Reward = {total_reward:.3f}, Steps = {steps}")\n        \n        if episode < 9:  # Don\'t reset after the last episode\n            obs = sim_env.reset_environment()\n    \n    print("\\nTraining simulation completed!")\n\ndef main():\n    """Main function for Isaac Sim AI Training simulation"""\n    carb.log_info("Starting Isaac Sim AI Training Simulation")\n    \n    try:\n        train_ai_model()\n    except Exception as e:\n        carb.log_error(f"Error in Isaac Sim AI Training: {e}")\n    finally:\n        carb.log_info("Isaac Sim AI Training Simulation completed")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"analysis",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Monitor training performance and reward trends"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate the effectiveness of domain randomization"}),"\n",(0,o.jsx)(n.li,{children:"Analyze the quality of synthetic data generated"}),"\n",(0,o.jsx)(n.li,{children:"Assess the transferability of trained policies"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"simulation-2-isaac-ros-perception-pipeline",children:"Simulation 2: Isaac ROS Perception Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Implement and evaluate GPU-accelerated perception pipelines using Isaac ROS packages for computer vision tasks."}),"\n",(0,o.jsx)(n.h3,{id:"setup-1",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Configure Isaac ROS perception packages with GPU acceleration"}),"\n",(0,o.jsx)(n.li,{children:"Set up camera and sensor data processing pipelines"}),"\n",(0,o.jsx)(n.li,{children:"Implement object detection and tracking systems"}),"\n",(0,o.jsx)(n.li,{children:"Validate perception accuracy and performance"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# isaac_ros_perception_sim.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, Imu, LaserScan\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import Header, String\nfrom cv_bridge import CvBridge\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\nimport numpy as np\nimport cv2\nimport torch\nimport time\nimport threading\nfrom collections import deque\n\nclass IsaacROSPipelineSimulator(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_pipeline_simulator')\n        \n        # Initialize CV bridge\n        self.bridge = CvBridge()\n        \n        # Publishers for simulation\n        self.image_pub = self.create_publisher(Image, '/camera/image_raw', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, '/camera/camera_info', 10)\n        self.imu_pub = self.create_publisher(Imu, '/imu/data', 10)\n        self.scan_pub = self.create_publisher(LaserScan, '/scan', 10)\n        \n        # Perception output publishers\n        self.detection_pub = self.create_publisher(Detection2DArray, '/detections', 10)\n        self.visualization_pub = self.create_publisher(Image, '/perception_visualization', 10)\n        \n        # Control publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n        # Internal state\n        self.latest_image = None\n        self.latest_imu = None\n        self.latest_scan = None\n        self.simulated_objects = []\n        \n        # Performance monitoring\n        self.inference_times = deque(maxlen=100)\n        self.detection_rates = deque(maxlen=100)\n        \n        # Simulation parameters\n        self.sim_width = 640\n        self.sim_height = 480\n        self.sim_fov = 60  # degrees\n        self.sim_frame_rate = 30  # Hz\n        \n        # Perception parameters\n        self.confidence_threshold = 0.5\n        self.nms_threshold = 0.4  # Non-maximum suppression\n        \n        # Create timer for simulation\n        self.timer = self.create_timer(1.0/self.sim_frame_rate, self.simulation_step)\n        \n        # Initialize perception models\n        self.perception_model = self.initialize_perception_model()\n        \n        self.get_logger().info('Isaac ROS Pipeline Simulator initialized')\n\n    def initialize_perception_model(self):\n        \"\"\"Initialize perception model for simulation\"\"\"\n        # In a real implementation, this would load an Isaac ROS DNN Inference model\n        # For this simulation, we'll create a simple model\n        \n        # Create a simple CNN for object detection simulation\n        model = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 16, 3, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(16, 32, 3, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.AdaptiveAvgPool2d((1, 1)),\n            torch.nn.Flatten(),\n            torch.nn.Linear(32, 80)  # 80 coco classes\n        )\n        \n        # Set model to evaluation mode\n        model.eval()\n        \n        self.get_logger().info('Perception model initialized')\n        return model\n\n    def simulation_step(self):\n        \"\"\"Main simulation step - generate sensor data and run perception\"\"\"\n        # Generate simulated sensor data\n        self.generate_simulated_image()\n        self.generate_simulated_imu()\n        self.generate_simulated_scan()\n        \n        # Run perception pipeline\n        if self.latest_image is not None:\n            self.run_perception_pipeline()\n    \n    def generate_simulated_image(self):\n        \"\"\"Generate simulated camera image with objects\"\"\"\n        # Create a synthetic image\n        img = np.zeros((self.sim_height, self.sim_width, 3), dtype=np.uint8)\n        \n        # Add background elements\n        cv2.rectangle(img, (0, 0), (self.sim_width, self.sim_height), (100, 150, 200), -1)  # Sky-like background\n        \n        # Add ground\n        cv2.rectangle(img, (0, int(0.7*self.sim_height)), (self.sim_width, self.sim_height), (50, 150, 50), -1)  # Ground\n        \n        # Add simulated objects (for perception testing)\n        objects = [\n            {'name': 'person', 'bbox': [100, 150, 80, 150], 'color': (0, 255, 0)},\n            {'name': 'car', 'bbox': [300, 250, 120, 80], 'color': (255, 0, 0)},\n            {'name': 'chair', 'bbox': [200, 300, 60, 80], 'color': (0, 0, 255)}\n        ]\n        \n        for obj in objects:\n            x, y, w, h = obj['bbox']\n            cv2.rectangle(img, (x, y), (x+w, y+h), obj['color'], -1)\n            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 255, 255), 2)  # White border\n            cv2.putText(img, obj['name'], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n        \n        # Add some noise to make it more realistic\n        noise = np.random.normal(0, 10, img.shape).astype(np.uint8)\n        img = cv2.add(img, noise)\n        \n        # Convert to ROS Image and publish\n        img_msg = self.bridge.cv2_to_imgmsg(img, encoding='bgr8')\n        img_msg.header.stamp = self.get_clock().now().to_msg()\n        img_msg.header.frame_id = 'camera_link'\n        \n        self.image_pub.publish(img_msg)\n        self.latest_image = img\n    \n    def generate_simulated_imu(self):\n        \"\"\"Generate simulated IMU data\"\"\"\n        imu_msg = Imu()\n        imu_msg.header.stamp = self.get_clock().now().to_msg()\n        imu_msg.header.frame_id = 'imu_link'\n        \n        # Simulate IMU readings with realistic noise\n        imu_msg.linear_acceleration.x = np.random.normal(0.0, 0.1)\n        imu_msg.linear_acceleration.y = np.random.normal(0.0, 0.1)\n        imu_msg.linear_acceleration.z = 9.8 + np.random.normal(0.0, 0.1)  # Gravity + noise\n        \n        imu_msg.angular_velocity.x = np.random.normal(0.0, 0.01)\n        imu_msg.angular_velocity.y = np.random.normal(0.0, 0.01)\n        imu_msg.angular_velocity.z = np.random.normal(0.0, 0.01)\n        \n        # Orientation (simplified)\n        imu_msg.orientation.w = 1.0\n        imu_msg.orientation.x = 0.0\n        imu_msg.orientation.y = 0.0\n        imu_msg.orientation.z = 0.0\n        \n        # Set covariance values\n        for i in range(9):\n            imu_msg.linear_acceleration_covariance[i] = 0.01 if i % 4 == 0 else 0.0\n            imu_msg.angular_velocity_covariance[i] = 0.01 if i % 4 == 0 else 0.0\n            imu_msg.orientation_covariance[i] = 0.1 if i % 4 == 0 else 0.0\n        \n        self.imu_pub.publish(imu_msg)\n        self.latest_imu = imu_msg\n\n    def generate_simulated_scan(self):\n        \"\"\"Generate simulated LIDAR scan data\"\"\"\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\n        scan_msg.header.frame_id = 'laser_link'\n        \n        # LIDAR parameters\n        scan_msg.angle_min = -np.pi\n        scan_msg.angle_max = np.pi\n        scan_msg.angle_increment = 2 * np.pi / 360  # 360 points\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 1.0 / self.sim_frame_rate\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 10.0\n        \n        # Generate simulated ranges with some objects\n        ranges = []\n        for i in range(360):\n            angle = scan_msg.angle_min + i * scan_msg.angle_increment\n            \n            # Simulate distance to nearest object\n            # Add some simulated objects at specific angles\n            distance = scan_msg.range_max  # Default to max range\n            \n            # Add simulated objects at specific positions\n            if 85 < i < 95:  # Front object at 2m\n                distance = 2.0 + np.random.normal(0, 0.05)\n            elif 265 < i < 275:  # Back object at 1.5m\n                distance = 1.5 + np.random.normal(0, 0.05)\n            else:\n                # Random noise and ground\n                if np.random.random() < 0.02:  # 2% chance of obstacle\n                    distance = np.random.uniform(0.5, 5.0)\n            \n            ranges.append(min(distance, scan_msg.range_max))\n        \n        scan_msg.ranges = ranges\n        scan_msg.intensities = []  # No intensity data for simulation\n        \n        self.scan_pub.publish(scan_msg)\n        self.latest_scan = scan_msg\n\n    def run_perception_pipeline(self):\n        \"\"\"Run the Isaac ROS perception pipeline on simulated data\"\"\"\n        start_time = time.time()\n        \n        # Convert image to tensor for model input\n        image_tensor = torch.from_numpy(self.latest_image).permute(2, 0, 1).float() / 255.0\n        image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n        \n        # Run object detection simulation\n        with torch.no_grad():\n            # In real Isaac ROS, this would call the actual DNN inference node\n            detections = self.simulate_object_detection(image_tensor)\n        \n        # Process detections and publish results\n        detection_array = self.process_detections(detections)\n        self.detection_pub.publish(detection_array)\n        \n        # Create and publish visualization\n        vis_image = self.create_visualization(self.latest_image, detections)\n        vis_msg = self.bridge.cv2_to_imgmsg(vis_image, encoding='bgr8')\n        vis_msg.header.stamp = self.get_clock().now().to_msg()\n        vis_msg.header.frame_id = 'camera_link'\n        self.visualization_pub.publish(vis_msg)\n        \n        # Record performance\n        inference_time = time.time() - start_time\n        self.inference_times.append(inference_time)\n        \n        # Calculate FPS\n        avg_inference_time = sum(self.inference_times) / len(self.inference_times) if self.inference_times else 0\n        avg_fps = 1.0 / avg_inference_time if avg_inference_time > 0 else 0\n        \n        self.get_logger().debug(f'Perception FPS: {avg_fps:.2f} (Inference time: {inference_time:.4f}s)')\n    \n    def simulate_object_detection(self, image_tensor):\n        \"\"\"Simulate object detection (in real implementation, use Isaac ROS DNN Inference)\"\"\"\n        # In a real Isaac ROS system, this would use Isaac ROS DNN Inference package\n        # For this simulation, return pseudo-detections\n        \n        # Simulate detections based on what's in the image\n        simulated_detections = []\n        \n        # Hardcoded detections to match the objects we put in the simulated image\n        objects_in_image = [\n            {'class': 'person', 'bbox': [100, 150, 180, 300], 'confidence': 0.85},\n            {'class': 'car', 'bbox': [300, 250, 420, 330], 'confidence': 0.78},\n            {'class': 'chair', 'bbox': [200, 300, 260, 380], 'confidence': 0.65}\n        ]\n        \n        for obj in objects_in_image:\n            # Only include if confidence is above threshold\n            if obj['confidence'] > self.confidence_threshold:\n                detection = {\n                    'class': obj['class'],\n                    'confidence': obj['confidence'],\n                    'bbox': obj['bbox']  # [x_min, y_min, x_max, y_max]\n                }\n                simulated_detections.append(detection)\n        \n        return simulated_detections\n\n    def process_detections(self, detections):\n        \"\"\"Process detections into ROS message format\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = 'camera_link'\n        \n        for det in detections:\n            detection_msg = Detection2D()\n            \n            # Calculate center and size from bbox [x_min, y_min, x_max, y_max]\n            x_min, y_min, x_max, y_max = det['bbox']\n            center_x = (x_min + x_max) / 2\n            center_y = (y_min + y_max) / 2\n            size_x = x_max - x_min\n            size_y = y_max - y_min\n            \n            detection_msg.bbox.center.x = float(center_x)\n            detection_msg.bbox.center.y = float(center_y)\n            detection_msg.bbox.size_x = float(size_x)\n            detection_msg.bbox.size_y = float(size_y)\n            \n            # Add classification result\n            from vision_msgs.msg import ObjectHypothesisWithPose\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = det['class']\n            hypothesis.hypothesis.score = det['confidence']\n            \n            detection_msg.results.append(hypothesis)\n            detection_array.detections.append(detection_msg)\n        \n        return detection_array\n\n    def create_visualization(self, image, detections):\n        \"\"\"Create visualization of detections on image\"\"\"\n        vis_image = image.copy()\n        \n        for detection in detections:\n            if detection['confidence'] > self.confidence_threshold:\n                bbox = detection['bbox']\n                x_min, y_min, x_max, y_max = bbox\n                \n                # Draw bounding box\n                cv2.rectangle(vis_image, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n                \n                # Draw label and confidence\n                label = f\"{detection['class']}: {detection['confidence']:.2f}\"\n                label_position = (int(x_min), int(y_min) - 10)\n                cv2.putText(vis_image, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n        \n        return vis_image\n\n    def get_performance_metrics(self):\n        \"\"\"Get performance metrics for the perception pipeline\"\"\"\n        if not self.inference_times:\n            return {\n                'avg_inference_time': 0.0,\n                'avg_fps': 0.0,\n                'latency_percentiles': [0.0, 0.0]\n            }\n        \n        avg_inference_time = sum(self.inference_times) / len(self.inference_times)\n        avg_fps = 1.0 / avg_inference_time if avg_inference_time > 0 else 0\n        \n        times_list = list(self.inference_times)\n        p50 = float(np.percentile(times_list, 50)) if len(times_list) > 1 else avg_inference_time\n        p95 = float(np.percentile(times_list, 95)) if len(times_list) > 1 else avg_inference_time\n        \n        return {\n            'avg_inference_time': avg_inference_time,\n            'avg_fps': avg_fps,\n            'latency_percentiles': [p50, p95],\n            'inference_count': len(self.inference_times)\n        }\n\nclass IsaacROSPerceptionValidator(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_perception_validator')\n        \n        # Subscribe to perception outputs\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.detection_validation_callback, 10)\n        self.vis_sub = self.create_subscription(\n            Image, '/perception_visualization', self.visualization_validation_callback, 10)\n        \n        # Performance monitoring\n        self.detected_objects = deque(maxlen=100)\n        self.validation_results = {}\n        \n        # Timer for periodic validation\n        self.validation_timer = self.create_timer(5.0, self.periodic_validation)\n        \n        self.get_logger().info('Isaac ROS Perception Validator initialized')\n\n    def detection_validation_callback(self, msg):\n        \"\"\"Validate incoming detections\"\"\"\n        # Count detected objects\n        num_detections = len(msg.detections)\n        self.detected_objects.append(num_detections)\n        \n        # Validate detection quality\n        for detection in msg.detections:\n            # Check that bounding boxes are valid\n            bbox = detection.bbox\n            if bbox.size_x <= 0 or bbox.size_y <= 0:\n                self.get_logger().warning('Invalid bounding box detected')\n            \n            # Check confidence scores\n            for result in detection.results:\n                if not (0.0 <= result.hypothesis.score <= 1.0):\n                    self.get_logger().warning(f'Invalid confidence score: {result.hypothesis.score}')\n\n    def visualization_validation_callback(self, msg):\n        \"\"\"Validate visualization output\"\"\"\n        # Check image dimensions\n        if msg.height != 480 or msg.width != 640:  # Our simulation dimensions\n            self.get_logger().warning(f'Unexpected image dimensions: {msg.width}x{msg.height}')\n\n    def periodic_validation(self):\n        \"\"\"Periodically validate perception performance\"\"\"\n        if not self.detected_objects:\n            self.get_logger().info('No detections received for validation')\n            return\n        \n        avg_detections = sum(self.detected_objects) / len(self.detected_objects)\n        \n        self.get_logger().info(f'Perception Validation - Avg Detections: {avg_detections:.2f}, '\n                              f'Buffer Size: {len(self.detected_objects)}')\n\ndef run_perception_simulation():\n    \"\"\"Run the Isaac ROS perception simulation\"\"\"\n    rclpy.init()\n    \n    # Create simulator and validator nodes\n    simulator = IsaacROSPipelineSimulator()\n    validator = IsaacROSPerceptionValidator()\n    \n    try:\n        # Combine nodes in single executor\n        executor = rclpy.executors.SingleThreadedExecutor()\n        executor.add_node(simulator)\n        executor.add_node(validator)\n        \n        # Run for a while to collect data\n        import signal\n        import sys\n        \n        def signal_handler(sig, frame):\n            print('Shutting down perception simulation...')\n            simulator.get_logger().info('Performance metrics:')\n            metrics = simulator.get_performance_metrics()\n            for key, value in metrics.items():\n                simulator.get_logger().info(f'  {key}: {value}')\n            rclpy.shutdown()\n            sys.exit(0)\n        \n        signal.signal(signal.SIGINT, signal_handler)\n        simulator.get_logger().info('Starting Isaac ROS perception simulation...')\n        executor.spin()\n        \n    except KeyboardInterrupt:\n        simulator.get_logger().info('Shutting down perception simulation...')\n        metrics = simulator.get_performance_metrics()\n        for key, value in metrics.items():\n            simulator.get_logger().info(f'{key}: {value}')\n    finally:\n        simulator.destroy_node()\n        validator.destroy_node()\n        rclpy.shutdown()\n\ndef main():\n    run_perception_simulation()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"analysis-1",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Measure perception accuracy and false positive rates"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate performance metrics (FPS, latency)"}),"\n",(0,o.jsx)(n.li,{children:"Assess the quality of object detections"}),"\n",(0,o.jsx)(n.li,{children:"Analyze computational resource usage"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"simulation-3-isaac-ros-control-systems",children:"Simulation 3: Isaac ROS Control Systems"}),"\n",(0,o.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Implement and test GPU-accelerated control systems using Isaac ROS navigation and manipulation applications."}),"\n",(0,o.jsx)(n.h3,{id:"setup-2",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Configure Isaac ROS navigation stack with GPU acceleration"}),"\n",(0,o.jsx)(n.li,{children:"Set up manipulation control systems with Isaac ROS"}),"\n",(0,o.jsx)(n.li,{children:"Implement high-level planning and execution systems"}),"\n",(0,o.jsx)(n.li,{children:"Validate control system performance and safety"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-2",children:"Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# isaac_ros_control_sim.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped, Point\nfrom nav_msgs.msg import Odometry, Path\nfrom sensor_msgs.msg import JointState, Imu, LaserScan\nfrom control_msgs.msg import JointTrajectoryControllerState\nfrom std_msgs.msg import Header, String, Bool\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nfrom visualization_msgs.msg import MarkerArray, Marker\nimport numpy as np\nimport time\nfrom scipy.spatial.transform import Rotation as R\nimport tf2_ros\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\n\nclass IsaacNavigationStackSimulator(Node):\n    def __init__(self):\n        super().__init__('isaac_navigation_stack_simulator')\n        \n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, '/odom', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.path_pub = self.create_publisher(Path, '/plan', 10)\n        self.localization_pub = self.create_publisher(PoseWithCovarianceStamped, '/amcl_pose', 10)\n        self.imu_pub = self.create_publisher(Imu, '/imu/data', 10)\n        \n        # Subscribers\n        self.goal_sub = self.create_subscription(\n            PoseStamped, '/goal_pose', self.goal_callback, 10)\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, '/cmd_vel', self.velocity_command_callback, 10)\n        \n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n        \n        # Robot state\n        self.robot_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta (radians)\n        self.robot_velocity = np.array([0.0, 0.0])    # linear, angular\n        self.target_pose = None\n        self.navigation_state = 'idle'  # idle, planning, executing, reached_goal\n        self.control_frequency = 50  # Hz\n        self.dt = 1.0 / self.control_frequency\n        \n        # Navigation parameters\n        self.linear_vel_limit = 1.0  # m/s\n        self.angular_vel_limit = 1.0  # rad/s\n        self.arrival_tolerance = 0.2  # meters\n        self.angle_tolerance = 0.1    # radians\n        \n        # Control gains\n        self.kp_linear = 1.0  # Proportional gain for linear velocity\n        self.kp_angular = 2.0  # Proportional gain for angular velocity\n        \n        # Path planning (simplified for simulation)\n        self.current_plan = []\n        \n        # Create control timer\n        self.control_timer = self.create_timer(self.dt, self.navigation_control_loop)\n        \n        # Performance monitoring\n        self.control_loop_times = []\n        \n        self.get_logger().info('Isaac Navigation Stack Simulator initialized')\n\n    def goal_callback(self, msg):\n        \"\"\"Handle navigation goal requests\"\"\"\n        # Extract goal pose\n        self.target_pose = np.array([\n            msg.pose.position.x,\n            msg.pose.position.y,\n            self.quaternion_to_yaw(msg.pose.orientation)\n        ])\n        \n        self.navigation_state = 'planning'\n        self.get_logger().info(f'New navigation goal received: ({self.target_pose[0]:.2f}, {self.target_pose[1]:.2f})')\n        \n        # Simple path planning (in reality, this would use navigation stack)\n        self.plan_simple_path()\n    \n    def plan_simple_path(self):\n        \"\"\"Simple path planning for simulation\"\"\"\n        if self.target_pose is None:\n            return\n        \n        # Create a simple path from current position to target\n        # In a real Isaac Navigation implementation, this would use more sophisticated planners\n        current_pos = self.robot_pose[:2]\n        target_pos = self.target_pose[:2]\n        \n        # Simple straight-line path\n        direction = target_pos - current_pos\n        distance = np.linalg.norm(direction)\n        \n        # Generate path points\n        steps = max(1, int(distance / 0.5))  # 0.5m between waypoints\n        path = []\n        \n        for i in range(steps + 1):\n            t = i / steps if steps > 0 else 0\n            point = current_pos + t * direction\n            path.append(np.array([point[0], point[1], 0.0]))  # Adding theta=0 as placeholder\n        \n        self.current_plan = path\n        self.navigation_state = 'executing'\n        \n        # Publish the path for visualization\n        self.publish_path()\n        \n        self.get_logger().info(f'Simple path planned with {len(path)} waypoints')\n\n    def velocity_command_callback(self, msg):\n        \"\"\"Handle velocity commands from navigation stack\"\"\"\n        # For simulation, we just log the command\n        self.get_logger().debug(f'Velocity command: linear={msg.linear.x:.2f}, angular={msg.angular.z:.2f}')\n    \n    def navigation_control_loop(self):\n        \"\"\"Main navigation control loop\"\"\"\n        start_time = time.time()\n        \n        if self.navigation_state == 'idle':\n            # Publish current pose but don't move\n            self.publish_odometry()\n            self.publish_tf()\n        \n        elif self.navigation_state == 'executing':\n            if self.target_pose is not None:\n                # Calculate control commands\n                cmd_vel = self.calculate_navigation_command()\n                \n                # Apply commands to simulated robot\n                self.apply_velocity_commands(cmd_vel)\n                \n                # Publish odometry and TF\n                self.publish_odometry()\n                self.publish_tf()\n                \n                # Check if reached goal\n                distance = np.linalg.norm(self.robot_pose[:2] - self.target_pose[:2])\n                angle_diff = abs(self.robot_pose[2] - self.target_pose[2])\n                \n                if distance < self.arrival_tolerance and angle_diff < self.angle_tolerance:\n                    self.navigation_state = 'reached_goal'\n                    self.get_logger().info('Navigation goal reached!')\n                    self.publish_goal_reached()\n            else:\n                # No target, stay idle\n                self.navigation_state = 'idle'\n        \n        elif self.navigation_state == 'reached_goal':\n            # Stop robot and stay at goal\n            stop_cmd = Twist()\n            self.apply_velocity_commands(stop_cmd)\n            self.publish_odometry()\n            self.publish_tf()\n        \n        # Record timing\n        elapsed_time = time.time() - start_time\n        self.control_loop_times.append(elapsed_time)\n    \n    def calculate_navigation_command(self):\n        \"\"\"Calculate navigation commands to reach target\"\"\"\n        cmd = Twist()\n        \n        # Current position vs target\n        target_pos = self.target_pose[:2]\n        current_pos = self.robot_pose[:2]\n        \n        # Direction to target\n        direction = target_pos - current_pos\n        distance = np.linalg.norm(direction)\n        \n        if distance > 0.1:  # If not very close to target\n            # Normalize direction vector\n            direction_norm = direction / distance\n            \n            # Calculate desired heading\n            desired_heading = np.arctan2(direction[1], direction[0])\n            \n            # Calculate heading error\n            heading_error = desired_heading - self.robot_pose[2]\n            \n            # Normalize to [-\u03c0, \u03c0]\n            while heading_error > np.pi:\n                heading_error -= 2 * np.pi\n            while heading_error < -np.pi:\n                heading_error += 2 * np.pi\n            \n            # Proportional control\n            cmd.linear.x = min(self.linear_vel_limit, distance * self.kp_linear)\n            cmd.angular.z = heading_error * self.kp_angular\n            \n            # Apply velocity limits\n            cmd.linear.x = np.clip(cmd.linear.x, -self.linear_vel_limit, self.linear_vel_limit)\n            cmd.angular.z = np.clip(cmd.angular.z, -self.angular_vel_limit, self.angular_vel_limit)\n        \n        return cmd\n\n    def apply_velocity_commands(self, cmd_vel):\n        \"\"\"Apply velocity commands to simulated robot\"\"\"\n        # Update robot state using simple kinematic model\n        linear_vel = cmd_vel.linear.x\n        angular_vel = cmd_vel.angular.z\n        \n        # Update orientation\n        self.robot_pose[2] += angular_vel * self.dt\n        self.robot_pose[2] = ((self.robot_pose[2] + np.pi) % (2 * np.pi)) - np.pi  # Normalize to [-\u03c0, \u03c0]\n        \n        # Update position based on new orientation\n        self.robot_pose[0] += linear_vel * np.cos(self.robot_pose[2]) * self.dt\n        self.robot_pose[1] += linear_vel * np.sin(self.robot_pose[2]) * self.dt\n        \n        # Update velocity state\n        self.robot_velocity[0] = linear_vel\n        self.robot_velocity[1] = angular_vel\n\n    def publish_odometry(self):\n        \"\"\"Publish odometry information\"\"\"\n        odom = Odometry()\n        odom.header.stamp = self.get_clock().now().to_msg()\n        odom.header.frame_id = 'odom'\n        odom.child_frame_id = 'base_link'\n        \n        # Position\n        odom.pose.pose.position.x = float(self.robot_pose[0])\n        odom.pose.pose.position.y = float(self.robot_pose[1])\n        odom.pose.pose.position.z = 0.0\n        \n        # Convert orientation from theta to quaternion\n        quat = self.yaw_to_quaternion(self.robot_pose[2])\n        odom.pose.pose.orientation.x = quat[0]\n        odom.pose.pose.orientation.y = quat[1]\n        odom.pose.pose.orientation.z = quat[2]\n        odom.pose.pose.orientation.w = quat[3]\n        \n        # Velocity\n        odom.twist.twist.linear.x = float(self.robot_velocity[0])\n        odom.twist.twist.angular.z = float(self.robot_velocity[1])\n        \n        self.odom_pub.publish(odom)\n\n    def publish_tf(self):\n        \"\"\"Publish transform from odom to base_link\"\"\"\n        t = TransformStamped()\n        \n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'odom'\n        t.child_frame_id = 'base_link'\n        \n        t.transform.translation.x = float(self.robot_pose[0])\n        t.transform.translation.y = float(self.robot_pose[1])\n        t.transform.translation.z = 0.0\n        \n        quat = self.yaw_to_quaternion(self.robot_pose[2])\n        t.transform.rotation.x = quat[0]\n        t.transform.rotation.y = quat[1]\n        t.transform.rotation.z = quat[2]\n        t.transform.rotation.w = quat[3]\n        \n        self.tf_broadcaster.sendTransform(t)\n\n    def publish_path(self):\n        \"\"\"Publish the current plan for visualization\"\"\"\n        if not self.current_plan:\n            return\n        \n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = 'odom'\n        \n        for waypoint in self.current_plan:\n            pose_stamped = PoseStamped()\n            pose_stamped.header = path_msg.header\n            pose_stamped.pose.position.x = float(waypoint[0])\n            pose_stamped.pose.position.y = float(waypoint[1])\n            pose_stamped.pose.position.z = 0.0\n            \n            # Simple orientation pointing toward next waypoint\n            if np.array_equal(waypoint, self.current_plan[-1]):  # Last waypoint\n                quat = self.yaw_to_quaternion(0.0)\n            else:\n                next_idx = self.current_plan.index(waypoint) + 1\n                if next_idx < len(self.current_plan):\n                    target = self.current_plan[next_idx][:2]\n                    current = waypoint[:2]\n                    angle = np.arctan2(target[1] - current[1], target[0] - current[0])\n                    quat = self.yaw_to_quaternion(angle)\n                else:\n                    quat = self.yaw_to_quaternion(0.0)\n            \n            pose_stamped.pose.orientation.x = quat[0]\n            pose_stamped.pose.orientation.y = quat[1]\n            pose_stamped.pose.orientation.z = quat[2]\n            pose_stamped.pose.orientation.w = quat[3]\n            \n            path_msg.poses.append(pose_stamped)\n        \n        self.path_pub.publish(path_msg)\n\n    def publish_goal_reached(self):\n        \"\"\"Publish goal reached notification\"\"\"\n        self.get_logger().info('Goal reached successfully')\n\n    def quaternion_to_yaw(self, orientation):\n        \"\"\"Convert quaternion to yaw angle\"\"\"\n        sinr_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosr_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return np.arctan2(sinr_cosp, cosr_cosp)\n\n    def yaw_to_quaternion(self, yaw):\n        \"\"\"Convert yaw angle to quaternion\"\"\"\n        cy = np.cos(yaw * 0.5)\n        sy = np.sin(yaw * 0.5)\n        return [0.0, 0.0, sy, cy]\n\nclass IsaacManipulationControllerSimulator(Node):\n    def __init__(self):\n        super().__init__('isaac_manipulation_controller_simulator')\n        \n        # Publishers\n        self.joint_state_pub = self.create_publisher(JointState, '/joint_states', 10)\n        self.cartesian_command_pub = self.create_publisher(PoseStamped, '/cartesian_command', 10)\n        self.gripper_command_pub = self.create_publisher(Bool, '/gripper_command', 10)\n        \n        # Subscribers\n        self.cartesian_goal_sub = self.create_subscription(\n            PoseStamped, '/cartesian_goal', self.cartesian_goal_callback, 10)\n        \n        # Robot state\n        self.arm_joints = {\n            'shoulder_pan_joint': 0.0,\n            'shoulder_lift_joint': 0.0,\n            'elbow_joint': 0.0,\n            'wrist_1_joint': 0.0,\n            'wrist_2_joint': 0.0,\n            'wrist_3_joint': 0.0\n        }\n        \n        self.gripper_open = True\n        self.arm_state = 'idle'  # idle, planning, moving_to_waypoints, executing_task\n        self.current_cartesian_pose = np.array([0.5, 0.0, 0.3, 0.0, 0.0, 0.0])  # x,y,z,rx,ry,rz\n        \n        # Control parameters\n        self.control_frequency = 100  # Hz\n        self.dt = 1.0 / self.control_frequency\n        self.joint_velocity_limits = 0.5  # rad/s\n        \n        # Create control timer\n        self.control_timer = self.create_timer(self.dt, self.manipulation_control_loop)\n        \n        self.get_logger().info('Isaac Manipulation Controller Simulator initialized')\n\n    def cartesian_goal_callback(self, msg):\n        \"\"\"Handle Cartesian pose goal requests\"\"\"\n        # Extract goal pose\n        goal_pose = np.array([\n            msg.pose.position.x,\n            msg.pose.position.y,\n            msg.pose.position.z,\n            msg.pose.orientation.x,\n            msg.pose.orientation.y,\n            msg.pose.orientation.z,\n            msg.pose.orientation.w\n        ])\n        \n        # Convert quaternion to Euler angles for simulation\n        r = R.from_quat([msg.pose.orientation.x, msg.pose.orientation.y, \n                         msg.pose.orientation.z, msg.pose.orientation.w])\n        euler = r.as_euler('xyz')\n        \n        self.current_cartesian_pose = np.array([\n            msg.pose.position.x,\n            msg.pose.position.y,\n            msg.pose.position.z,\n            euler[0], euler[1], euler[2]\n        ])\n        \n        self.arm_state = 'moving_to_waypoints'\n        self.get_logger().info(f'Cartesian goal received: ({goal_pose[0]:.3f}, {goal_pose[1]:.3f}, {goal_pose[2]:.3f})')\n    \n    def manipulation_control_loop(self):\n        \"\"\"Main manipulation control loop\"\"\"\n        if self.arm_state == 'idle':\n            # Publish current joint states\n            self.publish_joint_states()\n        \n        elif self.arm_state == 'moving_to_waypoints':\n            # Simulate movement to waypoints\n            # In a real Isaac Manipulation implementation, this would call the actual manipulation stack\n            \n            # Publish joint states with updated positions (simplified simulation)\n            self.apply_inverse_kinematics()\n            self.publish_joint_states()\n            \n            # Check if reached target (simplified - just simulate movement)\n            self.arm_state = 'idle'  # In simulation, assume immediate movement\n            self.get_logger().info('Cartesian pose reached')\n    \n    def apply_inverse_kinematics(self):\n        \"\"\"Simplified inverse kinematics for simulation\"\"\"\n        # This is a simplified simulation of IK\n        # In real Isaac Manipulation, this would use advanced IK solvers\n        \n        # Update joint positions based on desired Cartesian pose\n        # This is a very simplified representation\n        self.arm_joints['shoulder_pan_joint'] = self.current_cartesian_pose[0] * 0.5\n        self.arm_joints['shoulder_lift_joint'] = self.current_cartesian_pose[1] * 0.3\n        self.arm_joints['elbow_joint'] = self.current_cartesian_pose[2] * 0.2\n        self.arm_joints['wrist_1_joint'] = self.current_cartesian_pose[3]\n        self.arm_joints['wrist_2_joint'] = self.current_cartesian_pose[4]\n        self.arm_joints['wrist_3_joint'] = self.current_cartesian_pose[5]\n\n    def publish_joint_states(self):\n        \"\"\"Publish joint state information\"\"\"\n        joint_state = JointState()\n        joint_state.header.stamp = self.get_clock().now().to_msg()\n        joint_state.header.frame_id = 'base_link'\n        \n        for joint_name, joint_pos in self.arm_joints.items():\n            joint_state.name.append(joint_name)\n            joint_state.position.append(joint_pos)\n            joint_state.velocity.append(0.0)  # Simplified: no velocity feedback\n            joint_state.effort.append(0.0)   # Simplified: no effort feedback\n        \n        self.joint_state_pub.publish(joint_state)\n\nclass IsaacROSControlSystem:\n    def __init__(self):\n        self.navigation_simulator = None\n        self.manipulation_simulator = None\n        \n    def start_systems(self):\n        \"\"\"Start both navigation and manipulation simulation systems\"\"\"\n        rclpy.init()\n        \n        self.navigation_simulator = IsaacNavigationStackSimulator()\n        self.manipulation_simulator = IsaacManipulationControllerSimulator()\n        \n        try:\n            executor = rclpy.executors.SingleThreadedExecutor()\n            executor.add_node(self.navigation_simulator)\n            executor.add_node(self.manipulation_simulator)\n            \n            self.navigation_simulator.get_logger().info('Isaac ROS Control Systems running...')\n            executor.spin()\n            \n        except KeyboardInterrupt:\n            self.navigation_simulator.get_logger().info('Shutting down Isaac ROS Control Systems')\n        finally:\n            self.navigation_simulator.destroy_node()\n            self.manipulation_simulator.destroy_node()\n            rclpy.shutdown()\n\ndef run_control_simulation():\n    \"\"\"Run the Isaac ROS control system simulation\"\"\"\n    control_system = IsaacROSControlSystem()\n    control_system.start_systems()\n\ndef main():\n    run_control_simulation()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"analysis-2",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Evaluate navigation accuracy and path planning effectiveness"}),"\n",(0,o.jsx)(n.li,{children:"Assess manipulation control precision"}),"\n",(0,o.jsx)(n.li,{children:"Test system integration and coordination"}),"\n",(0,o.jsx)(n.li,{children:"Monitor resource utilization and performance"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"simulation-4-ai-model-deployment-and-performance",children:"Simulation 4: AI Model Deployment and Performance"}),"\n",(0,o.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Deploy and evaluate AI models in Isaac environment, focusing on performance optimization and real-time constraints."}),"\n",(0,o.jsx)(n.h3,{id:"setup-3",children:"Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Configure AI models for Isaac deployment"}),"\n",(0,o.jsx)(n.li,{children:"Optimize models for edge GPU deployment"}),"\n",(0,o.jsx)(n.li,{children:"Integrate AI with perception and control systems"}),"\n",(0,o.jsx)(n.li,{children:"Validate performance against real-time requirements"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-3",children:"Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# isaac_ai_deployment_sim.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch_tensorrt\nimport numpy as np\nimport time\nimport threading\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Optional\nimport matplotlib.pyplot as plt\n\n@dataclass\nclass DeploymentConfig:\n    \"\"\"Configuration for AI model deployment\"\"\"\n    model_path: str\n    input_shape: Tuple[int, ...]\n    output_shape: Tuple[int, ...]\n    precision: str = 'fp32'  # fp32, fp16, int8\n    batch_size: int = 1\n    max_latency: float = 0.05  # 50ms\n    min_throughput: float = 20  # FPS\n    device: str = 'cuda:0'\n\nclass ModelOptimizer:\n    def __init__(self, config: DeploymentConfig):\n        self.config = config\n        self.original_model = None\n        self.optimized_model = None\n        self.is_quantized = False\n    \n    def load_model(self):\n        \"\"\"Load the original model\"\"\"\n        try:\n            # In a real implementation, load the saved model\n            # model = torch.jit.load(self.config.model_path)\n            # self.original_model = model\n            # self.original_model.eval()\n            \n            # For this simulation, create a mock model\n            input_features = np.prod(self.config.input_shape[1:])\n            output_features = np.prod(self.config.output_shape[1:])\n            \n            self.original_model = nn.Sequential(\n                nn.Linear(input_features, 256),\n                nn.ReLU(),\n                nn.Linear(256, 128),\n                nn.ReLU(),\n                nn.Linear(128, output_features)\n            ).eval()\n            \n            print(f\"Mock model loaded with input shape {self.config.input_shape} and output shape {self.config.output_shape}\")\n            return True\n        except Exception as e:\n            print(f\"Failed to load model: {e}\")\n            return False\n    \n    def optimize_for_tensorrt(self):\n        \"\"\"Optimize model for TensorRT deployment\"\"\"\n        if self.original_model is None:\n            print(\"Model not loaded, cannot optimize\")\n            return False\n        \n        try:\n            # Create dummy input\n            dummy_input = torch.randn(self.config.batch_size, *self.config.input_shape[1:]).cuda()\n            \n            # Compile with Torch-TensorRT\n            compilation_args = {\n                \"inputs\": [torch_tensorrt.Input(\n                    shape=[self.config.batch_size, *self.config.input_shape[1:]]\n                )],\n                \"enabled_precisions\": {torch.float} if self.config.precision == 'fp32' else {torch.float, torch.half},\n                \"refit_enabled\": True,\n                \"debug\": False\n            }\n            \n            self.optimized_model = torch_tensorrt.compile(\n                self.original_model.cuda(),\n                **compilation_args\n            )\n            \n            print(f\"Model optimized for TensorRT with {self.config.precision} precision\")\n            return True\n            \n        except Exception as e:\n            print(f\"TensorRT optimization failed: {e}\")\n            # Fallback to original model if optimization fails\n            self.optimized_model = self.original_model\n            return False\n    \n    def quantize_model(self):\n        \"\"\"Apply quantization to reduce model size and improve performance\"\"\"\n        if self.original_model is None:\n            print(\"Model not loaded, cannot quantize\")\n            return False\n        \n        try:\n            # Create quantized version\n            self.original_model = self.original_model.cpu().eval()\n            quantized_model = torch.quantization.quantize_dynamic(\n                self.original_model, {nn.Linear}, dtype=torch.qint8\n            )\n            \n            self.optimized_model = quantized_model\n            self.is_quantized = True\n            \n            print(f\"Model quantized to INT8. Size reduction: ~75%\")\n            return True\n            \n        except Exception as e:\n            print(f\"Quantization failed: {e}\")\n            return False\n\nclass AIModelDeploymentSimulator:\n    def __init__(self, config: DeploymentConfig):\n        self.config = config\n        self.optimizer = ModelOptimizer(config)\n        self.model = None\n        self.device = torch.device(config.device if torch.cuda.is_available() else 'cpu')\n        \n        # Performance monitoring\n        self.inference_times = deque(maxlen=1000)\n        self.throughput_history = deque(maxlen=100)\n        self.power_consumption = deque(maxlen=100)\n        self.memory_usage = deque(maxlen=100)\n        \n        # Threading for performance measurement\n        self.monitoring_lock = threading.Lock()\n        \n        # Initialize model\n        self.initialize_deployment()\n    \n    def initialize_deployment(self):\n        \"\"\"Initialize the model deployment\"\"\"\n        print(\"Initializing AI model deployment...\")\n        \n        # Load model\n        if not self.optimizer.load_model():\n            print(\"Failed to load model, stopping deployment\")\n            return\n        \n        # Optimize model\n        if self.config.precision.startswith('fp'):  # Use TensorRT for floating point\n            if not self.optimizer.optimize_for_tensorrt():\n                print(\"TensorRT optimization failed, using original model\")\n                self.model = self.optimizer.original_model\n            else:\n                self.model = self.optimizer.optimized_model\n        else:  # Use quantization for integer types\n            if not self.optimizer.quantize_model():\n                print(\"Quantization failed, using original model\")\n                self.model = self.optimizer.original_model\n            else:\n                self.model = self.optimizer.optimized_model\n        \n        # Move model to device\n        self.model = self.model.to(self.device)\n        self.model.eval()\n        \n        print(\"Model deployment initialized successfully\")\n    \n    def simulate_inference(self, input_tensor: torch.Tensor) -> Tuple[torch.Tensor, float]:\n        \"\"\"Simulate model inference with performance measurement\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not initialized\")\n        \n        # Ensure correct input shape and device\n        input_tensor = input_tensor.to(self.device)\n        if input_tensor.dim() == 3:  # Add batch dimension if missing\n            input_tensor = input_tensor.unsqueeze(0)\n        \n        # Measure inference time\n        start_time = time.time()\n        \n        with torch.no_grad():\n            if self.is_quantized:\n                # Quantized model needs different handling\n                output = self.model(input_tensor.float())  # Convert to float for quantized model\n            else:\n                output = self.model(input_tensor)\n        \n        end_time = time.time()\n        inference_time = end_time - start_time\n        \n        # Record performance metrics\n        with self.monitoring_lock:\n            self.inference_times.append(inference_time)\n            \n            # Simulate power consumption (in watts)\n            power = 1.0 + 0.5 * (inference_time / self.config.max_latency)  # More inference time = more power\n            self.power_consumption.append(power)\n            \n            # Simulate memory usage (in MB)\n            mem_usage = 200 + np.random.uniform(-20, 20)  # Base memory + noise\n            self.memory_usage.append(mem_usage)\n        \n        return output, inference_time\n    \n    def run_performance_test(self, num_inferences: int = 1000, input_generator=None):\n        \"\"\"Run comprehensive performance test\"\"\"\n        print(f\"Running performance test with {num_inferences} inferences...\")\n        \n        # Default input generator if none provided\n        if input_generator is None:\n            def input_generator():\n                return torch.randn(*self.config.input_shape).to(self.device)\n        \n        # Warmup inferences\n        print(\"Warming up model...\")\n        for _ in range(10):\n            dummy_input = input_generator()\n            _, _ = self.simulate_inference(dummy_input)\n        \n        # Main test\n        inference_times = []\n        outputs = []\n        \n        print(\"Running performance test...\")\n        for i in range(num_inferences):\n            input_tensor = input_generator()\n            output, inf_time = self.simulate_inference(input_tensor)\n            \n            inference_times.append(inf_time)\n            outputs.append(output)\n            \n            if (i + 1) % 100 == 0:\n                print(f\"Completed {i + 1}/{num_inferences} inferences\")\n        \n        # Analyze results\n        avg_time = np.mean(inference_times)\n        std_time = np.std(inference_times)\n        avg_fps = 1.0 / avg_time if avg_time > 0 else 0\n        p95_latency = np.percentile(inference_times, 95)\n        \n        print(f\"\\nPerformance Results:\")\n        print(f\"  Average inference time: {avg_time:.6f}s ({avg_fps:.2f} FPS)\")\n        print(f\"  Standard deviation: {std_time:.6f}s\")\n        print(f\"  95th percentile latency: {p95_latency:.6f}s\")\n        print(f\"  Min/Max latency: {min(inference_times):.6f}s / {max(inference_times):.6f}s\")\n        \n        # Check requirements\n        meets_latency = avg_time <= self.config.max_latency\n        meets_throughput = avg_fps >= self.config.min_throughput\n        \n        print(f\"\\nRequirement Compliance:\")\n        print(f\"  Latency requirement (<{self.config.max_latency}s): {'\u2713' if meets_latency else '\u2717'}\")\n        print(f\"  Throughput requirement (>{self.config.min_throughput} FPS): {'\u2713' if meets_throughput else '\u2717'}\")\n        \n        return {\n            'avg_time': avg_time,\n            'std_time': std_time,\n            'avg_fps': avg_fps,\n            'p95_latency': p95_latency,\n            'min_max': (min(inference_times), max(inference_times)),\n            'meets_latency': meets_latency,\n            'meets_throughput': meets_throughput,\n            'inference_times': inference_times\n        }\n    \n    def get_performance_metrics(self) -> Dict:\n        \"\"\"Get current performance metrics\"\"\"\n        with self.monitoring_lock:\n            if not self.inference_times:\n                return {'error': 'No data collected yet'}\n            \n            recent_times = list(self.inference_times)[-100:]  # Last 100 inferences\n            avg_time = np.mean(recent_times)\n            std_time = np.std(recent_times) if len(recent_times) > 1 else 0\n            avg_fps = 1.0 / avg_time if avg_time > 0 else 0\n            p90_latency = np.percentile(recent_times, 90) if len(recent_times) > 1 else avg_time\n            \n            avg_power = np.mean(self.power_consumption) if self.power_consumption else 0\n            avg_memory = np.mean(self.memory_usage) if self.memory_usage else 0\n        \n        return {\n            'avg_inference_time': float(avg_time),\n            'std_inference_time': float(std_time),\n            'avg_fps': float(avg_fps),\n            'p90_latency': float(p90_latency),\n            'avg_power_consumption': float(avg_power),\n            'avg_memory_usage': float(avg_memory),\n            'sample_count': len(recent_times)\n        }\n    \n    def visualize_performance(self, results: Dict):\n        \"\"\"Create visualization of performance results\"\"\"\n        if 'inference_times' not in results:\n            print(\"No inference times data to visualize\")\n            return\n        \n        plt.figure(figsize=(15, 10))\n        \n        # Plot 1: Inference time distribution\n        plt.subplot(2, 3, 1)\n        plt.hist(results['inference_times'], bins=50, alpha=0.7, color='skyblue')\n        plt.axvline(results['avg_time'], color='red', linestyle='--', label=f\"Avg: {results['avg_time']:.6f}s\")\n        plt.axvline(self.config.max_latency, color='orange', linestyle='--', label=f\"Target: {self.config.max_latency}s\")\n        plt.xlabel('Inference Time (s)')\n        plt.ylabel('Frequency')\n        plt.title('Inference Time Distribution')\n        plt.legend()\n        \n        # Plot 2: Inference time over sequence\n        plt.subplot(2, 3, 2)\n        times = results['inference_times']\n        plt.plot(times, alpha=0.7, color='green')\n        plt.axhline(results['avg_time'], color='red', linestyle='--', label=f\"Avg: {results['avg_time']:.6f}s\")\n        plt.axhline(self.config.max_latency, color='orange', linestyle='--', label=f\"Target: {self.config.max_latency}s\")\n        plt.xlabel('Inference Sequence')\n        plt.ylabel('Inference Time (s)')\n        plt.title('Inference Time Over Sequence')\n        plt.legend()\n        \n        # Plot 3: Performance requirements\n        plt.subplot(2, 3, 3)\n        req_labels = ['Latency', 'Throughput']\n        req_status = [results['meets_latency'], results['meets_throughput']]\n        req_colors = ['green' if status else 'red' for status in req_status]\n        \n        bars = plt.bar(req_labels, [1 if s else 0 for s in req_status], color=req_colors)\n        plt.ylim(0, 1.2)\n        plt.title('Requirement Compliance')\n        \n        # Add text labels\n        for bar, status in zip(bars, req_status):\n            height = bar.get_height()\n            plt.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n                    'Met' if status else 'Not Met', ha='center', va='bottom')\n        \n        # Plot 4: Performance vs requirements\n        plt.subplot(2, 3, 4)\n        metrics = ['Latency (s)', 'Throughput (FPS)']\n        actual = [results['avg_time'], results['avg_fps']]\n        requirements = [self.config.max_latency, self.config.min_throughput]\n        \n        x = np.arange(len(metrics))\n        width = 0.35\n        \n        plt.bar(x - width/2, actual, width, label='Actual', alpha=0.8)\n        plt.bar(x + width/2, requirements, width, label='Required', alpha=0.8)\n        \n        plt.xlabel('Metrics')\n        plt.ylabel('Values')\n        plt.title('Performance vs Requirements')\n        plt.xticks(x, metrics)\n        plt.legend()\n        \n        # Plot 5: Resource consumption\n        plt.subplot(2, 3, 5)\n        # We'll simulate resource data for this example\n        time_points = np.arange(100)\n        power_usage = np.random.normal(2.5, 0.2, 100)  # Simulated power usage\n        memory_usage = np.random.normal(800, 50, 100)  # Simulated memory usage (MB)\n        \n        plt.plot(time_points, power_usage, label='Power Consumption (W)', color='red', alpha=0.7)\n        plt.plot(time_points, memory_usage, label='Memory Usage (MB)', color='blue', alpha=0.7)\n        plt.xlabel('Time')\n        plt.ylabel('Resource Usage')\n        plt.title('Resource Consumption Over Time')\n        plt.legend()\n        \n        # Plot 6: Efficiency metrics\n        plt.subplot(2, 3, 6)\n        efficiency = [results['avg_fps'], 1/results['avg_time'], 1000/results['avg_time']]  # FPS, Inferences/sec, Inferences/second*1000\n        eff_labels = ['FPS', '1/Time', 'Scaled Perf']\n        \n        plt.bar(eff_labels, efficiency, color=['orange', 'purple', 'brown'], alpha=0.7)\n        plt.ylabel('Efficiency Metric')\n        plt.title('Efficiency Metrics')\n        plt.xticks(rotation=45)\n        \n        plt.tight_layout()\n        plt.suptitle(f'AI Model Deployment Performance: Precision={self.config.precision}, Batch={self.config.batch_size}', \n                     y=1.02, fontsize=14)\n        plt.show()\n\nclass AIDeploymentSystem:\n    def __init__(self, configs: List[DeploymentConfig]):\n        self.configs = configs\n        self.simulators = {}\n        self.performance_results = {}\n        \n    def deploy_models(self):\n        \"\"\"Deploy all configured models\"\"\"\n        print(\"Starting AI model deployments...\")\n        \n        for i, config in enumerate(self.configs):\n            print(f\"\\nDeploying model {i+1}/{len(self.configs)}: {config.model_path}\")\n            simulator = AIModelDeploymentSimulator(config)\n            self.simulators[f\"model_{i}\"] = simulator\n        \n        print(f\"\\nSuccessfully deployed {len(self.simulators)} models\")\n    \n    def run_comparative_analysis(self):\n        \"\"\"Run comparative performance analysis\"\"\"\n        print(\"\\nRunning comparative analysis across different configurations...\")\n        \n        results = {}\n        \n        for name, simulator in self.simulators.items():\n            print(f\"\\nTesting {name}...\")\n            \n            # Generate dummy inputs for testing\n            def input_gen():\n                return torch.randn(*simulator.config.input_shape).to(simulator.device)\n            \n            # Run performance test\n            result = simulator.run_performance_test(num_inferences=100, input_generator=input_gen)\n            results[name] = result\n        \n        # Compare results\n        print(\"\\nComparative Analysis Results:\")\n        print(\"-\" * 50)\n        \n        for name, result in results.items():\n            config = [c for c in self.configs if f\"model_{list(self.simulators.keys()).index(name)}\" == name][0]\n            print(f\"{name} (Precision: {config.precision}):\")\n            print(f\"  Avg FPS: {result['avg_fps']:.2f}\")\n            print(f\"  Avg Latency: {result['avg_time']:.6f}s\")\n            print(f\"  95th %ile: {result['p95_latency']:.6f}s\")\n            print(f\"  Meets Requirements: Latency={result['meets_latency']}, Throughput={result['meets_throughput']}\")\n            print()\n        \n        # Visualize comparison\n        self.visualize_comparative_results(results)\n        \n        return results\n    \n    def visualize_comparative_results(self, results: Dict):\n        \"\"\"Visualize comparative results\"\"\"\n        names = list(results.keys())\n        avg_fps = [results[name]['avg_fps'] for name in names]\n        avg_times = [results[name]['avg_time'] for name in names]\n        meets_reqs = [results[name]['meets_throughput'] for name in names]\n        \n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        \n        # FPS comparison\n        axes[0].bar(names, avg_fps, alpha=0.7, \n                   color=['green' if meets else 'red' for meets in meets_reqs])\n        axes[0].set_title('Frames Per Second Comparison')\n        axes[0].set_ylabel('FPS')\n        axes[0].tick_params(axis='x', rotation=45)\n        \n        # Latency comparison\n        axes[1].bar(names, avg_times, alpha=0.7,\n                   color=['green' if meets else 'red' for meets in meets_reqs])\n        axes[1].axhline(y=0.05, color='orange', linestyle='--', label='Target Latency (50ms)')\n        axes[1].set_title('Average Inference Time Comparison')\n        axes[1].set_ylabel('Time (s)')\n        axes[1].legend()\n        axes[1].tick_params(axis='x', rotation=45)\n        \n        # Requirement compliance\n        req_status = [1 if meets else 0 for meets in meets_reqs]\n        axes[2].bar(names, req_status, alpha=0.7, \n                   color=['green' if meets else 'red' for meets in meets_reqs])\n        axes[2].set_title('Requirement Compliance')\n        axes[2].set_ylabel('Meets Throughput Req')\n        axes[2].set_ylim(0, 1.2)\n        axes[2].tick_params(axis='x', rotation=45)\n        \n        # Add labels to requirement compliance chart\n        for i, (name, meets) in enumerate(zip(names, meets_reqs)):\n            axes[2].text(i, meets + 0.05, 'Yes' if meets else 'No', \n                        ha='center', va='bottom')\n        \n        plt.tight_layout()\n        plt.suptitle('Comparative Performance Analysis', y=1.02)\n        plt.show()\n\ndef run_ai_deployment_simulation():\n    \"\"\"Run the AI deployment simulation\"\"\"\n    print(\"Starting Isaac AI Deployment Simulation\")\n    \n    # Define different deployment configurations to compare\n    configs = [\n        DeploymentConfig(\n            model_path=\"./models/perception_fp32.pth\",\n            input_shape=(1, 3, 224, 224),\n            output_shape=(1, 80),\n            precision='fp32',\n            batch_size=1,\n            max_latency=0.05,\n            min_throughput=20\n        ),\n        DeploymentConfig(\n            model_path=\"./models/perception_fp16.pth\",\n            input_shape=(1, 3, 224, 224),\n            output_shape=(1, 80),\n            precision='fp16',\n            batch_size=1,\n            max_latency=0.03,\n            min_throughput=30\n        ),\n        DeploymentConfig(\n            model_path=\"./models/control_quantized.pth\",\n            input_shape=(1, 24),\n            output_shape=(1, 2),\n            precision='int8',\n            batch_size=1,\n            max_latency=0.01,\n            min_throughput=100\n        )\n    ]\n    \n    # Create deployment system\n    deployment_system = AIDeploymentSystem(configs)\n    \n    # Deploy models\n    deployment_system.deploy_models()\n    \n    # Run comparative analysis\n    results = deployment_system.run_comparative_analysis()\n    \n    print(\"\\nAI Deployment Simulation Completed\")\n    return results\n\ndef main():\n    results = run_ai_deployment_simulation()\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*50)\n    print(\"DEPLOYMENT SIMULATION SUMMARY\")\n    print(\"=\"*50)\n    for model_name, metrics in results.items():\n        print(f\"{model_name}:\")\n        print(f\"  Average FPS: {metrics['avg_fps']:.2f}\")\n        print(f\"  Average Latency: {metrics['avg_time']:.6f}s\")\n        print(f\"  Requirement Compliance: {'PASS' if metrics['meets_throughput'] and metrics['meets_latency'] else 'FAIL'}\")\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"analysis-3",children:"Analysis"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Evaluate model deployment performance across different configurations"}),"\n",(0,o.jsx)(n.li,{children:"Assess the effectiveness of optimization techniques"}),"\n",(0,o.jsx)(n.li,{children:"Compare performance between different precision settings"}),"\n",(0,o.jsx)(n.li,{children:"Analyze resource utilization and efficiency"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,o.jsx)(n.p,{children:"This simulation module provided comprehensive hands-on experience with the NVIDIA Isaac Platform for AI-powered robotics applications. Students worked with Isaac Sim for generating synthetic data and training AI models, implemented GPU-accelerated perception pipelines using Isaac ROS, developed control systems with navigation and manipulation capabilities, and evaluated AI model deployment performance. The simulations emphasized practical implementation skills needed to develop and validate AI systems for real-world robotics applications using the Isaac Platform's specialized tools and optimizations."}),"\n",(0,o.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Isaac Sim"}),"\n",(0,o.jsx)(n.li,{children:"Isaac ROS"}),"\n",(0,o.jsx)(n.li,{children:"Isaac Navigation"}),"\n",(0,o.jsx)(n.li,{children:"Isaac Manipulation"}),"\n",(0,o.jsx)(n.li,{children:"TensorRT Optimization"}),"\n",(0,o.jsx)(n.li,{children:"Domain Randomization"}),"\n",(0,o.jsx)(n.li,{children:"GPU-Accelerated Perception"}),"\n",(0,o.jsx)(n.li,{children:"AI Model Deployment"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["NVIDIA Isaac Sim Documentation: ",(0,o.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/",children:"https://docs.omniverse.nvidia.com/isaacsim/"})]}),"\n",(0,o.jsxs)(n.li,{children:["Isaac ROS Documentation: ",(0,o.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/ros/",children:"https://docs.nvidia.com/isaac/ros/"})]}),"\n",(0,o.jsx)(n.li,{children:"Isaac Navigation Documentation"}),"\n",(0,o.jsx)(n.li,{children:"Isaac Manipulation Documentation"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);